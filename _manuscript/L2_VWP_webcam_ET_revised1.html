<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jason Geller">
<meta name="author" content="Yanina Prystauka">
<meta name="author" content="Sarah E. Colby">
<meta name="author" content="Julia R. Drouin">
<meta name="keywords" content="VWP, Tutorial, Webcam eye-tracking, R, Gorilla, Spoken word recognition, L2 processing">
<meta name="description" content="VWP WEBCAM TUTORIAL">

<title>Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="L2_VWP_webcam_ET_revised1_files/libs/clipboard/clipboard.min.js"></script>
<script src="L2_VWP_webcam_ET_revised1_files/libs/quarto-html/quarto.js"></script>
<script src="L2_VWP_webcam_ET_revised1_files/libs/quarto-html/popper.min.js"></script>
<script src="L2_VWP_webcam_ET_revised1_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="L2_VWP_webcam_ET_revised1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L2_VWP_webcam_ET_revised1_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L2_VWP_webcam_ET_revised1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L2_VWP_webcam_ET_revised1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L2_VWP_webcam_ET_revised1_files/libs/bootstrap/bootstrap-5eac69a7c4ba8f37e5c2caf69c2c538c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="L2_VWP_webcam_ET_revised1_files/libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="L2_VWP_webcam_ET_revised1_files/libs/tabwid-1.1.3/tabwid.js"></script>


<link rel="stylesheet" href="_extensions/wjschne/apaquarto/apa.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#method" id="toc-method" class="nav-link active" data-scroll-target="#method">Method</a>
  <ul class="collapse">
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants">Participants</a></li>
  <li><a href="#materials" id="toc-materials" class="nav-link" data-scroll-target="#materials">Materials</a>
  <ul class="collapse">
  <li><a href="#vwp" id="toc-vwp" class="nav-link" data-scroll-target="#vwp">VWP</a></li>
  <li><a href="#headphone-screener" id="toc-headphone-screener" class="nav-link" data-scroll-target="#headphone-screener">Headphone Screener</a></li>
  <li><a href="#participant-background-and-experiment-conditions-questionnaire" id="toc-participant-background-and-experiment-conditions-questionnaire" class="nav-link" data-scroll-target="#participant-background-and-experiment-conditions-questionnaire">Participant Background and Experiment Conditions Questionnaire</a></li>
  <li><a href="#procedure" id="toc-procedure" class="nav-link" data-scroll-target="#procedure">Procedure</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#preprocessing-data" id="toc-preprocessing-data" class="nav-link" data-scroll-target="#preprocessing-data">Preprocessing Data</a>
  <ul class="collapse">
  <li><a href="#load-packages" id="toc-load-packages" class="nav-link" data-scroll-target="#load-packages">Load Packages</a>
  <ul class="collapse">
  <li><a href="#package-installation-and-setup" id="toc-package-installation-and-setup" class="nav-link" data-scroll-target="#package-installation-and-setup">Package Installation and Setup</a></li>
  </ul></li>
  <li><a href="#reading-in-data" id="toc-reading-in-data" class="nav-link" data-scroll-target="#reading-in-data">Reading in Data</a>
  <ul class="collapse">
  <li><a href="#behavioral-trial-level-data" id="toc-behavioral-trial-level-data" class="nav-link" data-scroll-target="#behavioral-trial-level-data">Behavioral, Trial-level, Data</a></li>
  <li><a href="#eye-tracking-data" id="toc-eye-tracking-data" class="nav-link" data-scroll-target="#eye-tracking-data">Eye-Tracking Data</a></li>
  <li><a href="#audio-onset" id="toc-audio-onset" class="nav-link" data-scroll-target="#audio-onset">Audio onset</a></li>
  <li><a href="#trial-removal" id="toc-trial-removal" class="nav-link" data-scroll-target="#trial-removal">Trial Removal</a></li>
  <li><a href="#low-accuracy" id="toc-low-accuracy" class="nav-link" data-scroll-target="#low-accuracy">Low Accuracy</a></li>
  <li><a href="#rts" id="toc-rts" class="nav-link" data-scroll-target="#rts">RTs</a></li>
  <li><a href="#sampling-rate" id="toc-sampling-rate" class="nav-link" data-scroll-target="#sampling-rate">Sampling Rate</a></li>
  <li><a href="#out-of-bounds-outside-of-screen" id="toc-out-of-bounds-outside-of-screen" class="nav-link" data-scroll-target="#out-of-bounds-outside-of-screen">Out-of-Bounds (Outside of Screen)</a></li>
  </ul></li>
  <li><a href="#eye-tracking-data-1" id="toc-eye-tracking-data-1" class="nav-link" data-scroll-target="#eye-tracking-data-1">Eye-tracking data</a>
  <ul class="collapse">
  <li><a href="#convergence-and-confidence" id="toc-convergence-and-confidence" class="nav-link" data-scroll-target="#convergence-and-confidence">Convergence and Confidence</a></li>
  <li><a href="#combining-eye-and-trial-level-data" id="toc-combining-eye-and-trial-level-data" class="nav-link" data-scroll-target="#combining-eye-and-trial-level-data">Combining Eye and Trial-Level Data</a></li>
  </ul></li>
  <li><a href="#areas-of-interest" id="toc-areas-of-interest" class="nav-link" data-scroll-target="#areas-of-interest">Areas of Interest</a>
  <ul class="collapse">
  <li><a href="#zone-coordinates" id="toc-zone-coordinates" class="nav-link" data-scroll-target="#zone-coordinates">Zone Coordinates</a></li>
  </ul></li>
  <li><a href="#samples-to-bins" id="toc-samples-to-bins" class="nav-link" data-scroll-target="#samples-to-bins">Samples to Bins</a>
  <ul class="collapse">
  <li><a href="#downsampling" id="toc-downsampling" class="nav-link" data-scroll-target="#downsampling">Downsampling</a></li>
  <li><a href="#upsampling" id="toc-upsampling" class="nav-link" data-scroll-target="#upsampling">Upsampling</a></li>
  </ul></li>
  <li><a href="#aggregation" id="toc-aggregation" class="nav-link" data-scroll-target="#aggregation">Aggregation</a></li>
  <li><a href="#visualizing-time-course-data" id="toc-visualizing-time-course-data" class="nav-link" data-scroll-target="#visualizing-time-course-data">Visualizing Time Course Data</a></li>
  <li><a href="#gorilla-provided-coordinates" id="toc-gorilla-provided-coordinates" class="nav-link" data-scroll-target="#gorilla-provided-coordinates">Gorilla Provided Coordinates</a></li>
  <li><a href="#modeling-data" id="toc-modeling-data" class="nav-link" data-scroll-target="#modeling-data">Modeling Data</a>
  <ul class="collapse">
  <li><a href="#cpa" id="toc-cpa" class="nav-link" data-scroll-target="#cpa">CPA</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#replication-of-sarrett2022" id="toc-replication-of-sarrett2022" class="nav-link" data-scroll-target="#replication-of-sarrett2022">Replication of <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span></a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a>
  <ul class="collapse">
  <li><a href="#recruitment-of-l2-speakers" id="toc-recruitment-of-l2-speakers" class="nav-link" data-scroll-target="#recruitment-of-l2-speakers">Recruitment of L2 Speakers</a></li>
  <li><a href="#generalizability-to-other-platforms" id="toc-generalizability-to-other-platforms" class="nav-link" data-scroll-target="#generalizability-to-other-platforms">Generalizability to Other Platforms</a></li>
  <li><a href="#power" id="toc-power" class="nav-link" data-scroll-target="#power">Power</a></li>
  </ul></li>
  <li><a href="#recommendations-and-ways-forward" id="toc-recommendations-and-ways-forward" class="nav-link" data-scroll-target="#recommendations-and-ways-forward">Recommendations and Ways Forward</a>
  <ul class="collapse">
  <li><a href="#prioritize-external-webcams" id="toc-prioritize-external-webcams" class="nav-link" data-scroll-target="#prioritize-external-webcams">Prioritize External Webcams</a></li>
  <li><a href="#optimize-environmental-conditions" id="toc-optimize-environmental-conditions" class="nav-link" data-scroll-target="#optimize-environmental-conditions">Optimize Environmental Conditions</a></li>
  <li><a href="#conduct-a-priori-power-analysis" id="toc-conduct-a-priori-power-analysis" class="nav-link" data-scroll-target="#conduct-a-priori-power-analysis">Conduct a Priori Power Analysis</a></li>
  <li><a href="#collect-detailed-post-experiment-feedback" id="toc-collect-detailed-post-experiment-feedback" class="nav-link" data-scroll-target="#collect-detailed-post-experiment-feedback">Collect Detailed Post-Experiment Feedback</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="L2_VWP_webcam_ET_revised1.docx"><i class="bi bi-file-word"></i>MS Word (apaquarto)</a></li><li><a href="../L2_VWP_webcam_ET_revised1.pdf"><i class="bi bi-file-pdf"></i>PDF (apaquarto)</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<br>

<br>

<section id="title" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research</h1>
<div class="Author">
<br>

<p>Jason Geller<sup>1</sup>, Yanina Prystauka<sup>2</sup>, Sarah E. Colby<sup>3</sup>, and Julia R. Drouin<sup>4</sup></p>
<p><sup>1</sup>Department of Psychology and Neuroscience, Boston College</p>
<p><sup>2</sup>Department of Linguistic, Literary and Aesthetic Studies, University of Bergen</p>
<p><sup>3</sup>Department of Linguistics, University of Ottawa</p>
<p><sup>4</sup>Division of Speech and Hearing Sciences, University of North Carolina at Chapel Hill</p>
</div>
</section>
<section id="author-note" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Author Note</h1>
<p>Jason Geller <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0002-7459-4505</p>
<p>Yanina Prystauka <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0001-8258-2339</p>
<p>Sarah E. Colby <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0002-2956-3072</p>
<p>Julia R. Drouin <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0003-0798-3268</p>
<p>This study was not preregistered. The data and code for this manuscript can be found at https://github.com/jgeller112/L2_VWP_Webcam. The authors have no conflicts of interest to disclose. This work was supported by research start-up funds to JRD.</p>
<p>Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows: <em>Jason Geller</em><strong>: </strong>conceptualization, writing – original draft, data curation, writing – review &amp; editing, software, and formal analysis. <em>Yanina Prystauka</em><strong>: </strong>methodology, writing – review &amp; editing, and formal analysis. <em>Sarah E. Colby</em><strong>: </strong>methodology and writing – review &amp; editing. <em>Julia R. Drouin</em><strong>: </strong>methodology, conceptualization, writing – review &amp; editing, and funding acquisition</p>
<p>Correspondence concerning this article should be addressed to Jason Geller, Department of Psychology and Neuroscience, Boston College, Mcguinn Hall 405, Chestnut Hill, MA 02467-9991, USA, drjasongeller@gmail.com: jason.geller@bc.edu</p>
</section>
<section id="abstract" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Abstract</h1>
<div class="AbstractFirstParagraph">
<p>Eye-tracking has become a valuable tool for studying cognitive processes in second language acquisition and bilingualism (Godfroid et al., 2024). While research-grade infrared eye-trackers are commonly used, several factors limit their widespread adoption. Recently, consumer-based webcam eye-tracking has emerged as an attractive alternative, requiring only a personal webcam and internet access. However, webcam-based eye-tracking introduces unique design and preprocessing challenges that must be addressed to ensure valid results. To help researchers navigate these challenges, we developed a comprehensive tutorial focused on visual world webcam eye-tracking for second language research. This guide covers key preprocessing steps—from reading in raw data to visualization and analysis—highlighting the open-source R package webgazeR, freely available at: https://github.com/jgeller112/webgazer. To demonstrate these steps, we analyze data collected via the Gorilla platform (Anwyl-Irvine et al., 2020) using a single-word Spanish visual world paradigm (VWP), showcasing evidence of competition both within and between Spanish and English. This tutorial aims to empower researchers by providing a step-by-step guide to successfully conduct webcam-based visual world eye-tracking studies. To follow along, please download the complete manuscript, code, and data from: https://github.com/jgeller112/L2_VWP_Webcam.</p>
</div>
<p><em>Keywords</em>: VWP, Tutorial, Webcam eye-tracking, R, Gorilla, Spoken word recognition, L2 processing</p>
</section>
<section id="firstheader" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research</h1>
<p>Eye-tracking technology, which has a history spanning over a century, has seen remarkable advancements. In the early days, eye-tracking often required the use of contact lenses fitted with search coils—sometimes necessitating anesthesia—or the attachment of suction cups to the sclera of the eyes <span class="citation" data-cites="pluzyczka2018">(Płużyczka, 2018)</span>. These methods were not only cumbersome for researchers, but also uncomfortable and invasive for participants. Over time, such approaches have been replaced by non-invasive, lightweight, and user-friendly systems. Today, modern eye-tracking technology is widely accessible in laboratories worldwide, enabling researchers to tackle critical questions about cognitive processes. This evolution has had a profound impact on fields such as psycholinguistics and bilingualism, opening up new possibilities for understanding how language is processed in real time <span class="citation" data-cites="godfroid">(Godfroid et al., 2024)</span>.</p>
<p>In the last decade, there has been a gradual shift towards conducting more behavioral experiments online <span class="citation" data-cites="anderson2019 rodd2024">(Anderson et al., 2019; Rodd, 2024)</span>. This “onlineification” of behavioral research has driven the development of remote eye-tracking methods that do not rely on traditional laboratory settings. Allowing participants to use their own equipment from anywhere in the world opens the door to recruiting more diverse and historically underrepresented populations <span class="citation" data-cites="gosling2010">(Gosling et al., 2010)</span>. Behavioral research has long struggled with a lack of diverse and representative samples, relying heavily on participants who are predominantly Western, Educated, Industrialized, Rich, and Democratic (WEIRD) <span class="citation" data-cites="henrich2010">(Henrich et al., 2010)</span>. Additionally, we propose adding able-bodied to this acronym (WEIRD-A) <span class="citation" data-cites="peterson2021">(Peterson, 2021)</span>, to highlight the exclusion of individuals with disabilities who may face barriers to accessing research facilities. In language research, this issue is especially pronounced, as studies often focus on “modal” listeners and speakers—typically young, monolingual, and neurotypical <span class="citation" data-cites="blasi2022 bylund2024 mcmurray2010">(Blasi et al., 2022; Bylund et al., 2024; McMurray et al., 2010)</span>.</p>
<p>In this paper, we contribute to the growing body of research suggesting that webcam-based eye-tracking, which is administered remotely and requires access to only a computer webcam, can increase inclusivity and representation of the participant samples we include in research studies. Namely, by minimizing the requirements for participants to travel to a lab, use specialized equipment, or meet strict scheduling demands, webcam-based approaches can facilitate participation from individuals in rural or geographically isolated areas and people with disabilities that make getting to a lab difficult. This approach also promotes inclusion of broader sociodemographic groups that have been historically underrepresented in cognitive and developmental research. We illustrate this by replicating a visual world eye-tracking study with bilingual English-Spanish speaking participants <span class="citation" data-cites="sarrett2022">(Sarrett et al., 2022)</span> using online methods (i.e., recruitment via Prolific.co and webcam-based eye-tracking). To facilitate broader adoption of this approach, we also introduce our R package, webgazeR <span class="citation" data-cites="webgazeR">(Geller, 2025)</span>, and present a step-by-step tutorial for analyzing webcam-based VWP data.</p>
<p>This paper is divided into three parts. First, we introduce automated webcam-based eye-tracking. Second, we review the viability of conducting VWP studies using online eye-tracking methods. Third, we present a detailed tutorial for analyzing webcam-based VWP data with the webgazeR package, using our replication experiment to highlight the steps needed for preprocessing.</p>
<section id="webcam-eye-tracking-with-webgazer.js" class="level2">
<h2 data-anchor-id="webcam-eye-tracking-with-webgazer.js">Webcam Eye-Tracking with WebGazer.js</h2>
<p>There are two popular methods for online eye-tracking. One method, manual eye-tracking <span class="citation" data-cites="trueswell2008">(Trueswell, 2008)</span>, involves using video recordings of participants, which can be collected through online teleconferencing platforms such as Zoom (www.zoom.com). Here eye gaze (direction) is manually analyzed post-hoc frame by frame from these recordings. However, this method raises ethical and privacy concerns, as not all participants may be comfortable having their videos recorded and stored for analysis.</p>
<p>Another method, which is the focus of this paper, is automated eye-tracking or webcam eye-tracking. Webcam eye-tracking generally has three requirements for the participant: (1) a personal computer, tablet, or smartphone <span class="citation" data-cites="chen-sankey2023">(see Chen-Sankey et al., 2023)</span>, (2) an internet connection, and (3) a built-in or external camera. Gaze data is collected directly through a web browser without requiring any additional software installation, making it highly accessible.</p>
<p>A popular tool for enabling webcam-based eye-tracking is WebGazer.js <span class="citation" data-cites="papoutsaki2016">(Papoutsaki et al., 2016)</span> <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, an open-source, freely available, and actively maintained JavaScript library. WebGazer.js has already been integrated into several popular experimental platforms, including Gorilla, jsPsych, PsychoPy, Labvanced, and PCIbex <span class="citation" data-cites="anwyl-irvine2020 peirce2019 deleeuw2015 zehr2018penncontroller kaduk2024">(Anwyl-Irvine et al., 2020; Kaduk et al., 2024; Leeuw, 2015; Peirce et al., 2019; Zehr &amp; Schwarz, 2018)</span>. Because WebGazer.js runs locally on the participant’s machine, it does not store webcam video recordings, helping alleviate ethical and privacy concerns associated with online eye-tracking.</p>
<p>Under the hood, WebGazer.js uses machine learning to estimate gaze position in real time by fitting a facial mesh to the participant and detecting the location of the eyes. At each sampling point—determined by the participant’s device and webcam capabilities—x and y gaze coordinates are recorded. To improve accuracy, participants complete calibration and validation routines in which they fixate on targets in specific locations on the screen (in some cases a manual approach is used where users click on targets).</p>
</section>
<section id="eye-tracking-in-the-lab-vs.-online" class="level2">
<h2 data-anchor-id="eye-tracking-in-the-lab-vs.-online">Eye-tracking in the Lab vs.&nbsp;Online</h2>
<p>Several studies in psychology and psycholinguistics have evaluated the viability of WebGazer.js for online research. Generally, lab-based effects can be successfully replicated in online environments using WebGazer.js <span class="citation" data-cites="bogdan2024 bramlett2024 bramlett2025 özsoy2023 slim2023 slim2024 prystauka2024 vos2022 vandercruyssen2024">(Bogdan et al., 2024; Bramlett &amp; Wiener, 2024, 2025; Özsoy et al., 2023; Prystauka et al., 2024; Slim et al., 2024; Slim &amp; Hartsuiker, 2023; Van der Cruyssen et al., 2024; Vos et al., 2022)</span>. However, a critical finding across online replication studies is that effect sizes are often smaller and more variable than those observed in laboratory settings <span class="citation" data-cites="bogdan2024 slim2023 slim2024 vandercruyssen2024">(Bogdan et al., 2024; Slim et al., 2024; Slim &amp; Hartsuiker, 2023; Van der Cruyssen et al., 2024)</span>.</p>
<p>These attenuated effects likely stem from several technical limitations inherent to webcam-based eye-tracking. Unlike research-grade trackers that use infrared illumination and pupil–corneal reflection techniques—and can sample at rates up to 2,000 Hz with sub-degree spatial precision (0.1° to 0.35°) <span class="citation" data-cites="hooge2024 carter2020">(Carter &amp; Luke, 2020; Hooge et al., 2024)</span>—WebGazer.js typically operates at lower frame rates, around 30 Hz <span class="citation" data-cites="bramlett2024 prystauka2024">(Bramlett &amp; Wiener, 2024; Prystauka et al., 2024)</span>. Moreover, the performance of the algorithm is highly dependent on ambient lighting conditions, making it more susceptible to variability introduced by differences in head position, screen brightness, and background contrast.</p>
<p>There are also notable issues with the spatial and temporal accuracy of webcam-based eye-tracking using WebGazer.js. Spatial precision is often lower, with average errors frequently exceeding 1° of visual angle <span class="citation" data-cites="papoutsaki2016">(Papoutsaki et al., 2016)</span>. Temporal delays are also substantially larger, ranging from 200 ms to over 1000 ms <span class="citation" data-cites="semmelmann2018 slim2023 slim2024">(Semmelmann &amp; Weigelt, 2018; Slim et al., 2024; Slim &amp; Hartsuiker, 2023)</span>. Additionally, recent work by <span class="citation" data-cites="bogdan2024">Bogdan et al. (2024)</span> has documented a systematic bias in gaze estimates favoring centrally located stimuli.</p>
</section>
<section id="bringing-the-visual-world-paradigm-vwp-online" class="level2">
<h2 data-anchor-id="bringing-the-visual-world-paradigm-vwp-online">Bringing the Visual World Paradigm (VWP) Online</h2>
<p>Despite these technical challenges, webcam-based eye-tracking has proven particularly well-suited for adapting VWP <span class="citation" data-cites="tanenhaus1995 cooper1974">(Tanenhaus et al., 1995; cf. Cooper, 1974)</span> to online environments.</p>
<p>In the field of language research, few methods have had as enduring an impact as the VWP. Over the past 25 years, the VWP has enabled researchers to address a broad range of topics, including sentence processing <span class="citation" data-cites="altmann1999 huettig2011 kamide2003">(Altmann &amp; Kamide, 1999; Huettig et al., 2011; Kamide et al., 2003)</span>, spoken word recognition <span class="citation" data-cites="allopenna1998 dahan2001 huettig2007 mcmurray2002">(Allopenna et al., 1998; Dahan et al., 2001; Huettig &amp; McQueen, 2007; McMurray et al., 2002)</span>, bilingual language processing <span class="citation" data-cites="hopp2013 ito2018 rossi2019">(Hopp, 2013; Ito et al., 2018; Rossi et al., 2019)</span>, the effects of brain damage on language <span class="citation" data-cites="mirman2012 yee2008">(Mirman &amp; Graziano, 2012; Yee et al., 2008)</span>, and the impact of hearing loss on lexical access <span class="citation" data-cites="mcmurray2017">(McMurray et al., 2017)</span>.</p>
<p>What makes the widespread use of the VWP particularly remarkable is the simplicity of the task. In a typical VWP experiment, participants view a display of several objects, each represented by a picture, while their eye movements are recorded in real time as they listen to a spoken word or phrase. Researchers are commonly interested in the proportion of fixations directed to each image on the screen. Although variations of the task exist—and implementations may differ depending on specific research goals or design choices—the core finding remains consistent: as the speech signal unfolds, listeners initially distribute fixations across phonologically related images (e.g., cohort or rhyme competitors) before ultimately fixating on the image that matches the spoken word. This robust effect provides compelling evidence for anticipatory or predictive processing during language comprehension.</p>
<p>While eye movements are often time-locked to linguistic input, the relationship between eye movements and lexical processing is not one-to-one. Lexical activation interacts with non-lexical factors such as selective attention, visual salience, task demands, working memory, and prior expectations—all of which can shape where and when participants look <span class="citation" data-cites="eberhard1995 huettig2011 kamide2003 bramlett2025">(Bramlett &amp; Wiener, 2025; Eberhard et al., 1995; Huettig et al., 2011; Kamide et al., 2003)</span>. Nonetheless, the VWP remains a powerful and flexible tool for studying online language processing, offering fine-grained insights into how linguistic and cognitive processes unfold moment by moment.</p>
<p>Several attempts have been made to conduct these experiments online using webcam-based eye-tracking. Most online VWP replications have focused on sentence-based language processing. These studies have looked at effects of set size and determiners <span class="citation" data-cites="degen2021">(Degen et al., 2021)</span>, verb semantic constraint <span class="citation" data-cites="prystauka2024 slim2023">(Prystauka et al., 2024; Slim &amp; Hartsuiker, 2023)</span>, grammatical aspect and event comprehension <span class="citation" data-cites="vos2022">(Vos et al., 2022)</span>, and lexical interference <span class="citation" data-cites="prystauka2024">(Prystauka et al., 2024)</span>.</p>
<p>More relevant to the current tutorial are findings from single-word VWP studies conducted online. Recent research examined single-word speech perception online using a phonemic cohort task <span class="citation" data-cites="slim2024 bramlett2025">(Bramlett &amp; Wiener, 2025; Slim et al., 2024)</span>. In the cohort task, pictures were displayed randomly in one of four quadrants, and participants were instructed to fixate on the target based on the auditory cue. On each trial, one of the pictures was phonemically similar to the target in onset (e.g., <em>MILK</em> – <em>MITTEN</em>). <span class="citation" data-cites="slim2024">Slim et al. (2024)</span> were able to observe significant fixations to the cohort compared to the control condition, replicating lab-based single word VWP experiments with research grade eye-trackers <span class="citation" data-cites="allopenna1998">(e.g., Allopenna et al., 1998)</span>. However, time course differences were observed in the webcam-based setting such that competition effects occurred later in processing compared to traditional, lab-based eye-tracking.</p>
<p>Several factors have been proposed to explain the poor temporal performance in the VWP. These include reduced spatial precision, computational demands introduced by the WebGazer.js algorithm, slower internet connections, smaller areas of interest (AOIs), and calibration quality <span class="citation" data-cites="degen2021 slim2024 vanboxtel2024">(Boxtel et al., 2024; Degen et al., 2021; Slim et al., 2024)</span>.</p>
<p>Importantly, temporal issues are not observed in every case. Work has begun to address many of these challenges by leveraging updated versions of WebGazer.js and adopting different experimental platforms. For instance, <span class="citation" data-cites="vos2022">Vos et al. (2022)</span> reported a substantial reduction in temporal delays—approximately 50 ms—when using a newer version of WebGazer.js embedded within the jsPsych framework <span class="citation" data-cites="deleeuw2015">(Leeuw, 2015)</span>. Similarly, studies by <span class="citation" data-cites="prystauka2024">Prystauka et al. (2024)</span> and <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (2024)</span>, which utilized the Gorilla Experiment Builder in combination with the improved WebGazer algorithm, found timing and competition effects closely aligned with those observed in traditional lab-based VWP studies.</p>
<p>While these temporal delays do present a challenge, and are at present an open issue, the general findings that WebGazer.js can approximate looks to areas on the screen and replicate lab-based findings underscore the potential of adapting the VWP to online environments using webcam-based eye-tracking. Importantly, recent studies demonstrate that this approach can successfully capture key psycholinguistic effects—such as lexical competition during single-word speech recognition—in a manner comparable to traditional lab-based methods <span class="citation" data-cites="slim2024">(Slim et al., 2024)</span>.</p>
</section>
<section id="bilingual-competition-a-visual-world-webcam-eye-tracking-replication" class="level2">
<h2 data-anchor-id="bilingual-competition-a-visual-world-webcam-eye-tracking-replication">Bilingual Competition: A Visual World Webcam Eye-Tracking Replication</h2>
<p>A goal of the present study was to conceptually replicate a study by <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> wherein they examined the competitive dynamics of second-language (L2) learners of Spanish, whose first language (L1) is English, during spoken word recognition. Specifically, we investigated both within-language and cross-language (L2/L1) competition using webcam-based eye-tracking.</p>
<p>It is well established that lexical competition plays a central role in language processing <span class="citation" data-cites="magnuson2007">(Magnuson et al., 2007)</span>. During spoken word recognition, as the auditory signal unfolds over time, multiple lexical candidates—or competitors—can become partially activated. Successful recognition depends on resolving this competition by inhibiting or suppressing mismatching candidates. For example, upon hearing the initial segments of the word <em>wizard</em>, phonologically similar words such as <em>whistle</em> (cohort competitor) may be briefly activated. As the word continues to unfold, additional competitors like <em>blizzard</em> (a rhyme competitor) might also become active. For <em>wizard</em> to be accurately recognized, activation of competitors such as <em>whistle</em> and <em>blizzard</em> must ultimately be suppressed.</p>
<p>One important area of exploration concerns lexical competition across languages. There is growing evidence that lexical competition can occur cross-linguistically <span class="citation" data-cites="ju2004 spivey1999">(see Ju &amp; Luce, 2004; Spivey &amp; Marian, 1999)</span>. In a recent study, <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> investigated whether cross-linguistic competition arises in unbalanced L2 Spanish speakers—that is, individuals who acquired Spanish later in life. They used carefully controlled stimuli to examine both within-language and cross-language competition in adult L2 Spanish learners. Using a Spanish-language visual world paradigm, their study included two critical conditions:</p>
<ol type="1">
<li><p>Spanish-Spanish (within) condition: A Spanish competitor was presented alongside the target word. For example, if the target word spoken was <em>cielo</em> (sky), the Spanish competitor was <em>ciencia</em> (science).</p></li>
<li><p>Spanish-English (cross-ligustic) condition: An English competitor was presented for the Spanish target word. For example, if the target word spoken was <em>botas</em> (boots), the English competitor was <em>border</em>.</p></li>
</ol>
<p><span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> also included a no competition condition where the Spanish-English pairs were not cross-linguistic competitors (e.g., <em>frontera</em> as the target word and <em>botas</em> - <em>boots</em> as an unrelated item in the pair). They observed competition effects in both of the critical conditions: within (e.g., <em>cielo</em> - <em>ciencia</em>) and between (e.g., <em>botas</em> - <em>border</em>). Herein, we collected data to conceptually replicate their pattern of findings using a webcam approach.</p>
<p>There are two key differences between our dataset and the original study by <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> worth noting. First, <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> focused on adult unbalanced L2 Spanish speakers and posed more fine-grained questions about the time course of competition and resolution and its relationship with L2 language acquisition. Second, unlike <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> , who measured Spanish proficiency objectively using LexTALE-esp <span class="citation" data-cites="izura2014">(Izura et al., 2014)</span>) and ran this study using participants from a Spanish college course, we relied on participant filtering on Prolific (www.prolific.co) to recruit L2 Spanish speakers.</p>
<p>To conduct our online webcam replication, we used the experimental platform Gorilla <span class="citation" data-cites="anwyl-irvine2020">(Anwyl-Irvine et al., 2020)</span>, which integrates WebGazer.js for gaze tracking. We selected Gorilla because it offers robust WebGazer.js integration and seems to address several temporal accuracy concerns identified in other platforms <span class="citation" data-cites="slim2023 slim2024">(Slim et al., 2024; Slim &amp; Hartsuiker, 2023)</span>.</p>
</section>
<section id="tutorial-overview" class="level2">
<h2 data-anchor-id="tutorial-overview">Tutorial Overview</h2>
<p>This paper has two aims. First, we aim to provide evidence for lexical competition within and across languages in L2 Spanish speakers, using webcam-based eye-tracking with WebGazer.js. While there is growing interest in using VWP using webcam-based methods, lexical competition in single-word L2 processing has not yet been investigated using the online version of the VWP, making this a novel application. We hope that this work encourages researchers to explore more detailed questions about L2 processing using webcam-based eye-tracking.</p>
<p>Second, we offer a tutorial that outlines key preprocessing steps for analyzing webcam-based eye-tracking data. Building on recommendations proposed by <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (2024)</span>, our contribution focuses on data preprocessing— transforming raw gaze data into a format suitable for visualization and analysis. Here we introduce a new R package—<code>webgazeR</code><span class="citation" data-cites="webgazeR">(Geller, 2025)</span>—designed to streamline and standardize preprocessing for webcam-based eye-tracking studies. We believe that offering multiple, complementary resources enhances methodological transparency and supports broader adoption of webcam-based eye-tracking methods. For in-depth guidance on experimental design considerations, we refer readers to <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (2024)</span>.</p>
<p>Although <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (2024)</span>’s tutorial provides a lot of useful code, the experiment-specific nature of the code may pose challenges for newcomers. In contrast, the <code>webgazeR</code> package offers a modular, generalizable approach. It includes functions for importing raw data, filtering and visualizing sampling rates, extracting and assigning areas of interest (AOIs), downsampling and upsampling gaze data, interpolating and smoothing time series, and performing non-AOI-based analyses such as intersubject correlation (ISC), a method increasingly used to explore gaze synchrony in naturalistic paradigms (i.e., online learning) with webcam-based eye-tracking <span class="citation" data-cites="madsen2021">(Madsen et al., 2021)</span>.</p>
<p>We first begin by outlining the general methods used to conduct our webcam-based visual world experiment. Second, we detail the data preprocessing steps needed to prepare the data for analysis using <code>webgazeR</code>. Third, we demonstrate a statistical approach for analyzing the preprocessed data, highlighting its application and implications.</p>
<p>To promote transparency and reproducibility, all analyses were conducted in R <span class="citation" data-cites="R">(R Core Team, 2024)</span> using Quarto <span class="citation" data-cites="Allaire_Quarto_2024">(Allaire et al., 2024)</span>, an open-source publishing system that enables dynamic and reproducible documents. Figures, tables, and text are generated programmatically and embedded directly in the manuscript, ensuring seamless integration of results. To further enhance computational reproducibility, we employed the <code>rix</code> package <span class="citation" data-cites="rix">(Rodrigues &amp; Baumann, 2025)</span>, which leverages the Nix ecosystem <span class="citation" data-cites="nix">(Dolstra &amp; contributors, 2023)</span>. This approach captures not only the R package versions but also system dependencies at runtime. Researchers can reproduce the exact computational environment by installing the Nix package manager and using the provided <code>default.nix</code> file. Detailed setup instructions are included in the README file of the accompanying GitHub repository. A video tutorial is also provided.</p>
</section>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<p>All tasks herein can be previewed here <a href="https://app.gorilla.sc/openmaterials/953693">(https://app.gorilla.sc/openmaterials/953693</a>). The manuscript, data, and R code can be found on Github (<a href="https://github.com/jgeller112/webcam_gazeR_VWP" class="uri">https://github.com/jgeller112/webcam_gazeR_VWP</a>).</p>
<section id="participants" class="level2">
<h2 data-anchor-id="participants">Participants</h2>
<p>Participants were recruited through Prolific (www.prolific.co, 2024), an online participant recruitment platform. Our goal was to approximately double the sample size of Sarrett et al.&nbsp;(2022) to enhance statistical power and ensure greater generalizability of the findings. However, due to practical constraints and the challenges associated with online webcam eye-tracking (e.g., calibration failures) and also the limited pool of bilingual Spanish speakers, we were unable to achieve the targeted usable sample size. Therefore, we report the final sample based on all participants who met our predefined inclusion criteria.</p>
<p>Inclusion criteria required participants to: (1) be between 18 and 36 years old, (2) be native English speakers, (3) also be fluent in Spanish, and (4) reside in the United States. Criterion 1 was based on findings from <span class="citation" data-cites="colby2023">Colby and McMurray (2023)</span>, which suggest that age-related changes in spoken word recognition begin to emerge in individuals in their 40s; thus, we limited our sample to participants younger than 36. Criteria 2 and 3 ensured that we were recruiting native English speakers and those fluent in Spanish to test L1 and L2 interactions. Criterion 4 matched the population of the original study, which was conducted with university students in Iowa, and therefore we restricted recruitment to U.S. residents.</p>
<p>After agreeing to participate, individuals were redirected to the Gorilla experiment platform (www.gorilla.sc; <span class="citation" data-cites="anwyl-irvine2020">(Anwyl-Irvine et al., 2020)</span>). A flow diagram of participant progression through the experiment is shown in <a href="#fig-sankey" class="quarto-xref" aria-expanded="false">Figure&nbsp;1</a>. In total, 187 participants assessed the experimental platform and consented to be in the study. Of these, 121 passed the headphone screener checkpoint, and 111 proceeded to the VWP task. Out of the 111 participants who entered the VWP, 91 completed the final surveys at the end of the experiment. Among these, 32 participants successfully completed the VWP task with at least 100 trials, while 79 participants did not provide adequate data for inclusion, primarily due to failed calibration attempts. After applying additional exclusion criteria—namely, overall VWP task accuracy below 80%, excessive missing eye-tracking data (&gt;30%), and sampling rate &lt; 5hz —the final analytic sample consisted of 28 participants with usable eye-tracking data. Descriptive demographic information for the full sample that made it to the final survey is provided in <a href="#tbl-demo2" class="quarto-xref" aria-expanded="false">Table&nbsp;1</a>.</p>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-sankey" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="1">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-sankey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>This sankey plot illustrates the flow of participants from initial consent (<em>N</em> = 187) through each stage of the study to the final analyzed sample (N = 28). The width of each stream is proportional to the number of participants. Detours indicate points of attrition, including failures in the headphone screener (<em>N</em> = 66) and calibration (<em>N</em> = 76). Only participants who passed all screening and calibration stages, and completed the Visual World Paradigm (VWP), were included in the final sample.</p>
</div>
</figcaption>
<div aria-describedby="fig-sankey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/snakey_experiment.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="1" data-custom-style="FigureWithoutNote">
<div id="tbl-demo2" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="1">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-demo2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant demographic variables</p>
</div>
</figcaption>
<div aria-describedby="tbl-demo2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-b6104b16{table-layout:auto;}.cl-b60989ac{font-family:'Times New Roman';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b60989b6{font-family:'Times New Roman';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-b60989c0{font-family:'Times New Roman';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b60989c1{font-family:'Times New Roman';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b60989ca{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b60c54ca{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b60c54d4{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b60c54de{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:2pt;padding-right:2pt;line-height: 1;background-color:transparent;}.cl-b60c54df{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:2pt;padding-right:2pt;line-height: 1;background-color:transparent;}.cl-b60c54e8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b60c7464{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7465{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c746e{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c746f{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7478{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7479{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c747a{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7482{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7483{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7484{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c748c{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c748d{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c748e{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7496{background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7497{background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7498{background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c7499{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b60c74a0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-b6104b16"><thead><tr style="overflow-wrap:break-word;"><th class="cl-b60c7464"><p class="cl-b60c54ca"><span class="cl-b60989ac">Characteristic</span></p></th><th class="cl-b60c7465"><p class="cl-b60c54d4"><span class="cl-b60989ac">N = 91</span><span class="cl-b60989b6">1</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c0">Age</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">(20.0, 35.0), 28.2(4.4)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c7478"><p class="cl-b60c54de"><span class="cl-b60989c0">Gender</span></p></td><td class="cl-b60c7479"><p class="cl-b60c54df"><span class="cl-b60989c1"></span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Female</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">42 / 91 (46%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Male</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">49 / 91 (54%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c747a"><p class="cl-b60c54de"><span class="cl-b60989c0">Spoken dialect</span></p></td><td class="cl-b60c7482"><p class="cl-b60c54df"><span class="cl-b60989c1"></span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Do not know</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">11 / 91 (12%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Midwestern</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">19 / 91 (21%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c7483"><p class="cl-b60c54de"><span class="cl-b60989c1">New England</span></p></td><td class="cl-b60c7484"><p class="cl-b60c54df"><span class="cl-b60989c1">11 / 91 (12%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c7483"><p class="cl-b60c54de"><span class="cl-b60989c1">Other (please specify)</span></p></td><td class="cl-b60c7484"><p class="cl-b60c54df"><span class="cl-b60989c1">7 / 91 (7.7%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Pacific northwest</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">7 / 91 (7.7%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Pacific southwest</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">7 / 91 (7.7%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Southern</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">21 / 91 (23%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Southwestern</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">8 / 91 (8.8%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c748c"><p class="cl-b60c54de"><span class="cl-b60989c0">Ethnicity</span></p></td><td class="cl-b60c748d"><p class="cl-b60c54df"><span class="cl-b60989c1"></span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Decline to state</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">1 / 91 (1.1%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Hispanic or Latino</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">38 / 91 (42%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Not Hispanic or Latino</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">52 / 91 (57%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c748e"><p class="cl-b60c54de"><span class="cl-b60989c0">Race</span></p></td><td class="cl-b60c7496"><p class="cl-b60c54df"><span class="cl-b60989c1"></span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">American Indian/Alaska Native</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">2 / 91 (2.2%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Asian</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">13 / 91 (14%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Black or African American</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">10 / 91 (11%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Decline to state</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">7 / 91 (7.7%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">More than one race</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">4 / 91 (4.4%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">White</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">55 / 91 (60%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c748e"><p class="cl-b60c54de"><span class="cl-b60989c0">Browser</span></p></td><td class="cl-b60c7496"><p class="cl-b60c54df"><span class="cl-b60989c1"></span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Chrome </span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">77 / 91 (85%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c7483"><p class="cl-b60c54de"><span class="cl-b60989c1">Edge </span></p></td><td class="cl-b60c7484"><p class="cl-b60c54df"><span class="cl-b60989c1">3 / 91 (3.3%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Firefox </span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">7 / 91 (7.7%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c1">Safari </span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">4 / 91 (4.4%)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c746e"><p class="cl-b60c54de"><span class="cl-b60989c0">Years Speaking Spanish</span></p></td><td class="cl-b60c746f"><p class="cl-b60c54df"><span class="cl-b60989c1">(0, 35), 15(10)</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b60c7497"><p class="cl-b60c54de"><span class="cl-b60989c0">% Experience Using Spanish Daily Life</span></p></td><td class="cl-b60c7498"><p class="cl-b60c54df"><span class="cl-b60989c1">25(23)</span></p></td></tr></tbody><tfoot><tr style="overflow-wrap:break-word;"><td colspan="2" class="cl-b60c7499"><p class="cl-b60c54e8"><span class="cl-b60989b6">1</span><span class="cl-b60989ca">(Min, Max), Mean(SD); n / N (%); Mean(SD)</span></p></td></tr></tfoot></table></div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="materials" class="level2">
<h2 data-anchor-id="materials">Materials</h2>
<section id="vwp" class="level3">
<h3 data-anchor-id="vwp">VWP</h3>
<section id="items" class="level4">
<h4 data-anchor-id="items">Items.</h4>
<p>We adapted materials from <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span>. In their cross-linguistic VWP, participants were presented with four pictures and a spoken Spanish word and had to select the image that matched the spoken word by clicking on it. The word stimuli for the experiment were chosen from textbooks used by students in their first and second year college Spanish courses.</p>
<p>The item sets consisted of two types of phonologically-related word pairs: one pair of Spanish-Spanish words and another of Spanish-English words. The Spanish-Spanish pairs were unrelated to the Spanish-English pairs. All the word pairs were carefully controlled on a number of dimensions <span class="citation" data-cites="sarrett2022">(see Sarrett et al., 2022)</span>. There were three experimental conditions: (1) the Spanish-Spanish (within) condition, where one of the Spanish words was the target and the other was the competitor; (2) the Spanish-English (cross-linguistic) condition, where a Spanish word was the target and its English phonological cohort served as the competitor; and (3) the No Competitor condition, where the Spanish word did not overlap with any other word in the set. The Spanish-Spanish condition had twice as many trials as the other conditions due to the interchangeable nature of the target and competitor words in that pair.</p>
<p>Each item within a set appeared four times as the target word, resulting in a total of 240 trials (15 sets × 4 items per set × 4 repetitions). Each set included one Spanish–Spanish cohort pair and one Spanish–English cohort pair. In the Spanish–Spanish condition, both words in the pair served as mutual competitors—for example, <em>cielo</em> activated <em>ciencia</em>, and vice versa. This bidirectional relationship yielded 120 trials for the Spanish–Spanish condition.</p>
<p>In contrast, the Spanish–English pairs had an asymmetrical relationship: only one item in each pair functioned as a competitor (e.g., <em>botas</em> could activate <em>frontera</em>, but <em>frontera</em> did not have a corresponding competitor). As a result, there were 60 trials each for the Spanish–English and No Competitor conditions. Across all trials, target items were equally distributed among the four screen quadrants to ensure balanced visual presentation</p>
</section>
<section id="stimuli" class="level4">
<h4 data-anchor-id="stimuli">Stimuli.</h4>
<p>In <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> all auditory stimuli were recorded by a female bilingual speaker whose native language was Mexican Spanish and also spoke English. Stimuli were recorded in a sound-attenuated room sampled at 44.1 kHz. Auditory tokens were edited to reduce noise and remove clicks. The auditory tokens were then amplitude normalized to 70 dB SPL. For each target word, there were four separate recordings so each instance was unique.</p>
<p>Visual stimuli were images from a commercial clipart database that were selected by a consensus method involving a small group of students. All .wav files were converted to .mp3 for online data collection. All stimuli can be found here: <a href="https://osf.io/mgkd2/" class="uri">https://osf.io/mgkd2/</a>.</p>
</section>
</section>
<section id="headphone-screener" class="level3">
<h3 data-anchor-id="headphone-screener">Headphone Screener</h3>
<p>Headphones were required for all participants. To ensure compliance, we administered a six-trial headphone screening task adapted from <span class="citation" data-cites="milne2021">Milne et al. (2021)</span>, which is available for implementation on the Gorilla platform. On each trial, three tones of the same frequency and duration were presented sequentially. One tone had a lower amplitude than the other two tones. Tones were presented in stereo, but the tones in the left and right channels were 180 out of phase across stereo channels—in free field, these sounds should cancel out or create distortion, whereas they will be perfectly clear over headphones. The listener picked which of the three tones was the quietest. Performance is generally at the ceiling when wearing headphones but poor when listening in the free field (due to phase cancellation).</p>
</section>
<section id="participant-background-and-experiment-conditions-questionnaire" class="level3">
<h3 data-anchor-id="participant-background-and-experiment-conditions-questionnaire">Participant Background and Experiment Conditions Questionnaire</h3>
<p>We had participants complete a demographic questionnaire as part of the study. The questions covered basic demographic information, including age, gender, spoken dialect, ethnicity, and race. To gauge L2 experience, we asked participants when they started speaking Spanish, how many years of Spanish speaking experience they had, and to provide, on a scale between 0-100, how often they use Spanish in their daily lives.</p>
<p>To further probe into data quality issues and get a better sense of why participants could not make it through the experiment, participants answered a series of questions at the end of the experiment related to their personal health and environmental conditions during the experiment. These questions addressed any history of vision problems (e.g., corrected vision, eye disease, or drooping eyelids) and whether they were currently taking medications that might impair judgment. Participants also indicated if they were wearing eyeglasses, contacts, makeup, false eyelashes, or hats.</p>
<p>The questionnaire asked about natural light in the room, if they were using a built-in camera or an external one (with an option to specify the brand), and their estimated distance from the camera. Participants were asked to estimate how many times they looked at their phone or got up during the experiment and whether their environment was distraction-free.</p>
<p>Additional questions assessed the clarity of calibration instructions, allowing participants to suggest improvements, and asked if they were wearing a mask during the session. These questions aimed to gather insights into personal and environmental factors that could impact data quality and participant comfort during the experiment.</p>
</section>
<section id="procedure" class="level3">
<h3 data-anchor-id="procedure">Procedure</h3>
<p>All tasks and questionnaires were developed using the Gorilla Experiment Builder’s graphical user interface (GUI) and integrated coding tools <span class="citation" data-cites="anwyl-irvine2020">(Anwyl-Irvine et al., 2020)</span>. Each participant completed the study in a single session lasting approximately 45 minutes. Tasks were presented in a fixed order: informed consent, headphone screening, the spoken word Visual World Paradigm (VWP) task, and a set of questionnaire items. These are available to view here: <a href="https://app.gorilla.sc/openmaterials/953693" class="uri">https://app.gorilla.sc/openmaterials/953693</a>.</p>
<p>Only personal computers were permitted for participation. Upon entering the study from Prolific, participants were presented with a consent form. Once consent was given, participants completed a headphone screening test. They had three attempts to pass this test. If unsuccessful by the third attempt, participants were directed to an early exit screen, followed by the questionnaire. They had three attempts to pass this test. If unsuccessful by the third attempt, participants were directed to an early exit screen, followed by the questionnaire.</p>
<p>If the headphone screener was passed, participants were next introduced to the VWP task. This began with instructional videos providing specific guidance on the ideal experiment setup for eye-tracking and calibration procedures. You can view the videos here: <a href="https://osf.io/mgkd2/" class="uri">https://osf.io/mgkd2/</a>. Participants were then required to enter full-screen mode before calibration. A 9-point calibration procedure was used. Calibration occurred every 60 trials for a total of 3 calibrations. Participants had three attempts to successfully complete each calibration phase. If calibration was unsuccessful, participants were directed to an early exit screen, followed by the questionnaire.</p>
<p>In the main VWP task, each trial began with a 500 ms fixation cross at the center of the screen. This was followed by a preview screen displaying four images, each positioned in a corner of the screen. After 1500 ms, a start button appeared in the center. Participants clicked the button to confirm they were focused on the center before the audio played. Once clicked, the audio was played, and the images remained visible. Participants were instructed to click the image that best matched the spoken target word, while their eye movements were recorded. Eye movements were only recorded on that screen. <a href="#fig-vwptrial" class="quarto-xref" aria-expanded="false">Figure&nbsp;2</a> displays the VWP trial sequence.</p>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-vwptrial" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="2">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-vwptrial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;2</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>VWP trial schematic</p>
</div>
</figcaption>
<div aria-describedby="fig-vwptrial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Figure1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>After completing the main VWP task, participants proceeded to the final questionnaire, which included questions about the eye-tracking task and basic demographic information. Participants were then thanked for their participation.</p>
</div>
</section>
</section>
</section>
<section id="preprocessing-data" class="level1">
<h1>Preprocessing Data</h1>
<p>After the data is collected you can begin preprocessing your data. Below we highlight the steps needed to preprocess your webcam eye-tracking data and get it ready for analysis. For some of this preprocessing we will use the newly created <code>webgazeR</code> package (v. 0.7.2).</p>
<p>For preprocessing visual world webcam eye data, we follow seven general steps (see <a href="#fig-preprco" class="quarto-xref" aria-expanded="false">Figure&nbsp;3</a>):</p>
<ol type="1">
<li><p>Reading in data</p></li>
<li><p>Data exclusion</p></li>
<li><p>Combining trial- and eye-level data</p></li>
<li><p>Assigning areas of interest (AOIs)</p></li>
<li><p>Time binning</p>
<ol type="1">
<li><p>Downsampling</p></li>
<li><p>Upsampling (optional)</p></li>
</ol></li>
<li><p>Aggregating (optional)</p></li>
<li><p>Visualization</p></li>
</ol>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-preprco" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="3">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-preprco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;3</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Preprocessing steps for webcam eye-tracking data using webgazeR functions</p>
</div>
</figcaption>
<div aria-describedby="fig-preprco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/preprocessing_fig1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>For each of these steps, we will display R code chunks demonstrating how to perform each step with helper functions (if applicable) from the <code>webgazeR</code> <span class="citation" data-cites="webgazeR">(Geller, 2025)</span> package in R.</p>
</div>
<section id="load-packages" class="level2">
<h2 data-anchor-id="load-packages">Load Packages</h2>
<section id="package-installation-and-setup" class="level3">
<h3 data-anchor-id="package-installation-and-setup">Package Installation and Setup</h3>
<p>Before proceeding, make sure to load the required packages by running the code below. If you already have these packages installed and loaded, feel free to skip this step. The code in this tutorial will not run correctly if any of the necessary packages are missing or not properly loaded.</p>
<section id="webgazer-installation" class="level4">
<h4 data-anchor-id="webgazer-installation">webgazeR Installation.</h4>
<p>The <code>webgazeR</code> package is installed from the Github repository using the <code>remotes</code> <span class="citation" data-cites="remotes">(Csárdi et al., 2024)</span> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(remotes) <span class="co"># install github repo</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">"jgeller112/webgazeR"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once this is installed, <code>webgazeR</code> can be loaded along with additional useful packages. The following code will load the required packages or install them if you do not have them on your system.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># List of required packages</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>required_packages <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"tidyverse"</span>,      <span class="co"># data wrangling</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">"here"</span>,           <span class="co"># relative paths instead of absolute aids in reproducibility</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"tinytable"</span>,      <span class="co"># nice tables</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"janitor"</span>,        <span class="co"># functions for cleaning up your column names</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">"webgazeR"</span>,       <span class="co"># has webcam functions</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">"readxl"</span>,         <span class="co"># read in Excel files</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">"ggokabeito"</span>,     <span class="co"># color-blind friendly palettes</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="st">"flextable"</span>,      <span class="co"># Word tables</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="st">"permuco"</span>,        <span class="co"># permutation analysis</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">"foreach"</span>,        <span class="co"># permutation analysis</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="st">"geomtextpath"</span>,   <span class="co"># for plotting labels on lines of ggplot figures</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"cowplot"</span>         <span class="co"># combine ggplot figures</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once <code>webgazeR</code> and other helper packages have been installed and loaded the user is ready to start cleaning your data.</p>
</section>
</section>
</section>
<section id="reading-in-data" class="level2">
<h2 data-anchor-id="reading-in-data">Reading in Data</h2>
<section id="behavioral-trial-level-data" class="level3">
<h3 data-anchor-id="behavioral-trial-level-data">Behavioral, Trial-level, Data</h3>
<p>To process eye-tracking data you will need to make sure you have both the behavioral data and the eye-tracking data files. We have all the data needed in the repository by navigating to the L2 subfolder from the main project directory (~/data/L2). For the behavioral data, Gorilla produces a <code>.csv</code> file that includes trial-level information (here contained in the object <code>L2_data)</code>. The files needed are called <code>data_exp_196386-v5_task-scf6.csv</code>. and <code>data_exp_196386-v6_task-scf6.csv</code>. We have two files because we ran a modified version of the experiment.</p>
<p>The .csv files contain meta-data for each each trial, such as what picture were presented on each trial, which object was the target, reaction times, audio presentation times, what object was clicked on, etc. To load our data files into our R environment, we use the <code>here</code> <span class="citation" data-cites="here">(Müller, 2020)</span> package to set a relative rather than an absolute path to our files. We read in the data files from the repository for both versions of the task and merge the files together. <code>L2_data</code> merges both <code>data_exp_196386-v5_task-scf6.csv</code> and <code>data_exp_196386-v6_task-scf6.csv</code> into one object.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in trial level data </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># combine data from version 5 and 6 of the task</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>L2_1 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"data_exp_196386-v5_task-scf6.csv"</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>L2_2 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"data_exp_196386-v6_task-scf6.csv"</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>L2_data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(L2_1, L2_2) <span class="co"># bind the two objects together </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="eye-tracking-data" class="level3">
<h3 data-anchor-id="eye-tracking-data">Eye-Tracking Data</h3>
<p>Gorilla currently saves each participant’s eye-tracking data on a per-trial basis. The <code>raw</code> subfolder in the project repository contains the eye-tracking files by participant for each trial individually (~/data/L2/raw). Contained in those files, we have information pertaining to each trial such as participant id, time since trial started, x and y coordinates of looks, convergence (the model’s confidence in finding a face (and accurately predicting eye movements), face confidence (represents the support vector machine (SVM) classifier score for the face model fit), and information pertaining to the the AOI screen coordinates (standardized and user-specific). The <code>vwp_files_L2</code> object below contains a list of all the files contained in the folder. Because <code>vwp_files_L2</code> contains trial data as well as calibration data, we remove the calibration trials and save the non-calibration to to <code>vwp_paths_filtered_L2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the list of all files in the folder</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># thank you to Reviewer 1 for suggesting this code</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>vwp_files_L2 <span class="ot">&lt;-</span> <span class="fu">list.files</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"raw"</span>), <span class="at">full.names =</span> <span class="cn">TRUE</span>, <span class="at">pattern =</span> <span class="st">"</span><span class="sc">\\</span><span class="st">.(csv|xlsx)$"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># remove calibration trials </span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">discard</span>(<span class="sc">~</span> <span class="fu">grepl</span>(<span class="st">"calibration"</span>, .x)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When data is generated from Gorilla, each trial in your experiment is saved as a separate file. To analyze the data, these individual files need to be combined into a single dataset. The <code>merge_webcam_files()</code> function from webgazeR is designed for this purpose. It reads all trial-level files from a specified folder—regardless of file format (.csv, .tsv, or .xlsx)—and merges them into one cohesive tibble or data frame.</p>
<p>Before using <code>merge_webcam_files()</code>, ensure your working directory is set to the location where the raw files are stored. The function automatically standardizes column names using clean_names(), binds the files together, and filters the data to retain only the relevant rows. Specifically, it keeps rows where the type column equals “prediction”, which are the rows that contain actual eye-tracking predictions. It also filters based on the screen_index argument: if you collected gaze data across multiple screens, you can specify one or several indices (e.g., screen_index = c(1, 4, 5)).</p>
<p>In addition to merging and filtering, <code>merge_webcam_files()</code> requires the user to explicitly map critical columns—subject, trial, time, and x/y gaze coordinates. This makes the function highly flexible and robust across different experimental platforms. For instance, the function automatically renames the spreadsheet_row column to trial, and converts subject and trial into factors for compatibility with downstream analyses.</p>
<p>Currently, the kind argument supports “gorilla” data, but future extensions will add support for other platforms like Labvanced <span class="citation" data-cites="kaduk2024">(Kaduk et al., 2024)</span>, PsychoPy <span class="citation" data-cites="peirce2019">(Peirce et al., 2019)</span>, and PCIbex <span class="citation" data-cites="zehr2018penncontroller">(Zehr &amp; Schwarz, 2018)</span>. By explicitly allowing platform specification and flexible column mapping, merge_webcam_files() ensures a consistent and streamlined pipeline for preparing webcam eye-tracking data for analysis.</p>
<p>As a general note, all steps should be followed in order due to the renaming of column names. If you encounter an error it might be because column names have not been changed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setwd</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"raw"</span>)) <span class="co"># set working directory to raw data folder</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>edat_L2 <span class="ot">&lt;-</span> <span class="fu">merge_webcam_files</span>(vwp_files_L2, <span class="at">screen_index=</span><span class="dv">4</span>, <span class="at">col_map =</span> <span class="fu">list</span>(<span class="at">subject =</span> <span class="st">"participant_id"</span>, <span class="at">trial=</span><span class="st">"spreadsheet_row"</span>, <span class="at">time=</span><span class="st">"time_elapsed"</span>, <span class="at">x=</span><span class="st">"x_pred_normalised"</span>, <span class="at">y=</span><span class="st">"y_pred_normalised"</span>), <span class="at">kind=</span><span class="st">"gorilla"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To ensure high-quality data, we applied a set of behavioral and eye-tracking exclusion criteria prior to merging datasets. Participants were excluded if they met any of the following conditions: (1) failure to successfully calibrate throughout the experiment (fewer than 100 completed trials), (2) low behavioral accuracy (below 80%), (3) low sampling rate (below 5 Hz), or (4) a high proportion of gaze samples falling outside the display area (greater than 30%).</p>
<p>Successful calibration is critical for reliable eye-tracking measurements, as poor calibration directly compromises the spatial accuracy of gaze data <span class="citation" data-cites="blascheck2017">(Blascheck et al., 2017)</span>. Requiring a sufficient number of completed trials is crucial for ensuring adequate statistical power and stable individual-level parameter estimates, particularly in tasks with high trial-to-trial variability <span class="citation" data-cites="brysbaert2018">(Brysbaert &amp; Stevens, 2018)</span>. We choose 100 trials as this meant participants passed at least two calibration attempts during the study. Behavioral accuracy ( &gt;= 80%) was used as an additional screening measure because low task performance may indicate a lack of attention, misunderstanding of the task, or random responding, all of which could undermine both the behavioral and eye-movement data quality <span class="citation" data-cites="bianco2021">(Bianco et al., 2021)</span>. Filtering based on sampling rate ensures that datasets with too few gaze samples (due to technical or environmental issues) are removed, as low sampling rates significantly degrade temporal precision and bias gaze metrics <span class="citation" data-cites="semmelmann2018">(Semmelmann &amp; Weigelt, 2018)</span>. Finally, we excluded participants with excessive off-screen data (&gt;30%) because this indicates poor gaze tracking, likely caused by head movement, poor lighting, or loss of face detection. At this time, there is no set guide on what constitutes acceptable data loss for webcam-based studies. We felt 30% was a reasonable cut-off. At the trial-level, we also removed incorrect trials and trials where sampling rate was &lt; 5 Hz.</p>
<p>What we will do first is create a cleaned up version of our behavioral, trial-level data <code>L2_data</code> by creating an object named <code>eye_behav_L2</code> that selects useful columns from that file and renames stimuli to make them more intuitive. Because most of this will be user-specific, no function is called here. Below we describe the preprocessing done on the behavioral data file. The below code processes and transforms the <code>L2_data</code> dataset into a cleaned and structured format for further analysis. First, the code renames several columns for easier access using <code>janitor::clean_names()</code><span class="citation" data-cites="janitor">(Firke, 2023)</span> function. We then select only the columns we need and filter the dataset to include only rows where <code>screen_name</code> is “VWP” and <code>zone_type</code> is called “response_button_image”, representing the picture selected for that trial. Afterward, the function renames additional columns (<code>tlpic</code> to <code>TL</code>, <code>trpic</code> to <code>TR</code>, etc.). We also renamed <code>participant_private_id</code> to <code>subject</code>, <code>spreadsheet_row</code> to <code>trial</code>, and <code>reaction_time</code> to <code>RT</code>. This makes our columns consistent with the <code>edat_L2</code> above for merging later on. Lastly, <code>reaction time</code> (RT) is converted to a numeric format for further numerical analysis.</p>
<p>It is important to note here that what the behavioral spreadsheet denotes as trial is not in fact the trial number used in the eye-tracking files. Thus it is imperative you use <code>spreadsheet row</code> as trial number to merge the two files successfully.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>eye_behav_L2 <span class="ot">&lt;-</span> L2_data <span class="sc">%&gt;%</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  janitor<span class="sc">::</span><span class="fu">clean_names</span>() <span class="sc">%&gt;%</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Select specific columns to keep in the dataset</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(participant_private_id, correct, tlpic, trpic, blpic, brpic, condition, </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                eng_targetword, targetword, typetl, typetr, typebl, typebr, zone_name, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                zone_type, reaction_time, spreadsheet_row, response, screen_name) <span class="sc">%&gt;%</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Filter the rows where 'Zone.Type' equals "response_button_image"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># participants clicked on preview screen so now need to filter based on screen. </span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(screen_name <span class="sc">==</span> <span class="st">"VWP"</span>, zone_type <span class="sc">==</span> <span class="st">"response_button_image"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Rename columns for easier use and readability</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">TL =</span> tlpic,             <span class="co"># Rename 'tlpic' to 'TL'</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">TR =</span> trpic,             <span class="co"># Rename 'trpic' to 'TR'</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">BL =</span> blpic,             <span class="co"># Rename 'blpic' to 'BL'</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">BR =</span> brpic,             <span class="co"># Rename 'brpic' to 'BR'</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">targ_loc =</span> zone_name,   <span class="co"># Rename 'zone_name' to 'targ_loc'</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">subject =</span> participant_private_id,  <span class="co"># Rename 'participant_private_id' to 'subject'</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">trial =</span> spreadsheet_row,  <span class="co"># Rename 'spreadsheet_row' to 'trial'</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">acc =</span> correct,          <span class="co"># Rename 'correct' to 'acc' (accuracy)</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">RT =</span> reaction_time      <span class="co"># Rename 'reaction_time' to 'RT'</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Convert the 'RT' (Reaction Time) column to numeric type</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">RT =</span> <span class="fu">as.numeric</span>(RT),</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>                <span class="at">subject =</span> <span class="fu">as.factor</span>(subject),</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>                <span class="at">trial =</span> <span class="fu">as.factor</span>(trial))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="audio-onset" class="level3">
<h3 data-anchor-id="audio-onset">Audio onset</h3>
<p>Because we are playing audio on each trial and running this experiment from the browser, audio onset is never going to be consistent across participants. In Gorilla there is an option to collect advanced audio features (you must make sure you select this when designing the study) such as when the audio play was requested, played, and ended. We will want to incorporate this timing information into our analysis pipeline. Gorilla records the onset of the audio which varies by participant. We are extracting that in the <code>audio_rt_L2</code> object by filtering <code>zone_type</code> to <code>content_web_audio</code> and a response equal to “AUDIO PLAY EVENT FIRED”. This will tell us when the audio was triggered in the experiment. We are creating a column called (<code>RT_audio</code>) which we will use later on to correct for audio delays. Please note that on some trials the audio may not play. This is a function of the browser a participant is using and the experimenter has no control over this (see <a href="https://support.gorilla.sc/support/troubleshooting-and-technical/technical-checklist#autoplayingsoundandvideo" class="uri">https://support.gorilla.sc/support/troubleshooting-and-technical/technical-checklist#autoplayingsoundandvideo</a>). When running your experiment on a different platform, make sure you try and request this information, or at the very least acknowledge audio delay.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>audio_rt_L2 <span class="ot">&lt;-</span> L2_data <span class="sc">%&gt;%</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  janitor<span class="sc">::</span><span class="fu">clean_names</span>()<span class="sc">%&gt;%</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">select</span>(participant_private_id,zone_type, spreadsheet_row, reaction_time, response) <span class="sc">%&gt;%</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(zone_type<span class="sc">==</span><span class="st">"content_web_audio"</span>, response<span class="sc">==</span><span class="st">"AUDIO PLAY EVENT FIRED"</span>)<span class="sc">%&gt;%</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>() <span class="sc">%&gt;%</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="st">"subject"</span> <span class="ot">=</span> <span class="st">"participant_private_id"</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>       <span class="st">"trial"</span> <span class="ot">=</span><span class="st">"spreadsheet_row"</span>,  </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>       <span class="st">"RT_audio"</span> <span class="ot">=</span> <span class="st">"reaction_time"</span>, </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>       <span class="st">"Fired"</span> <span class="ot">=</span> <span class="st">"response"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="fu">select</span>(<span class="sc">-</span>zone_type) <span class="sc">%&gt;%</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">mutate</span>(<span class="at">RT_audio=</span><span class="fu">as.numeric</span>(RT_audio)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then merge this information with <code>eye_behav_L2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># merge the audio Rt data to the trial level object</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>trial_data_rt_L2 <span class="ot">&lt;-</span> <span class="fu">merge</span>(eye_behav_L2, audio_rt_L2, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"trial"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="trial-removal" class="level3">
<h3 data-anchor-id="trial-removal">Trial Removal</h3>
<p>As stated above, participants who did not successfully calibrate 3 times or less were rejected from the experiment. Deciding to remove trials is ultimately up to the researcher. In our case, we removed participants with less than 100 trials. Let’s take a look at how many participants meet this criterion by probing the <code>trial_data_rt_L2</code> object.In <a href="#tbl-partL2" class="quarto-xref" aria-expanded="false">Table&nbsp;2</a> we can see several participants failed some of the calibration attempts and do not have an adequate number of trials. Again we make no strong recommendations here. If you decide to use a criterion such as this, we recommend pre-registering your choice.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find out how many trials each participant had</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>edatntrials_L2 <span class="ot">&lt;-</span>trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">group_by</span>(subject)<span class="sc">%&gt;%</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">ntrials=</span><span class="fu">length</span>(<span class="fu">unique</span>(trial)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-tbl-align="center" prefix="" data-tblnum="2" data-custom-style="FigureWithoutNote">
<div id="tbl-partL2" class="cell quarto-float quarto-figure quarto-figure-center" data-tbl-align="center" prefix="" data-tblnum="2">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-partL2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;2</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participants with less than 100 trials</p>
</div>
</figcaption>
<div aria-describedby="tbl-partL2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<!-- preamble start -->

    <script>

      function styleCell_sqs89clzlkyebgwmnfzc(i, j, css_id) {
          var table = document.getElementById("tinytable_sqs89clzlkyebgwmnfzc");
          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors
          if (cell) {
              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);
              cell.classList.add(css_id);
          } else {
              console.warn(`Cell at (${i}, ${j}) not found.`);
          }
      }
      function insertSpanRow(i, colspan, content) {
        var table = document.getElementById('tinytable_sqs89clzlkyebgwmnfzc');
        var newRow = table.insertRow(i);
        var newCell = newRow.insertCell(0);
        newCell.setAttribute("colspan", colspan);
        // newCell.innerText = content;
        // this may be unsafe, but innerText does not interpret <br>
        newCell.innerHTML = content;
      }
      function spanCell_sqs89clzlkyebgwmnfzc(i, j, rowspan, colspan) {
        var table = document.getElementById("tinytable_sqs89clzlkyebgwmnfzc");
        const targetRow = table.rows[i];
        const targetCell = targetRow.cells[j];
        for (let r = 0; r < rowspan; r++) {
          // Only start deleting cells to the right for the first row (r == 0)
          if (r === 0) {
            // Delete cells to the right of the target cell in the first row
            for (let c = colspan - 1; c > 0; c--) {
              if (table.rows[i + r].cells[j + c]) {
                table.rows[i + r].deleteCell(j + c);
              }
            }
          }
          // For rows below the first, delete starting from the target column
          if (r > 0) {
            for (let c = colspan - 1; c >= 0; c--) {
              if (table.rows[i + r] && table.rows[i + r].cells[j]) {
                table.rows[i + r].deleteCell(j);
              }
            }
          }
        }
        // Set rowspan and colspan of the target cell
        targetCell.rowSpan = rowspan;
        targetCell.colSpan = colspan;
      }
      // tinytable span after
      window.addEventListener('load', function () {
          var cellsToStyle = [
            // tinytable style arrays after
          { positions: [ { i: 0, j: 0 }, { i: 0, j: 1 },  ], css_id: 'tinytable_css_ucisictc9ly2a3wu0ocq',}, 
          { positions: [ { i: 16, j: 0 }, { i: 16, j: 1 },  ], css_id: 'tinytable_css_649p9ubjuq8v7ocivvgf',}, 
          ];

          // Loop over the arrays to style the cells
          cellsToStyle.forEach(function (group) {
              group.positions.forEach(function (cell) {
                  styleCell_sqs89clzlkyebgwmnfzc(cell.i, cell.j, group.css_id);
              });
          });
      });
    </script>

    <style>
      /* tinytable css entries after */
      .table td.tinytable_css_ucisictc9ly2a3wu0ocq, .table th.tinytable_css_ucisictc9ly2a3wu0ocq { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
      .table td.tinytable_css_649p9ubjuq8v7ocivvgf, .table th.tinytable_css_649p9ubjuq8v7ocivvgf { border-bottom: solid #d3d8dc 0.1em; }
    </style>
    <div class="container">
      <table class="table table-borderless" id="tinytable_sqs89clzlkyebgwmnfzc" style="table-layout: fixed; width: 90% !important; margin-left: auto; margin-right: auto;" data-quarto-disable-processing="true">
        <thead>
        
              <tr>
                <th scope="col">subject</th>
                <th scope="col">ntrials</th>
              </tr>
        </thead>
        
        <tbody>
                <tr>
                  <td>12102265</td>
                  <td> 2</td>
                </tr>
                <tr>
                  <td>12110638</td>
                  <td>55</td>
                </tr>
                <tr>
                  <td>12110829</td>
                  <td>59</td>
                </tr>
                <tr>
                  <td>12110878</td>
                  <td>59</td>
                </tr>
                <tr>
                  <td>12110897</td>
                  <td>60</td>
                </tr>
                <tr>
                  <td>12111234</td>
                  <td>57</td>
                </tr>
                <tr>
                  <td>12111244</td>
                  <td>58</td>
                </tr>
                <tr>
                  <td>12111363</td>
                  <td>58</td>
                </tr>
                <tr>
                  <td>12111663</td>
                  <td>57</td>
                </tr>
                <tr>
                  <td>12111703</td>
                  <td>58</td>
                </tr>
                <tr>
                  <td>12111869</td>
                  <td>60</td>
                </tr>
                <tr>
                  <td>12111960</td>
                  <td>46</td>
                </tr>
                <tr>
                  <td>12112152</td>
                  <td>59</td>
                </tr>
                <tr>
                  <td>12212113</td>
                  <td>56</td>
                </tr>
                <tr>
                  <td>12213826</td>
                  <td>99</td>
                </tr>
                <tr>
                  <td>12213965</td>
                  <td>59</td>
                </tr>
        </tbody>
      </table>
    </div>
<!-- hack to avoid NA insertion in last line -->
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>Let’s remove participants with less than 100 trials from the analysis using the below code.</p>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>trial_data_rt_L2 <span class="ot">&lt;-</span> trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(subject <span class="sc">%in%</span> edatntrials_bad_L2<span class="sc">$</span>subject)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="low-accuracy" class="level3">
<h3 data-anchor-id="low-accuracy">Low Accuracy</h3>
<p>In our experiment, we want to make sure accuracy is high (&gt; 80%). Again, we want participants that are fully attentive in the experiment. In the below code, we keep participants with accuracy equal to or above 80% and only include correct trials and assign it to <code>trial_data_acc_clean_L2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Calculate mean accuracy per subject and filter out subjects with mean accuracy &lt; 0.8</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>subject_mean_acc_L2 <span class="ot">&lt;-</span> trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(subject) <span class="sc">%&gt;%</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">mean_acc =</span> <span class="fu">mean</span>(acc, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(mean_acc <span class="sc">&gt;</span> <span class="fl">0.8</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Join the mean accuracy back to the main dataset and exclude trials with accuracy &lt; 0.8</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>trial_data_acc_clean_L2 <span class="ot">&lt;-</span> trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(subject_mean_acc_L2, <span class="at">by =</span> <span class="st">"subject"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(acc<span class="sc">==</span><span class="dv">1</span>) <span class="co"># only use accurate responses for fixation analysis</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rts" class="level3">
<h3 data-anchor-id="rts">RTs</h3>
<p>There is much debate on how to handle reaction time (RT) data <span class="citation" data-cites="miller2023">(see Miller, 2023)</span>. Because of this. we leave it up to the reader and researcher to decide what to do with RTs. In this tutorial we leave RTs untouched.</p>
</section>
<section id="sampling-rate" class="level3">
<h3 data-anchor-id="sampling-rate">Sampling Rate</h3>
<p>While most commercial eye-trackers sample at a constant rate, data captured by webcams are widely inconsistent. Below is some code to calculate the sampling rate of each participant. Ideally, you should not have a sampling rate less than 5 Hz. It has been recommended you drop those values <span class="citation" data-cites="bramlett2024">(Bramlett &amp; Wiener, 2024)</span> The below function <code>analyze_sample_rate()</code> calculates the sampling rate for each subject and each trial in our eye-tracking dataset (<code>edat_L2</code>). The <code>analyze_sample_rate()</code> function provides overall statistics, including the option to report mean or&nbsp;median <span class="citation" data-cites="bramlett2024">(Bramlett &amp; Wiener, 2024)</span> sampling rate and standard deviation of sampling rates in your experiment. Sampling rate calculations followed standard procedures <span class="citation" data-cites="bramlett2024 prystauka2024">(e.g., Bramlett &amp; Wiener, 2024; Prystauka et al., 2024)</span>. The function also generates a histogram of sampling rates by-subject. Looking at <a href="#fig-samprate-L2" class="quarto-xref" aria-expanded="false">Figure&nbsp;4</a>, the sampling rate ranges from 5 to 35 Hz with a median sampling rate of 21.56. This corresponds to previous webcam eye-tracking work <span class="citation" data-cites="bramlett2024 prystauka2024">(e.g., Bramlett &amp; Wiener, 2024; Prystauka et al., 2024)</span></p>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>samp_rate_L2 <span class="ot">&lt;-</span> <span class="fu">analyze_sampling_rate</span>(edat_L2, <span class="at">summary_stat=</span><span class="st">"Median"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overall Median Sampling Rate (Hz): 21.56 
Overall SD of Sampling Rate (Hz): 7.44 </code></pre>
</div>
<div class="cell-output-display">
<div id="fig-samprate-L2" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="4">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-samprate-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;4</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant sampling-rate for L2 experiment. A histogram and overlayed density plot shows median sampling rate by participant. The overall median and SD is highlighted in red.</p>
</div>
</figcaption>
<div aria-describedby="fig-samprate-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-samprate-L2-1.svg" class="img-fluid figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>When using the above function, separate data frames are produced by-participants and by-trial. These can be added to the behavioral data frame using the below code.</p>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>trial_data_L2 <span class="ot">&lt;-</span> <span class="fu">merge</span>(trial_data_acc_clean_L2, samp_rate_L2, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"trial"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use this information to filter out data with poor sampling rates. Users can use the <code>filter_sampling_rate()</code> function. The <code>filter_sampling_rate()</code> function is designed to process a dataset containing participant-level and trial-level sampling rates. It allows the user to either filter out data that falls below a certain sampling rate threshold or simply label it as “bad”. The function gives flexibility by allowing the threshold to be applied at the participant-level, trial-level, or both. It also lets the user decide whether to remove the data or flag it as below the threshold without removing it. If <code>action</code> = remove, the function will output how many subjects and trials were removed using the threshold. We leave it up to the user to decide what to do with low sampling rates and make no specific recommendations. Here we use the <code>filter_sampling_rate()</code> function to remove trials and participants from the<code>trial_data_L2</code> object.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>filter_edat_L2 <span class="ot">&lt;-</span> <span class="fu">filter_sampling_rate</span>(trial_data_L2,<span class="at">threshold =</span> <span class="dv">5</span>,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">action =</span> <span class="st">"remove"</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">by =</span> <span class="st">"both"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="out-of-bounds-outside-of-screen" class="level3">
<h3 data-anchor-id="out-of-bounds-outside-of-screen">Out-of-Bounds (Outside of Screen)</h3>
<p>It is essential to exclude gaze points that fall outside the screen, as these indicate unreliable estimates of gaze location. The <code>gaze_oob()</code>function quantifies how many data points fall outside these bounds, using the eye-tracking dataset (e.g., edat_L2) and the standardized screen dimensions—here set to (1, 1) because Gorilla recommends using standardized coordinates. If the <code>remove</code> argument is set to TRUE, the function applies an outer-edge filtering method to eliminate these out-of-bounds points <span class="citation" data-cites="bramlett2024">(see Bramlett &amp; Wiener, 2024)</span>. The outer-edge approach appears to be a less biased approach based on demonstrations from <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (2024)</span>, where they showed minimal data loss compared to other approaches (e.g., inner-edge approach).</p>
<p>The function returns a summary table showing the total number and percentage of gaze points that fall outside the bounds, broken down by axis (X, Y), as well as the combined total (see <a href="#tbl-oob-L2" class="quarto-xref" aria-expanded="false">Table&nbsp;3</a>). It also returns three additional tibbles: (1) missingness by-subject, (2) missingness by-trial, and (3) a cleaned dataset with all the data merged, and the problematic rows removed if specified. These outputs can be referenced in a final report or manuscript. As shown in <a href="#fig-fixquads" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a>, no fixation points fall outside the standardized coordinate range.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>oob_data_L2 <span class="ot">&lt;-</span> <span class="fu">gaze_oob</span>(<span class="at">data=</span>edat_L2, <span class="at">subject_col =</span> <span class="st">"subject"</span>,</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">trial_col =</span> <span class="st">"trial"</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">x_col =</span> <span class="st">"x"</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">y_col =</span> <span class="st">"y"</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">screen_size =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="co"># standardized coordinates have screen size 1,1</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">remove =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="3" data-custom-style="FigureWithoutNote">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>oob_data_L2<span class="sc">$</span>subject_results <span class="sc">%&gt;%</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), <span class="sc">~</span><span class="fu">round</span>(.x, <span class="dv">2</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename_with</span>(<span class="sc">~</span> <span class="fu">gsub</span>(<span class="st">"_"</span>, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, .x)) <span class="sc">%&gt;%</span>         <span class="co"># Replace underscores with line breaks</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename_with</span>(<span class="sc">~</span> <span class="fu">gsub</span>(<span class="st">"percentage"</span>, <span class="st">"%"</span>, .x, <span class="at">ignore.case =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;%</span>  <span class="co"># Replace 'percent' with '%'</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>() <span class="sc">%&gt;%</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">flextable</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fontsize</span>(<span class="at">size =</span> <span class="dv">12</span>) <span class="sc">%&gt;%</span>  <span class="co"># Reduce font size</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">padding</span>(<span class="at">padding =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">font</span>(<span class="at">fontname =</span> <span class="st">"Times New Roman"</span>, <span class="at">part =</span> <span class="st">"all"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_table_properties</span>(<span class="at">layout=</span><span class="st">"autofit"</span>) <span class="sc">%&gt;%</span> <span class="co"># Reduce padding inside cells</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autofit</span>() <span class="sc">%&gt;%</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_apa</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="tbl-oob-L2" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="3">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-oob-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;3</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Out of bounds gaze statistics by-participant (for 6 participants)</p>
</div>
</figcaption>
<div aria-describedby="tbl-oob-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-bb0611dc{table-layout:auto;}.cl-bb003802{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bb003816{font-family:'Times New Roman';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bb02b0f0{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-bb02b104{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:1pt;padding-top:1pt;padding-left:1pt;padding-right:1pt;line-height: 2;background-color:transparent;}.cl-bb02cb3a{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb44{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb45{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb58{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb59{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb5a{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb62{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb63{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb6c{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb6d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb76{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb77{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb80{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb81{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bb02cb8a{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-bb0611dc"><thead><tr style="overflow-wrap:break-word;"><th class="cl-bb02cb3a"><p class="cl-bb02b0f0"><span class="cl-bb003802">subject</span></p></th><th class="cl-bb02cb44"><p class="cl-bb02b0f0"><span class="cl-bb003802">total</span><br><span class="cl-bb003802">trials</span></p></th><th class="cl-bb02cb45"><p class="cl-bb02b0f0"><span class="cl-bb003802">total</span><br><span class="cl-bb003802">points</span></p></th><th class="cl-bb02cb58"><p class="cl-bb02b0f0"><span class="cl-bb003802">outside</span><br><span class="cl-bb003802">count</span></p></th><th class="cl-bb02cb59"><p class="cl-bb02b0f0"><span class="cl-bb003802">subject</span><br><span class="cl-bb003802">missing</span><br><span class="cl-bb003802">%</span></p></th><th class="cl-bb02cb58"><p class="cl-bb02b0f0"><span class="cl-bb003802">x</span><br><span class="cl-bb003802">outside</span><br><span class="cl-bb003802">count</span></p></th><th class="cl-bb02cb58"><p class="cl-bb02b0f0"><span class="cl-bb003802">y</span><br><span class="cl-bb003802">outside</span><br><span class="cl-bb003802">count</span></p></th><th class="cl-bb02cb58"><p class="cl-bb02b0f0"><span class="cl-bb003802">x</span><br><span class="cl-bb003802">outside</span><br><span class="cl-bb003802">%</span></p></th><th class="cl-bb02cb58"><p class="cl-bb02b0f0"><span class="cl-bb003802">y</span><br><span class="cl-bb003802">outside</span><br><span class="cl-bb003802">%</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-bb02cb5a"><p class="cl-bb02b104"><span class="cl-bb003816">12102265</span></p></td><td class="cl-bb02cb62"><p class="cl-bb02b104"><span class="cl-bb003816">60.00</span></p></td><td class="cl-bb02cb63"><p class="cl-bb02b104"><span class="cl-bb003816">6,192.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">1,132.00</span></p></td><td class="cl-bb02cb6d"><p class="cl-bb02b104"><span class="cl-bb003816">18.28</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">202.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">947.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">3.26</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">15.29</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-bb02cb5a"><p class="cl-bb02b104"><span class="cl-bb003816">12102286</span></p></td><td class="cl-bb02cb62"><p class="cl-bb02b104"><span class="cl-bb003816">240.00</span></p></td><td class="cl-bb02cb63"><p class="cl-bb02b104"><span class="cl-bb003816">11,765.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">354.00</span></p></td><td class="cl-bb02cb6d"><p class="cl-bb02b104"><span class="cl-bb003816">3.01</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">267.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">181.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">2.27</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">1.54</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-bb02cb5a"><p class="cl-bb02b104"><span class="cl-bb003816">12102530</span></p></td><td class="cl-bb02cb62"><p class="cl-bb02b104"><span class="cl-bb003816">240.00</span></p></td><td class="cl-bb02cb63"><p class="cl-bb02b104"><span class="cl-bb003816">9,011.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">385.00</span></p></td><td class="cl-bb02cb6d"><p class="cl-bb02b104"><span class="cl-bb003816">4.27</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">244.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">147.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">2.71</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">1.63</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-bb02cb5a"><p class="cl-bb02b104"><span class="cl-bb003816">12110559</span></p></td><td class="cl-bb02cb62"><p class="cl-bb02b104"><span class="cl-bb003816">240.00</span></p></td><td class="cl-bb02cb63"><p class="cl-bb02b104"><span class="cl-bb003816">11,887.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">415.00</span></p></td><td class="cl-bb02cb6d"><p class="cl-bb02b104"><span class="cl-bb003816">3.49</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">194.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">221.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">1.63</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">1.86</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-bb02cb5a"><p class="cl-bb02b104"><span class="cl-bb003816">12110579</span></p></td><td class="cl-bb02cb62"><p class="cl-bb02b104"><span class="cl-bb003816">178.00</span></p></td><td class="cl-bb02cb63"><p class="cl-bb02b104"><span class="cl-bb003816">5,798.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">1,061.00</span></p></td><td class="cl-bb02cb6d"><p class="cl-bb02b104"><span class="cl-bb003816">18.30</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">696.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">435.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">12.00</span></p></td><td class="cl-bb02cb6c"><p class="cl-bb02b104"><span class="cl-bb003816">7.50</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-bb02cb76"><p class="cl-bb02b104"><span class="cl-bb003816">12110585</span></p></td><td class="cl-bb02cb77"><p class="cl-bb02b104"><span class="cl-bb003816">240.00</span></p></td><td class="cl-bb02cb80"><p class="cl-bb02b104"><span class="cl-bb003816">13,974.00</span></p></td><td class="cl-bb02cb81"><p class="cl-bb02b104"><span class="cl-bb003816">776.00</span></p></td><td class="cl-bb02cb8a"><p class="cl-bb02b104"><span class="cl-bb003816">5.55</span></p></td><td class="cl-bb02cb81"><p class="cl-bb02b104"><span class="cl-bb003816">83.00</span></p></td><td class="cl-bb02cb81"><p class="cl-bb02b104"><span class="cl-bb003816">694.00</span></p></td><td class="cl-bb02cb81"><p class="cl-bb02b104"><span class="cl-bb003816">0.59</span></p></td><td class="cl-bb02cb81"><p class="cl-bb02b104"><span class="cl-bb003816">4.97</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-fixquads" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="5">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-fixquads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;5</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Looks to each quadrant of the screen</p>
</div>
</figcaption>
<div aria-describedby="fig-fixquads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-fixquads-1.svg" class="img-fluid figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>We can use the <code>data_clean</code> tibble returned by the <code>gaze_oob</code>() function to filter out trials and subjects with more than 30% missing data. The value of 30% is just a suggestion and should not be used as a rule of thumb for all studies nor are we endorsing this value.</p>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># remove participants with more than 30% missing data and trials with more than 30% missing data</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a> filter_oob <span class="ot">&lt;-</span> oob_data_L2<span class="sc">$</span>data_clean <span class="sc">%&gt;%</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(trial_missing_percentage <span class="sc">&lt;=</span> <span class="dv">30</span> <span class="sc">|</span> subject_missing_percentage <span class="sc">&lt;=</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="eye-tracking-data-1" class="level2">
<h2 data-anchor-id="eye-tracking-data-1">Eye-tracking data</h2>
<section id="convergence-and-confidence" class="level3">
<h3 data-anchor-id="convergence-and-confidence">Convergence and Confidence</h3>
<p>To ensure data quality, we removed rows with poor convergence and low face confidence from our eye-tracking dataset. As described in <span class="citation" data-cites="prystauka2024">Prystauka et al. (2024)</span>, the Gorilla eye-tracking output includes two key columns for this purpose: <code>convergence</code> and <code>face_conf</code> (similar variables may be available in other platforms as well). The <code>convergence</code> column contains values between 0 and 1, with lower values indicating better convergence—that is, greater model confidence in predicting gaze location and finding a face. Values below 0.5 typically reflect adequate convergence. The <code>face_conf</code> column reflects how confidently the algorithm detected a face in the frame, also ranging from 0 to 1. Here, values above 0.5 indicate a good model fit.</p>
<p>Accordingly, we filtered the <code>edat_L2</code>dataset to include only rows where convergence &lt; 0.5 and face_conf &gt; 0.5, and saved the cleaned dataset as edat_1_L2.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>edat_1_L2 <span class="ot">&lt;-</span> filter_oob <span class="sc">%&gt;%</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">filter</span>(convergence <span class="sc">&lt;=</span> .<span class="dv">5</span>, face_conf <span class="sc">&gt;=</span> .<span class="dv">5</span>) <span class="co"># remove poor convergnce and face confidence</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="combining-eye-and-trial-level-data" class="level3">
<h3 data-anchor-id="combining-eye-and-trial-level-data">Combining Eye and Trial-Level Data</h3>
<p>Next, we will combine the eye-tracking data and behavioral data. In this case, we’ll use merge to add the behavioral data to the eye-tracking data. This ensures that all rows from the eye-tracking data are preserved, even if there isn’t a matching entry in the behavioral data (missing values will be filled with NA). The resulting object is called dat_L2.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>dat_L2 <span class="ot">&lt;-</span> <span class="fu">merge</span>(edat_1_L2, filter_edat_L2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="areas-of-interest" class="level2">
<h2 data-anchor-id="areas-of-interest">Areas of Interest</h2>
<section id="zone-coordinates" class="level3">
<h3 data-anchor-id="zone-coordinates">Zone Coordinates</h3>
<p>In the lab, we can control many aspects of the experiment that cannot be controlled online. Participants will be completing the experiment under a variety of conditions including, different computers, with very different screen dimensions. To control for this, Gorilla outputs standardized zone coordinates (labeled as <code>x_pred_normalised</code> and <code>y_pred_normalised</code> in the eye-tracking file) . As discussed in the Gorilla documentation, the Gorilla lays everything out in a 4:3 frame and makes that frame as big as possible. The normalized coordinates are then expressed relative to this frame; for example, the coordinate 0.5, 0.5 will always be the center of the screen, regardless of the size of the participant’s screen. We used the normalized coordinates in our analysis (in general, you should always use normalized coordinates). However, there are a few different ways to specify the four coordinates of the screen, which are worth highlighting here.</p>
<section id="quadrant-approach" class="level4">
<h4 data-anchor-id="quadrant-approach">Quadrant Approach.</h4>
<p>One way is to make the AOIs as big as possible, dividing the screen into four quadrants. This approach has been used in several studies [e.g., <span class="citation" data-cites="bramlett2024 prystauka2024">(Bramlett &amp; Wiener, 2024; Prystauka et al., 2024)</span>. <a href="#tbl-quadcor" class="quarto-xref" aria-expanded="false">Table&nbsp;4</a> lists coordinates for the quadrant approach and <a href="#fig-quads" class="quarto-xref" aria-expanded="false">Figure&nbsp;6</a> shows how each quadrant looks in standardized space.</p>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="4" data-custom-style="FigureWithoutNote">
<div id="tbl-quadcor" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="4">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-quadcor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;4</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Quandrant coordinates in standardized space</p>
</div>
</figcaption>
<div aria-describedby="tbl-quadcor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-c664dd06{table-layout:auto;}.cl-c65f2de8{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c65f2df2{font-family:'Times New Roman';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c661943e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-c6619448{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 2;background-color:transparent;}.cl-c661aea6{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aeb0{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aeb1{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aeba{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aebb{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aec4{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aec5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aece{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aecf{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aed0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aed8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aee2{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aee3{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aee4{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aeec{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aeed{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aeee{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c661aef6{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-c664dd06"><thead><tr style="overflow-wrap:break-word;"><th class="cl-c661aea6"><p class="cl-c661943e"><span class="cl-c65f2de8">loc</span></p></th><th class="cl-c661aeb0"><p class="cl-c661943e"><span class="cl-c65f2de8">x_normalized</span></p></th><th class="cl-c661aeb0"><p class="cl-c661943e"><span class="cl-c65f2de8">y_normalized</span></p></th><th class="cl-c661aeb1"><p class="cl-c661943e"><span class="cl-c65f2de8">width_normalized</span></p></th><th class="cl-c661aeba"><p class="cl-c661943e"><span class="cl-c65f2de8">height_normalized</span></p></th><th class="cl-c661aebb"><p class="cl-c661943e"><span class="cl-c65f2de8">xmin</span></p></th><th class="cl-c661aebb"><p class="cl-c661943e"><span class="cl-c65f2de8">ymin</span></p></th><th class="cl-c661aec4"><p class="cl-c661943e"><span class="cl-c65f2de8">xmax</span></p></th><th class="cl-c661aec4"><p class="cl-c661943e"><span class="cl-c65f2de8">ymax</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-c661aec5"><p class="cl-c6619448"><span class="cl-c65f2df2">TL</span></p></td><td class="cl-c661aece"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aece"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aecf"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed0"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed8"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aed8"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aee2"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aee2"><p class="cl-c6619448"><span class="cl-c65f2df2">1.00</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c661aec5"><p class="cl-c6619448"><span class="cl-c65f2df2">TR</span></p></td><td class="cl-c661aece"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aece"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aecf"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed0"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed8"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed8"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aee2"><p class="cl-c6619448"><span class="cl-c65f2df2">1.00</span></p></td><td class="cl-c661aee2"><p class="cl-c6619448"><span class="cl-c65f2df2">1.00</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c661aec5"><p class="cl-c6619448"><span class="cl-c65f2df2">BL</span></p></td><td class="cl-c661aece"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aece"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aecf"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed0"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aed8"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aed8"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aee2"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aee2"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c661aee3"><p class="cl-c6619448"><span class="cl-c65f2df2">BR</span></p></td><td class="cl-c661aee4"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aee4"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aeec"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aeed"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aeee"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td><td class="cl-c661aeee"><p class="cl-c6619448"><span class="cl-c65f2df2">0.00</span></p></td><td class="cl-c661aef6"><p class="cl-c6619448"><span class="cl-c65f2df2">1.00</span></p></td><td class="cl-c661aef6"><p class="cl-c6619448"><span class="cl-c65f2df2">0.50</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-quads" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="6">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-quads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;6</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>AOI coordinates in standardized space using the quadrant approach</p>
</div>
</figcaption>
<div aria-describedby="fig-quads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-quads-1.svg" class="img-fluid figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<section id="matching-conditions-with-screen-locations" class="level5">
<h5 data-anchor-id="matching-conditions-with-screen-locations">Matching Conditions with Screen Locations.</h5>
<p>The goal of the below code is to assign condition codes (e.g., Target, Unrelated, Unrelated2, and Cohort) to each image in the dataset based on the screen location where the image is displayed (e.g., TL, TR, BL, BR).</p>
<p>For each trial, the images are dynamically placed at different screen locations, and the code maps each image to its corresponding condition based on these locations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming your data is in a data frame called dat_L2</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>dat_L2 <span class="ot">&lt;-</span> dat_L2 <span class="sc">%&gt;%</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Target =</span> <span class="fu">case_when</span>(</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> TL,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> TR,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> BL,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> BR,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span>  <span class="co"># Default to NA if no match</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">Unrelated =</span> <span class="fu">case_when</span>(</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> TL,</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> TR,</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> BL,</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> BR,</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">Unrelated2 =</span> <span class="fu">case_when</span>(</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> TL,</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> TR,</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> BL,</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> BR,</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">Cohort =</span> <span class="fu">case_when</span>(</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> TL,</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> TR,</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> BL,</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> BR,</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In addition to tracking the condition of each image during randomized trials, a custom function, <code>find_location()</code>, determines the specific screen location of each image by comparing it against the list of possible locations. This function ensures that the appropriate location is identified or returns <code>NA</code> if no match exists. Specifically, <code>find_location()</code> first checks if the image is <code>NA</code> (missing). If the image is <code>NA</code>, the function returns <code>NA</code>, meaning that there’s no location to find for this image. If the image is not <code>NA</code>, the function creates a vector called <code>loc_names</code> that lists the names of the possible locations. It then attempts to match the given image with the locations. If a match is found, it returns the name of the location (e.g., TL, TR, BL, or BR) of the image.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the function to each of the targ, cohort, rhyme, and unrelated columns</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>dat_colnames_L2 <span class="ot">&lt;-</span> dat_L2 <span class="sc">%&gt;%</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>( </span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">targ_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(<span class="at">TL =</span> TL, <span class="at">TR =</span> TR, <span class="at">BL =</span> BL, <span class="at">BR =</span> BR), Target),</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">cohort_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(<span class="at">TL =</span> TL, <span class="at">TR =</span> TR, <span class="at">BL =</span> BL, <span class="at">BR =</span> BR), Cohort),</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">unrelated_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(<span class="at">TL =</span> TL, <span class="at">TR =</span> TR, <span class="at">BL =</span> BL, <span class="at">BR =</span> BR), Unrelated),</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">unrelated2_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(<span class="at">TL =</span> TL, <span class="at">TR =</span> TR, <span class="at">BL =</span> BL, <span class="at">BR =</span> BR), Unrelated2)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we do this we can use the <code>assign_aoi()</code> function to loop through our object called <code>dat_colnames_L2</code> and assign locations (i.e., TR, TL, BL, BR) to where participants looked at on the screen. This requires the <code>x</code> and <code>y</code> coordinates and the location of our aois <code>aoi_loc</code>. Here we are using the quadrant approach. This function will label non-looks and off screen coordinates with NA. To make it easier to read we change the numerals assigned by the function to actual screen locations (e.g., TL, TR, BL, BR).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>assign_L2 <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">assign_aoi</span>(dat_colnames_L2,<span class="at">X=</span><span class="st">"x"</span>, <span class="at">Y=</span><span class="st">"y"</span>,<span class="at">aoi_loc =</span> aoi_loc)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>AOI_L2 <span class="ot">&lt;-</span> assign_L2 <span class="sc">%&gt;%</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">loc1 =</span> <span class="fu">case_when</span>(</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">1</span> <span class="sc">~</span> <span class="st">"TL"</span>, </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">2</span> <span class="sc">~</span> <span class="st">"TR"</span>, </span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">3</span> <span class="sc">~</span> <span class="st">"BL"</span>, </span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">4</span> <span class="sc">~</span> <span class="st">"BR"</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <code>AOI_L2</code> we label looks to Targets, Unrelated, and Cohort items with 1 (looked) and 0 (no look) using the <code>case_when</code> function from the <code>tidyverse</code> <span class="citation" data-cites="wickham2017">(Wickham, 2017)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>AOI_L2 <span class="ot">&lt;-</span> AOI_L2 <span class="sc">%&gt;%</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">target =</span> <span class="fu">case_when</span>(loc1 <span class="sc">==</span> targ_loc <span class="sc">~</span> <span class="dv">1</span>, <span class="cn">TRUE</span> <span class="sc">~</span> <span class="dv">0</span>),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">unrelated =</span> <span class="fu">case_when</span>(loc1 <span class="sc">==</span> unrelated_loc <span class="sc">~</span> <span class="dv">1</span>, <span class="cn">TRUE</span> <span class="sc">~</span> <span class="dv">0</span>),</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">unrelated2 =</span> <span class="fu">case_when</span>(loc1 <span class="sc">==</span> unrelated2_loc <span class="sc">~</span> <span class="dv">1</span>, <span class="cn">TRUE</span> <span class="sc">~</span> <span class="dv">0</span>),</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">cohort =</span> <span class="fu">case_when</span>(loc1 <span class="sc">==</span> cohort_loc <span class="sc">~</span> <span class="dv">1</span>, <span class="cn">TRUE</span> <span class="sc">~</span> <span class="dv">0</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The locations of looks need to be pivoted into long format—that is, converted from separate columns into a single column. This transformation makes the data easier to visualize and analyze. We use the <code>pivot_longer()</code> function from the <code>tidyverse</code> to combine the columns (Target, Unrelated, Unrelated2, and Cohort) into a single column called <code>condition1</code>. Additionally, we create another column called <code>Looks</code>, which contains the values from the original columns (e.g., 0 or 1 for whether the area was looked at).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>dat_long_aoi_me_L2 <span class="ot">&lt;-</span> AOI_L2 <span class="sc">%&gt;%</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(subject, trial, condition, target, cohort, unrelated, unrelated2, time, x, y, RT_audio) <span class="sc">%&gt;%</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cols =</span> <span class="fu">c</span>(target, unrelated, unrelated2, cohort),</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">names_to =</span> <span class="st">"condition1"</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">values_to =</span> <span class="st">"Looks"</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We further clean up the object by first cleaning up the condition codes. They have a numeral appended to them and that should be removed. We then adjust the timing in the <code>gaze_sub_L2_comp</code> object by aligning time to the actual audio onset. To achieve this, we subtract <code>RT_audio</code> from time for each trial. In addition, we subtract 300 ms from this to account for the 100 ms of silence at the beginning of each audio clip and 200 ms to account for the oculomotor delay when planning an eye movement <span class="citation" data-cites="viviani1990">(Viviani, 1990)</span>. Additionally, we set our interest period between 0 ms (audio onset) and 2000 ms. This was chosen based on the time course figures in <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> . It is important that you choose your interest area carefully and preferably you preregister it. The interest period you choose can bias your findings <span class="citation" data-cites="peelle2021">(Peelle &amp; Van Engen, 2021)</span>. We also filter out gaze coordinates that fall outside the standardized window, ensuring only valid data points are retained. The resulting object <code>gaze_sub_long_L2</code> provides the corrected time column spanning from -200 ms to 2000 ms relative to stimulus onset with looks outside the screen removed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># repalce the numbers appended to conditions that somehow got added </span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>dat_long_aoi_me_comp <span class="ot">&lt;-</span> dat_long_aoi_me_L2 <span class="sc">%&gt;%</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">condition =</span> <span class="fu">str_replace</span>(condition, <span class="st">"TCUU-SPENG</span><span class="sc">\\</span><span class="st">d*"</span>, <span class="st">"TCUU-SPENG"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">condition =</span> <span class="fu">str_replace</span>(condition, <span class="st">"TCUU-SPSP</span><span class="sc">\\</span><span class="st">d*"</span>, <span class="st">"TCUU-SPSP"</span>))<span class="sc">%&gt;%</span> </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dat_long_aoi_me_comp has condition corrected </span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2_long <span class="ot">&lt;-</span>dat_long_aoi_me_comp<span class="sc">%&gt;%</span> </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="fu">group_by</span>(subject, trial, condition) <span class="sc">%&gt;%</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">time =</span> (time<span class="sc">-</span>RT_audio)<span class="sc">-</span><span class="dv">300</span>) <span class="sc">%&gt;%</span> <span class="co"># subtract audio rt onset and account for occ motor planning and silence in audio</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">filter</span>(time <span class="sc">&gt;=</span> <span class="sc">-</span><span class="dv">200</span>, time <span class="sc">&lt;</span> <span class="dv">2000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
</section>
<section id="samples-to-bins" class="level2">
<h2 data-anchor-id="samples-to-bins">Samples to Bins</h2>
<section id="downsampling" class="level3">
<h3 data-anchor-id="downsampling">Downsampling</h3>
<p>Downsampling into larger time bins is a common practice in gaze data analysis, as it helps create a more manageable dataset and reduces noise. When using research grade eye-trackers, downsampling is an optional step in the preprocessing pipeline. However, with consumer-based webcam eye-tracking it is recommended you downsample your data so participants have consistent bin sizes (e.g., <span class="citation" data-cites="slim2023 slim2024">(Slim et al., 2024; Slim &amp; Hartsuiker, 2023)</span>). In <code>webgazeR</code> we included the <code>downsample_gaze()</code> function to assist with this process. We apply this function to the <code>gaze_sub_L2_long</code> object,and set the <code>bin.length</code> argument to 100, which groups the data into 100-millisecond intervals. This adjustment means that each bin now represents a 100 ms passage of time. We specify time as the variable to base these bins on, allowing us to focus on broader patterns over time rather than individual millisecond fluctuations. There is no agreed upon downsampling value, but with webcam data larger bins are preferred <span class="citation" data-cites="slim2023">(see Slim &amp; Hartsuiker, 2023)</span>.</p>
<p>In addition, the <code>downsample_gaze()</code> allows you to aggregate across other variables, such as <code>condition</code>, <code>condition1</code>, and use the newly created <code>time_bins</code> variable, which represents the time intervals over which we aggregate data. The resulting downsampled dataset, output as <a href="#tbl-agg-sub" class="quarto-xref" aria-expanded="false">Table&nbsp;5</a>, provides a simplified and more concise view of gaze patterns, making it easier to analyze and interpret broader trends.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2 <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">downsample_gaze</span>(gaze_sub_L2_long, <span class="at">bin.length=</span><span class="dv">100</span>, <span class="at">timevar=</span><span class="st">"time"</span>, <span class="at">aggvars=</span><span class="fu">c</span>(<span class="st">"condition"</span>, <span class="st">"condition1"</span>, <span class="st">"time_bin"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="5" data-custom-style="FigureWithoutNote">
<div id="tbl-agg-sub" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="5">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-agg-sub-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;5</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Aggregated proportion looks for each condition in each 100 ms time bin</p>
</div>
</figcaption>
<div aria-describedby="tbl-agg-sub-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-cb8d2338{}.cl-cb87a4b2{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cb87a4bc{font-family:'Times New Roman';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cb8a01b2{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-cb8a01bc{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:2pt;padding-right:2pt;line-height: 2;background-color:transparent;}.cl-cb8a1a58{width:1.262in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a62{width:1.089in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a63{width:0.943in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a6c{width:0.545in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a6d{width:1.262in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a6e{width:1.089in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a6f{width:0.943in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a76{width:0.545in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a77{width:1.262in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a78{width:1.089in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a80{width:0.943in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cb8a1a81{width:0.545in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-cb8d2338"><thead><tr style="overflow-wrap:break-word;"><th class="cl-cb8a1a58"><p class="cl-cb8a01b2"><span class="cl-cb87a4b2">condition</span></p></th><th class="cl-cb8a1a62"><p class="cl-cb8a01b2"><span class="cl-cb87a4b2">condition1</span></p></th><th class="cl-cb8a1a63"><p class="cl-cb8a01b2"><span class="cl-cb87a4b2">time_bin</span></p></th><th class="cl-cb8a1a6c"><p class="cl-cb8a01b2"><span class="cl-cb87a4b2">Fix</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-cb8a1a6d"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">TCUU-ENGSP</span></p></td><td class="cl-cb8a1a6e"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">cohort</span></p></td><td class="cl-cb8a1a6f"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">-200.00</span></p></td><td class="cl-cb8a1a76"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.26</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cb8a1a6d"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">TCUU-ENGSP</span></p></td><td class="cl-cb8a1a6e"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">cohort</span></p></td><td class="cl-cb8a1a6f"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">-100.00</span></p></td><td class="cl-cb8a1a76"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.26</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cb8a1a6d"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">TCUU-ENGSP</span></p></td><td class="cl-cb8a1a6e"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">cohort</span></p></td><td class="cl-cb8a1a6f"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.00</span></p></td><td class="cl-cb8a1a76"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.25</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cb8a1a6d"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">TCUU-ENGSP</span></p></td><td class="cl-cb8a1a6e"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">cohort</span></p></td><td class="cl-cb8a1a6f"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">100.00</span></p></td><td class="cl-cb8a1a76"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.25</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cb8a1a6d"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">TCUU-ENGSP</span></p></td><td class="cl-cb8a1a6e"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">cohort</span></p></td><td class="cl-cb8a1a6f"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">200.00</span></p></td><td class="cl-cb8a1a76"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.23</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cb8a1a77"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">TCUU-ENGSP</span></p></td><td class="cl-cb8a1a78"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">cohort</span></p></td><td class="cl-cb8a1a80"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">300.00</span></p></td><td class="cl-cb8a1a81"><p class="cl-cb8a01bc"><span class="cl-cb87a4bc">0.23</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>To simplify the analysis, we combine the two unrelated conditions and average them (this is for the proportional plots).</p>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Average Fix for unrelated and unrelated2, then combine with the rest</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2_avg <span class="ot">&lt;-</span> gaze_sub_L2 <span class="sc">%&gt;%</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(condition, time_bin) <span class="sc">%&gt;%</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">Fix =</span> <span class="fu">mean</span>(Fix[condition1 <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"unrelated"</span>, <span class="st">"unrelated2"</span>)], <span class="at">na.rm =</span> <span class="cn">TRUE</span>),</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">condition1 =</span> <span class="st">"unrelated"</span>,  <span class="co"># Assign the combined label</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">.groups =</span> <span class="st">"drop"</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine with rows that do not include unrelated or unrelated2</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_rows</span>(gaze_sub_L2 <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="sc">!</span>condition1 <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"unrelated"</span>, <span class="st">"unrelated2"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above will not include the subject variable. If you want to keep participant-level data we need to add <code>subject</code> to the <code>aggvars</code> argument.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add subject-level data</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2_id <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">downsample_gaze</span>(gaze_sub_L2_long, <span class="at">bin.length=</span><span class="dv">100</span>, <span class="at">timevar=</span><span class="st">"time"</span>, <span class="at">aggvars=</span><span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"condition"</span>, <span class="st">"condition1"</span>, <span class="st">"time_bin"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="upsampling" class="level3">
<h3 data-anchor-id="upsampling">Upsampling</h3>
<p>Users may wish to upsample their data rather than downsample it. This is standard in some preprocessing pipelines in pupillometry <span class="citation" data-cites="kret2018">(Kret &amp; Sjak-Shie, 2018)</span> and has recently been applied to webcam-based eye-tracking data <span class="citation" data-cites="madsen2021">(Madsen et al., 2021)</span>. Like downsampling, upsampling standardizes the time intervals between samples; however, it also increases the sampling rate, which can produce smoother, less noisy data. This is useful if you want to align webcam eye-tracking with other measures (e.g., EEG).</p>
<p>Our webgazeR package provides several functions to assist with this process. The <code>upsample_gaze()</code> function allows users to upsample their gaze data to a higher sampling rate (e.g., 250 Hz or even 1000 Hz). After upsampling, users can apply the <code>smooth_gaze()</code> function to reduce noise (<code>webgazeR</code> uses a n-point moving average) followed by the <code>interpolate_gaze()</code> function to fill in missing values using linear interpolation. Below we show you how to use the function, but do not apply to the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>AOI_upsample <span class="ot">&lt;-</span> AOI <span class="sc">%&gt;%</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(subject, trial) <span class="sc">%&gt;%</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">upsample_gaze</span>(</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">gaze_cols =</span> <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"y"</span>),</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">upsample_pupil =</span> <span class="cn">FALSE</span>, </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">target_hz =</span> <span class="dv">250</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>AOI_smooth<span class="ot">=</span><span class="fu">smooth_gaze</span>(AOI_upsample, <span class="at">n =</span> <span class="dv">5</span>, <span class="at">x_col =</span> <span class="st">"x"</span>, <span class="at">y_col =</span> <span class="st">"y"</span>,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">trial_col =</span> <span class="st">"trial"</span>, <span class="at">subject_col =</span> <span class="st">"subject"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>aoi_interp <span class="ot">&lt;-</span> <span class="fu">interpolate_gaze</span>(deduplicated_data,<span class="at">x_col =</span> <span class="st">"x_pred_normalised"</span>, <span class="at">y_col =</span> <span class="st">"y_pred_normalised"</span>,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">trial_col =</span> <span class="st">"trial"</span>, <span class="at">subject_col =</span> <span class="st">"subject"</span>, <span class="at">time_col=</span><span class="st">"time"</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="aggregation" class="level2">
<h2 data-anchor-id="aggregation">Aggregation</h2>
<p>Aggregation is an optional step. If you do not plan to analyze proportion data, and instead want time binned data with binary outcomes preserved please set the <code>aggvars</code> argument to “none.” This will return a time binned column, but will not aggregate over other variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get back trial level data with no aggregation</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_id <span class="ot">&lt;-</span> <span class="fu">downsample_gaze</span>(gaze_sub_L2_long, <span class="at">bin.length=</span><span class="dv">100</span>, <span class="at">timevar=</span><span class="st">"time"</span>, <span class="at">aggvars=</span><span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We need to make sure we only have one unrelated value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make only one unrelated condition</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_id <span class="ot">&lt;-</span> gaze_sub_id <span class="sc">%&gt;%</span> </span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">condition1 =</span> <span class="fu">ifelse</span>(condition1<span class="sc">==</span><span class="st">"unrelated2"</span>, <span class="st">"unrelated"</span>, condition1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualizing-time-course-data" class="level2">
<h2 data-anchor-id="visualizing-time-course-data">Visualizing Time Course Data</h2>
<p>To simplify plotting your time-course data, we have created the <code>plot_IA_proportions()</code> function. This function takes several arguments. The <code>ia_column</code> argument specifies the column containing your AOI labels. The <code>time_column</code> argument requires the name of your time bin column, and the <code>proportion_column</code> argument specifies the column containing fixation or look proportions. Additional arguments allow you to specify custom names for each IA in the <code>ia_mapping</code> argument, enabling you to label them as desired. In order to use this function, you must use the <code>downsample_gaze()</code> function.</p>
<p>Below, we have plotted the time-course data for each condition in <a href="#fig-L2comp" class="quarto-xref" aria-expanded="false">Figure&nbsp;7</a>. By default, the graphs utilize a color-blind-friendly palette from the <code>ggokabeito</code> package <span class="citation" data-cites="ggokabeito">(Barrett, 2021)</span>. However, you can set the argument <code>use_color</code> = FALSE to generate a non-colored version of the figure, where different line types and shapes differentiate conditions. Additionally, since these are ggplot objects, you can further customize them as needed to suit your analysis or presentation preferences.</p>
<div class="cell FigureWithoutNote" data-fig-heigh="10" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-L2comp" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="7">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-L2comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;7</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Comparison of L2 competition effect in the No Competitor (a), Spanish-English (b), the Spanish-Spanish (c) conditions</p>
</div>
</figcaption>
<div aria-describedby="fig-L2comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-L2comp-1.svg" class="img-fluid figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="gorilla-provided-coordinates" class="level2">
<h2 data-anchor-id="gorilla-provided-coordinates">Gorilla Provided Coordinates</h2>
<p>Thus far, we have used the coordinates representing the four quadrants of the screen. However, Gorilla provides their own quadrants representing image location on the screen. To the authors’ knowledge, these quadrants have not been looked at in any studies reporting eye-tracking results. Let’s examine how reasonable our results are with the Gorilla provided coordinates.</p>
<p>We will use the function <code>extract_aois()</code> to get the standardized coordinates for each quadrant on screen. You can use the <code>zone_names</code> argument to get the zones you want to use. In our example, we want the <code>TL</code>, <code>BR</code>, <code>BL</code> <code>TR</code> coordinates. We input the object from above <code>vwp_paths_filtered_L2</code> that contains all our eye-tracking files and extract the coordinates we want. These are labeled in <a href="#tbl-gorgaze" class="quarto-xref" aria-expanded="false">Table&nbsp;6</a>. In <a href="#fig-gor-L2" class="quarto-xref" aria-expanded="false">Figure&nbsp;8</a> we can see that the AOIs are a bit smaller than then when using the quadrant approach. We can take these coordinates and use them in our analysis.Looking at <a href="#fig-L2comp-gor" class="quarto-xref" aria-expanded="false">Figure&nbsp;9</a>, we see the data is a bit noisier than the quadrant approach, but the curves are reasonable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the extract_aois fucntion</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>aois_L2 <span class="ot">&lt;-</span> <span class="fu">extract_aois</span>(vwp_paths_filtered_L2, <span class="at">zone_names =</span>  <span class="fu">c</span>(<span class="st">"TL"</span>, <span class="st">"BR"</span>, <span class="st">"TR"</span>, <span class="st">"BL"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="6" data-custom-style="FigureWithoutNote">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>aois_L2 <span class="sc">%&gt;%</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">flextable</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>     <span class="fu">fontsize</span>(<span class="at">size =</span> <span class="dv">12</span>) <span class="sc">%&gt;%</span>  <span class="co"># Reduce font size</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">padding</span>(<span class="at">padding =</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">font</span>(<span class="at">fontname =</span> <span class="st">"Times New Roman"</span>, <span class="at">part =</span> <span class="st">"all"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_table_properties</span>(<span class="at">layout=</span><span class="st">"autofit"</span>) <span class="sc">%&gt;%</span> <span class="co"># Reduce padding inside cells</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autofit</span>() <span class="sc">%&gt;%</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_apa</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="tbl-gorgaze" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="6">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-gorgaze-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;6</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Gorilla provided standardized gaze coordinates</p>
</div>
</figcaption>
<div aria-describedby="tbl-gorgaze-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-cde4c0a0{table-layout:auto;}.cl-cddf265e{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cddf2672{font-family:'Times New Roman';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cde18516{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-cde18520{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 2;background-color:transparent;}.cl-cde19e48{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e49{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e52{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e53{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e5c{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e66{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e67{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e68{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e70{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e71{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e72{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e7a{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e7b{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e7c{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e84{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e85{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e86{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cde19e8e{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-cde4c0a0"><thead><tr style="overflow-wrap:break-word;"><th class="cl-cde19e48"><p class="cl-cde18516"><span class="cl-cddf265e">loc</span></p></th><th class="cl-cde19e49"><p class="cl-cde18516"><span class="cl-cddf265e">x_normalized</span></p></th><th class="cl-cde19e49"><p class="cl-cde18516"><span class="cl-cddf265e">y_normalized</span></p></th><th class="cl-cde19e52"><p class="cl-cde18516"><span class="cl-cddf265e">width_normalized</span></p></th><th class="cl-cde19e53"><p class="cl-cde18516"><span class="cl-cddf265e">height_normalized</span></p></th><th class="cl-cde19e5c"><p class="cl-cde18516"><span class="cl-cddf265e">xmin</span></p></th><th class="cl-cde19e5c"><p class="cl-cde18516"><span class="cl-cddf265e">ymin</span></p></th><th class="cl-cde19e66"><p class="cl-cde18516"><span class="cl-cddf265e">xmax</span></p></th><th class="cl-cde19e66"><p class="cl-cde18516"><span class="cl-cddf265e">ymax</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-cde19e67"><p class="cl-cde18520"><span class="cl-cddf2672">BL</span></p></td><td class="cl-cde19e68"><p class="cl-cde18520"><span class="cl-cddf2672">0.03</span></p></td><td class="cl-cde19e68"><p class="cl-cde18520"><span class="cl-cddf2672">0.04</span></p></td><td class="cl-cde19e70"><p class="cl-cde18520"><span class="cl-cddf2672">0.26</span></p></td><td class="cl-cde19e71"><p class="cl-cde18520"><span class="cl-cddf2672">0.25</span></p></td><td class="cl-cde19e72"><p class="cl-cde18520"><span class="cl-cddf2672">0.03</span></p></td><td class="cl-cde19e72"><p class="cl-cde18520"><span class="cl-cddf2672">0.04</span></p></td><td class="cl-cde19e7a"><p class="cl-cde18520"><span class="cl-cddf2672">0.29</span></p></td><td class="cl-cde19e7a"><p class="cl-cde18520"><span class="cl-cddf2672">0.29</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cde19e67"><p class="cl-cde18520"><span class="cl-cddf2672">TL</span></p></td><td class="cl-cde19e68"><p class="cl-cde18520"><span class="cl-cddf2672">0.02</span></p></td><td class="cl-cde19e68"><p class="cl-cde18520"><span class="cl-cddf2672">0.74</span></p></td><td class="cl-cde19e70"><p class="cl-cde18520"><span class="cl-cddf2672">0.26</span></p></td><td class="cl-cde19e71"><p class="cl-cde18520"><span class="cl-cddf2672">0.25</span></p></td><td class="cl-cde19e72"><p class="cl-cde18520"><span class="cl-cddf2672">0.02</span></p></td><td class="cl-cde19e72"><p class="cl-cde18520"><span class="cl-cddf2672">0.74</span></p></td><td class="cl-cde19e7a"><p class="cl-cde18520"><span class="cl-cddf2672">0.28</span></p></td><td class="cl-cde19e7a"><p class="cl-cde18520"><span class="cl-cddf2672">0.99</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cde19e67"><p class="cl-cde18520"><span class="cl-cddf2672">TR</span></p></td><td class="cl-cde19e68"><p class="cl-cde18520"><span class="cl-cddf2672">0.73</span></p></td><td class="cl-cde19e68"><p class="cl-cde18520"><span class="cl-cddf2672">0.75</span></p></td><td class="cl-cde19e70"><p class="cl-cde18520"><span class="cl-cddf2672">0.24</span></p></td><td class="cl-cde19e71"><p class="cl-cde18520"><span class="cl-cddf2672">0.24</span></p></td><td class="cl-cde19e72"><p class="cl-cde18520"><span class="cl-cddf2672">0.73</span></p></td><td class="cl-cde19e72"><p class="cl-cde18520"><span class="cl-cddf2672">0.75</span></p></td><td class="cl-cde19e7a"><p class="cl-cde18520"><span class="cl-cddf2672">0.97</span></p></td><td class="cl-cde19e7a"><p class="cl-cde18520"><span class="cl-cddf2672">0.99</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-cde19e7b"><p class="cl-cde18520"><span class="cl-cddf2672">BR</span></p></td><td class="cl-cde19e7c"><p class="cl-cde18520"><span class="cl-cddf2672">0.73</span></p></td><td class="cl-cde19e7c"><p class="cl-cde18520"><span class="cl-cddf2672">0.06</span></p></td><td class="cl-cde19e84"><p class="cl-cde18520"><span class="cl-cddf2672">0.23</span></p></td><td class="cl-cde19e85"><p class="cl-cde18520"><span class="cl-cddf2672">0.25</span></p></td><td class="cl-cde19e86"><p class="cl-cde18520"><span class="cl-cddf2672">0.73</span></p></td><td class="cl-cde19e86"><p class="cl-cde18520"><span class="cl-cddf2672">0.06</span></p></td><td class="cl-cde19e8e"><p class="cl-cde18520"><span class="cl-cddf2672">0.96</span></p></td><td class="cl-cde19e8e"><p class="cl-cde18520"><span class="cl-cddf2672">0.31</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-gor-L2" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="8">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-gor-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;8</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Gorilla provided standardized coordinates for the four quadrants on the screen</p>
</div>
</figcaption>
<div aria-describedby="fig-gor-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-gor-L2-1.svg" class="img-fluid figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>assign_L2_gor <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">assign_aoi</span>(dat_colnames_L2,<span class="at">X=</span><span class="st">"x"</span>, <span class="at">Y=</span><span class="st">"y"</span>,<span class="at">aoi_loc =</span> aois_L2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-L2comp-gor" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="9">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-L2comp-gor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;9</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Comparison of competition effects with Gorilla standardized coordinates</p>
</div>
</figcaption>
<div aria-describedby="fig-L2comp-gor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-L2comp-gor-1.svg" class="img-fluid figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="modeling-data" class="level2">
<h2 data-anchor-id="modeling-data">Modeling Data</h2>
<p>Once the data have been preprocessed, the next step is analysis. A variety of analytic approaches are available for VWP data, including growth curve analysis (GCA), cluster permutation analysis (CPA), generalized additive mixed models (GAMMs), logistic multilevel models, and divergent point analysis (DPA). Fortunately, there is a wealth of excellent resources and tutorials demonstrating how to apply these methods to both lab-based <span class="citation" data-cites="ito2023 stone2021 coretta2024 mirmanGrowthCurveAnalysis seedorff2018">(Coretta &amp; Casillas, 2024; see Ito &amp; Knoeferle, 2023; Mirman &amp; CRC Press., n.d.; Seedorff et al., 2018; Stone et al., 2021)</span> and online <span class="citation" data-cites="bramlett2024">(see Bramlett &amp; Wiener, 2024)</span> visual world eye-tracking data.</p>
<p>This paper’s goal, however, is to not evaluate different analytic approaches and tell readers what they should use. All methods have their strengths and weaknesses <span class="citation" data-cites="ito2023">(see Ito &amp; Knoeferle, 2023)</span>. Nevertheless, statistical modeling should be guided by the questions researchers have and thus serious thought needs to be given to the proper analysis. In the VWP, there are two general questions one might be interested in: (1) Are there any overall difference in fixations between conditions and (2) Are there any time course differences in fixations between conditions (and/or groups).</p>
<p>With our data, one question we might want to answer is if there are any fixation differences between the cohort and unrelated conditions across the time course. One statistical approach we chose to highlight to answer this question is a cluster permutation analysis (CPA). The CPA is suitable for testing differences between two conditions or groups over an interest period while controlling for multiple comparisons and autocorrelation. Given the time latency issues common in webcam-basted studies, <span class="citation" data-cites="slim2024">Slim et al. (2024)</span> recommended using an approach like CPA.</p>
<section id="cpa" class="level3">
<h3 data-anchor-id="cpa">CPA</h3>
<p>CPA is a technique that has become increasingly popular, particularly in the field of cognitive neuropsychology, for analyzing MEG and EEG data <span class="citation" data-cites="maris2007">(Maris &amp; Oostenveld, 2007)</span>. While its adoption in VWP studies has been relatively slow, it is now beginning to appear more frequently <span class="citation" data-cites="huang2020 ito2023">(see Huang &amp; Snedeker, 2020; Ito &amp; Knoeferle, 2023)</span>. Notably, its use is growing in online eye-tracking studies <span class="citation" data-cites="slim2024 slim2023 vos2022">(see Slim et al., 2024; Slim &amp; Hartsuiker, 2023; Vos et al., 2022)</span>.</p>
<p>Before we show you how to apply this method to the current dataset, we want to briefly explain what CPA is. The CPA is a data-driven approach that increases statistical power while controlling for Type I errors across multiple comparisons—exactly what we need when analyzing fixations across the time course.</p>
<p>The clustering procedure involves three main steps:</p>
<ol type="1">
<li><p>Cluster Formation: With our data, a multilevel logistic model is conducted for every data point (condition by time). Please note that any statistical test can be run here. Adjacent data points that surpass the mass univariate significance threshold (e.g., p &lt; .05) are combined into clusters. The cluster-level statistic, typically the sum of the t-values (or F-values) within the cluster, is computed labeled as SumStatitic is output below). By clustering adjacent significant data points, this step accounts for autocorrelation by considering temporal dependencies rather than treating each data point as independent.</p></li>
<li><p>Null Distribution Creation: Next, the same analysis is run as in step 1. However, the analysis is based on randomly permuting or shuffling the conditions within subjects. This principle of exchangeability is important here, as it suggests that the condition labels can be exchanged without altering the underlying data structure. This randomization is repeated n times (e.g., 1000 shuffles), and for each permutation, the cluster-level statistic is computed. This step addresses the issue of multiple comparisons by constructing a distribution of cluster-level statistics under the null hypothesis, providing a baseline against which observed cluster statistics can be compared. By doing so, the method controls the family-wise error rate and ensures that significant findings are not simply due to chance.</p></li>
<li><p>Significance Testing: The cluster-level statistics from the observed (real) comparison is compared to the null distribution we created above Clusters with statistics falling in the highest or lowest 2.5% of the null distribution are considered significant (e.g., <em>p</em> &lt; 0.05).</p></li>
</ol>
<p>To perform CPA, we will load in the <code>permutes</code> <span class="citation" data-cites="permutes">(Voeten, 2023)</span>, <code>permuco</code> <span class="citation" data-cites="permuco">(Frossard &amp; Renaud, 2021)</span>, <code>foreach</code> <span class="citation" data-cites="foreach">( &amp; Weston, 2022)</span>, and <code>Parallel</code> <span class="citation" data-cites="doParallel">(Corporation &amp; Weston, 2022)</span> packages in R. Loading these packages allow us to use the <code>cluster.glmer()</code>function to run a cluster permutation (10,000 rimes) across multiple system cores to speed up the process. We run a CPA on the <code>gaze_sub_id</code> object where each row in <code>Looks</code> denotes whether the AOI was fixated, with values of zero (not fixated) or one (fixated).</p>
<p>Below you find sample code to perform multilevel CPA in R (please see the Github repository for elaborated code needed to perform CPA.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(permutes) <span class="co"># cpa</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(permuco) <span class="co"># cpa</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>total_perms <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>cpa.lme <span class="ot">&lt;-</span>  permutes<span class="sc">::</span><span class="fu">clusterperm.glmer</span>(Looks<span class="sc">~</span> condition1_code <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>subject) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>trial), <span class="at">data=</span>gaze_sub_L2_cp1, <span class="at">series.var=</span><span class="sc">~</span>time_bin, <span class="at">nperm =</span> total_perms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="7" data-custom-style="FigureWithoutNote">
<div id="tbl-clustermass" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="7">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;7</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Clustermass statistics for the Spanish-Spanish condition</p>
</div>
</figcaption>
<div aria-describedby="tbl-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-d038ef52{table-layout:auto;}.cl-d03378b0{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d03378ce{font-family:'Times New Roman';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d035d416{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d035d420{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d035d42a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:2pt;padding-right:2pt;line-height: 1;background-color:transparent;}.cl-d035d42b{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:2pt;padding-right:2pt;line-height: 1;background-color:transparent;}.cl-d035ed34{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed3e{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed48{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed52{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed5c{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed5d{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed66{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed67{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed70{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed7a{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed7b{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed84{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed8e{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed8f{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035ed98{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035eda2{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035edac{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d035edad{background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-d038ef52"><thead><tr style="overflow-wrap:break-word;"><th class="cl-d035ed34"><p class="cl-d035d416"><span class="cl-d03378b0">cluster</span></p></th><th class="cl-d035ed3e"><p class="cl-d035d416"><span class="cl-d03378b0">cluster_mass</span></p></th><th class="cl-d035ed48"><p class="cl-d035d416"><span class="cl-d03378b0">p.cluster_mass</span></p></th><th class="cl-d035ed52"><p class="cl-d035d416"><span class="cl-d03378b0">bin_start</span></p></th><th class="cl-d035ed5c"><p class="cl-d035d416"><span class="cl-d03378b0">bin_end</span></p></th><th class="cl-d035ed5d"><p class="cl-d035d416"><span class="cl-d03378b0">t</span></p></th><th class="cl-d035ed66"><p class="cl-d035d420"><span class="cl-d03378b0">sign</span></p></th><th class="cl-d035ed67"><p class="cl-d035d416"><span class="cl-d03378b0">time_start</span></p></th><th class="cl-d035ed70"><p class="cl-d035d416"><span class="cl-d03378b0">time_end</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-d035ed7a"><p class="cl-d035d42a"><span class="cl-d03378ce">1</span></p></td><td class="cl-d035ed7b"><p class="cl-d035d42a"><span class="cl-d03378ce">236.34</span></p></td><td class="cl-d035ed84"><p class="cl-d035d42a"><span class="cl-d03378ce">0</span></p></td><td class="cl-d035ed8e"><p class="cl-d035d42a"><span class="cl-d03378ce">7</span></p></td><td class="cl-d035ed8f"><p class="cl-d035d42a"><span class="cl-d03378ce">13</span></p></td><td class="cl-d035ed98"><p class="cl-d035d42a"><span class="cl-d03378ce">5.48</span></p></td><td class="cl-d035eda2"><p class="cl-d035d42b"><span class="cl-d03378ce">1</span></p></td><td class="cl-d035edac"><p class="cl-d035d42a"><span class="cl-d03378ce">500</span></p></td><td class="cl-d035edad"><p class="cl-d035d42a"><span class="cl-d03378ce">1,100</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-clustermass" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="10">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;10</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Average looks in the cross-linguistic VWP task over time for the Spanish-Spanish condition (a) and the Spanish-English condition (b). The shaded rectangles indicate when cohort looks were greater than chance based on the CPA.</p>
</div>
</figcaption>
<div aria-describedby="fig-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_revised1_files/figure-html/fig-clustermass-1.svg" class="img-fluid figure-img" width="768">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>In the analysis for the Spanish-Spanish condition, one significant cluster was observed between 500 and 1,100 ms, as indicated in the summary statistics from <a href="#tbl-clustermass" class="quarto-xref" aria-expanded="false">Table&nbsp;7</a>. The positive <code>SumStatistic</code> value associated with this cluster suggests that competition was greater during this time window. This result implies that cohorts in the Spanish-Spanish condition exhibited stronger effects or competition compared to unrelated items. In <a href="#fig-clustermass" class="quarto-xref" aria-expanded="false">Figure&nbsp;10</a> significant clusters are highlighted for both the Spanish-Spanish and Spanish-English conditions. Both conditions show one significant cluster. Overall, the analysis suggests that both the Spanish-Spanish and Spanish-English conditions demonstrate significant competitor effects.</p>
</div>
<section id="effect-size" class="level4">
<h4 data-anchor-id="effect-size">Effect Size.</h4>
<p>It is important to address the issue of effect sizes in the context of CPA. Calculating effect sizes for CPA is not straightforward, as the technique is designed to evaluate temporal clusters rather than individual time points. <span class="citation" data-cites="slim2024 meyer2021">(Slim et al., 2024; but also see Meyer et al., 2021)</span> outline three possible approaches for estimating effect sizes in CPA: (1) computing the effect size within a predefined time window (often the same window used for identifying clusters), (2) calculating an average effect size across the entire cluster, and (3) reporting the maximum effect observed within the cluster. Each method has its trade-offs in terms of interpretability and comparability across studies, and the choice should be guided by theoretical considerations and the research question at hand.</p>
</section>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Webcam eye-tracking is a relatively nascent technology, and as such, there is limited guidance available for researchers. To ameliorate this, we created a tutorial to assist new users of visual world webcam eye-tracking, using some of the best practices available <span class="citation" data-cites="bramlett2024">(e.g., Bramlett &amp; Wiener, 2024)</span>. To further facilitate this process, we created the <code>webgazeR</code> package, which contains several helper functions designed to streamline data preprocessing, analysis, and visualization.</p>
<p>In this tutorial, we covered the basic steps of running a visual world webcam-based eye-tracking experiment. We highlighted these steps by using data from a cross-linguistic VWP looking at competitive processes in L2 speakers of Spanish. Specifically, we attempted to replicate the experiment by <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> where they observed within- and between L2/L1 competition using carefully crafted materials.</p>
<section id="replication-of-sarrett2022" class="level2">
<h2 data-anchor-id="replication-of-sarrett2022">Replication of <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span></h2>
<p>While the main purpose of this tutorial was to highlight the steps needed to analyze webcam eye-tracking data, replicating <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> allowed us to not only assess whether within and between L2/L1 competition can be found in a spoken word recognition VWP experiment online, but also provide insight in how to run VWP studies online and the issues associated with it.</p>
<p>Our conceptual replication yielded highly encouraging results, revealing robust competition effects both within-language (Spanish-Spanish) and across-language (Spanish-English) conditions—closely mirroring those reported by <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span>. However, several key analytic, methodological, and sample differences between our study and theirs warrant discussion.</p>
<p>A major analytic difference lies in how the time course of competition was examined. While <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> employed a non-linear curve-fitting approach <span class="citation" data-cites="mcmurray2010">(see McMurray et al., 2010)</span>, we used cluster-based permutation analysis (CPA). This methodological distinction limits direct comparisons regarding the temporal dynamics of competition. Nonetheless, the overall time course patterns align surprisingly well: our CPA identified a significant cluster starting at 500 ms, while <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> observed effects beginning around 400 ms—suggesting a modest delay of approximately 100 ms in our online data. This delay is still markedly smaller than in previous webcam-based studies <span class="citation" data-cites="semmelmann2018 slim2024">(e.g., Semmelmann &amp; Weigelt, 2018; Slim et al., 2024)</span>, reflecting progress in online eye-tracking. That said, it’s important to note that CPA is not ideally suited for making precise temporal inferences about onset or offset of effects <span class="citation" data-cites="fields2019 ito2023">(Fields &amp; Kuperberg, 2019; Ito &amp; Knoeferle, 2023)</span>.</p>
<p>Design differences between the studies also play a critical role. In <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span>, participants previewed the images in each quadrant for 1000 ms, followed by the appearance of a central red dot they clicked to trigger audio playback. After selecting the target, a 250 ms inter-trial interval (ITI) preceded the next trial.</p>
<p>In contrast, our sequence began with a 500 ms fixation cross (serving as the ITI), followed by a longer 1500 ms preview. The images then disappeared, and participants clicked a centrally placed start button to initiate audio playback, at which point the images reappeared. Upon target selection, the next trial began immediately. We also imposed a 5-second timeout for non-responses. Additionally, our study included 250 trials—fewer than the 450 in the original study<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>—but still more than most webcam-based research. Despite the reduced trial count, we observed parallel competition effects in both language conditions, underscoring the robustness of the findings.</p>
<p>Several motivations guided these design adaptations. Online testing introduces greater variability in participants’ setups (e.g., device type, connection quality), so we opted for a longer preview period to enhance the likelihood of observing competition effects. Prior work suggests this can boost competition signals in the VWP <span class="citation" data-cites="apfelbaum2021">(Apfelbaum et al., 2021)</span>. The start-button mechanism ensured trials began from a centralized gaze position, helping minimize quadrant-based bias. Finally, the timeout feature helped mitigate issues of inattention common in unsupervised online environments.</p>
<p>Participant recruitment also differed. <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> recruited students from a Spanish language course and assessed proficiency using the LexTALE-Spanish test <span class="citation" data-cites="izura2014">(Izura et al., 2014)</span>. Our participants were recruited through Prolific with more limited screening, allowing us only to filter by native language and reported experience with another language. This constraint likely contributed to differences in language profiles between samples. Whereas <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> included L2 learners with verified proficiency, our sample encompassed a broader and more variable group of L2 speakers, with limited verification of language skills (see <a href="#tbl-demo2" class="quarto-xref" aria-expanded="false">Table&nbsp;1</a> for details). This broader variability may help explain the absence of a sustained cohort competition effect in our study.</p>
<p>In sum, while there are notable differences in methods and samples, the convergence of competition effects across both studies—within and across languages—supports the robustness of these phenomena across diverse research contexts. Still, we view these results as a promising step rather than definitive evidence. A more systematic investigation is needed to fully establish the generalizability of these effects.</p>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="8" data-custom-style="FigureWithoutNote">
<div id="tbl-question" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="8">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-question-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;8</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Eye-tracking questionnaire items</p>
</div>
</figcaption>
<div aria-describedby="tbl-question-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<!-- preamble start -->

    <script>

      function styleCell_jh35aqeq1n3r33b7jnvd(i, j, css_id) {
          var table = document.getElementById("tinytable_jh35aqeq1n3r33b7jnvd");
          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors
          if (cell) {
              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);
              cell.classList.add(css_id);
          } else {
              console.warn(`Cell at (${i}, ${j}) not found.`);
          }
      }
      function insertSpanRow(i, colspan, content) {
        var table = document.getElementById('tinytable_jh35aqeq1n3r33b7jnvd');
        var newRow = table.insertRow(i);
        var newCell = newRow.insertCell(0);
        newCell.setAttribute("colspan", colspan);
        // newCell.innerText = content;
        // this may be unsafe, but innerText does not interpret <br>
        newCell.innerHTML = content;
      }
      function spanCell_jh35aqeq1n3r33b7jnvd(i, j, rowspan, colspan) {
        var table = document.getElementById("tinytable_jh35aqeq1n3r33b7jnvd");
        const targetRow = table.rows[i];
        const targetCell = targetRow.cells[j];
        for (let r = 0; r < rowspan; r++) {
          // Only start deleting cells to the right for the first row (r == 0)
          if (r === 0) {
            // Delete cells to the right of the target cell in the first row
            for (let c = colspan - 1; c > 0; c--) {
              if (table.rows[i + r].cells[j + c]) {
                table.rows[i + r].deleteCell(j + c);
              }
            }
          }
          // For rows below the first, delete starting from the target column
          if (r > 0) {
            for (let c = colspan - 1; c >= 0; c--) {
              if (table.rows[i + r] && table.rows[i + r].cells[j]) {
                table.rows[i + r].deleteCell(j);
              }
            }
          }
        }
        // Set rowspan and colspan of the target cell
        targetCell.rowSpan = rowspan;
        targetCell.colSpan = colspan;
      }
      // tinytable span after
      window.addEventListener('load', function () {
          var cellsToStyle = [
            // tinytable style arrays after
          { positions: [ { i: 13, j: 0 },  ], css_id: 'tinytable_css_iojgg7fweefq1uoylqpl',}, 
          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_94a642iu2let6v5f1ow4',}, 
          ];

          // Loop over the arrays to style the cells
          cellsToStyle.forEach(function (group) {
              group.positions.forEach(function (cell) {
                  styleCell_jh35aqeq1n3r33b7jnvd(cell.i, cell.j, group.css_id);
              });
          });
      });
    </script>

    <style>
      /* tinytable css entries after */
      .table td.tinytable_css_iojgg7fweefq1uoylqpl, .table th.tinytable_css_iojgg7fweefq1uoylqpl { border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_94a642iu2let6v5f1ow4, .table th.tinytable_css_94a642iu2let6v5f1ow4 { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
    </style>
    <div class="container">
      <table class="table table-borderless" id="tinytable_jh35aqeq1n3r33b7jnvd" style="table-layout: fixed; width: 90% !important; margin-left: auto; margin-right: auto;" data-quarto-disable-processing="true">
        <thead>
        
              <tr>
                <th scope="col">Question</th>
              </tr>
        </thead>
        
        <tbody>
                <tr>
                  <td>1.    Do you have a history of vision problems (e.g., corrected vision, eye disease, or drooping eyelids)?                                                   </td>
                </tr>
                <tr>
                  <td>2.    Are you on any medications currently that can impair your judgement?                                                                                   </td>
                </tr>
                <tr>
                  <td>If yes, please list below:                                                                                                                                 </td>
                </tr>
                <tr>
                  <td>4.    Does your room currently have natural light?                                                                                                           </td>
                </tr>
                <tr>
                  <td>5.    Are you using the built in camera?                                                                                                                     </td>
                </tr>
                <tr>
                  <td>If no, what brand of camera are you using?                                                                                                                 </td>
                </tr>
                <tr>
                  <td>6.    Please estimate how far you think you were sitting from the camera during the experiment (an arm's length from your monitor is about 20 inches (51 cm).</td>
                </tr>
                <tr>
                  <td>7.    Approximately how many times did you look at your phone during the experiment?                                                                         </td>
                </tr>
                <tr>
                  <td>8.    Approximately how many times did you get up during the experiment?                                                                                     </td>
                </tr>
                <tr>
                  <td>9.    Was the environment you took the experiment in distraction free?                                                                                       </td>
                </tr>
                <tr>
                  <td>10. When you had to calibrate, were the instructions clear?                                                                                                </td>
                </tr>
                <tr>
                  <td>11. What additional information would you add to help make things easier to understand?                                                                    </td>
                </tr>
                <tr>
                  <td>12. Are you wearing a mask?                                                                                                                                </td>
                </tr>
        </tbody>
      </table>
    </div>
<!-- hack to avoid NA insertion in last line -->
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="9" data-custom-style="FigureWithoutNote">
<div id="tbl-goodbad" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="9">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-goodbad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;9</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Responses to eye-tracking questions for participants who successfully calibrated (good) vs.&nbsp;participants who had trouble calibrating (bad)</p>
</div>
</figcaption>
<div aria-describedby="tbl-goodbad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<!-- preamble start -->

    <script>

      function styleCell_3tm0igczcjdo3f89350y(i, j, css_id) {
          var table = document.getElementById("tinytable_3tm0igczcjdo3f89350y");
          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors
          if (cell) {
              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);
              cell.classList.add(css_id);
          } else {
              console.warn(`Cell at (${i}, ${j}) not found.`);
          }
      }
      function insertSpanRow(i, colspan, content) {
        var table = document.getElementById('tinytable_3tm0igczcjdo3f89350y');
        var newRow = table.insertRow(i);
        var newCell = newRow.insertCell(0);
        newCell.setAttribute("colspan", colspan);
        // newCell.innerText = content;
        // this may be unsafe, but innerText does not interpret <br>
        newCell.innerHTML = content;
      }
      function spanCell_3tm0igczcjdo3f89350y(i, j, rowspan, colspan) {
        var table = document.getElementById("tinytable_3tm0igczcjdo3f89350y");
        const targetRow = table.rows[i];
        const targetCell = targetRow.cells[j];
        for (let r = 0; r < rowspan; r++) {
          // Only start deleting cells to the right for the first row (r == 0)
          if (r === 0) {
            // Delete cells to the right of the target cell in the first row
            for (let c = colspan - 1; c > 0; c--) {
              if (table.rows[i + r].cells[j + c]) {
                table.rows[i + r].deleteCell(j + c);
              }
            }
          }
          // For rows below the first, delete starting from the target column
          if (r > 0) {
            for (let c = colspan - 1; c >= 0; c--) {
              if (table.rows[i + r] && table.rows[i + r].cells[j]) {
                table.rows[i + r].deleteCell(j);
              }
            }
          }
        }
        // Set rowspan and colspan of the target cell
        targetCell.rowSpan = rowspan;
        targetCell.colSpan = colspan;
      }
      // tinytable span after
      window.addEventListener('load', function () {
          var cellsToStyle = [
            // tinytable style arrays after
          { positions: [ { i: 10, j: 0 }, { i: 10, j: 1 }, { i: 10, j: 2 }, { i: 10, j: 3 },  ], css_id: 'tinytable_css_oor861dnkeqgbzosnvvu',}, 
          { positions: [ { i: 0, j: 0 }, { i: 0, j: 1 }, { i: 0, j: 2 }, { i: 0, j: 3 },  ], css_id: 'tinytable_css_12fhk0ahskf7esrdgl2x',}, 
          ];

          // Loop over the arrays to style the cells
          cellsToStyle.forEach(function (group) {
              group.positions.forEach(function (cell) {
                  styleCell_3tm0igczcjdo3f89350y(cell.i, cell.j, group.css_id);
              });
          });
      });
    </script>

    <style>
      /* tinytable css entries after */
      .table td.tinytable_css_oor861dnkeqgbzosnvvu, .table th.tinytable_css_oor861dnkeqgbzosnvvu { border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_12fhk0ahskf7esrdgl2x, .table th.tinytable_css_12fhk0ahskf7esrdgl2x { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
    </style>
    <div class="container">
      <table class="table table-borderless" id="tinytable_3tm0igczcjdo3f89350y" style="table-layout: fixed; width: 90% !important; margin-left: auto; margin-right: auto;" data-quarto-disable-processing="true">
        <thead>
        
              <tr>
                <th scope="col">Question</th>
                <th scope="col">Response</th>
                <th scope="col">Good</th>
                <th scope="col">Bad</th>
              </tr>
        </thead>
        
        <tbody>
                <tr>
                  <td>1.    Do you have a history of vision problems (e.g., corrected vision, eye disease, or drooping eyelids)?</td>
                  <td>No </td>
                  <td> 65.71</td>
                  <td>64.29</td>
                </tr>
                <tr>
                  <td>1.    Do you have a history of vision problems (e.g., corrected vision, eye disease, or drooping eyelids)?</td>
                  <td>Yes</td>
                  <td> 34.29</td>
                  <td>35.71</td>
                </tr>
                <tr>
                  <td>2.    Are you on any medications currently that can impair your judgement?                                </td>
                  <td>No </td>
                  <td>100.00</td>
                  <td>98.21</td>
                </tr>
                <tr>
                  <td>2.    Are you on any medications currently that can impair your judgement?                                </td>
                  <td>Yes</td>
                  <td>  0.00</td>
                  <td> 1.79</td>
                </tr>
                <tr>
                  <td>4.    Does your room currently have natural light?                                                        </td>
                  <td>No </td>
                  <td> 40.00</td>
                  <td>26.79</td>
                </tr>
                <tr>
                  <td>4.    Does your room currently have natural light?                                                        </td>
                  <td>Yes</td>
                  <td> 60.00</td>
                  <td>73.21</td>
                </tr>
                <tr>
                  <td>5.    Are you using the built in camera?                                                                  </td>
                  <td>No </td>
                  <td> 14.29</td>
                  <td> 8.93</td>
                </tr>
                <tr>
                  <td>5.    Are you using the built in camera?                                                                  </td>
                  <td>Yes</td>
                  <td> 85.71</td>
                  <td>91.07</td>
                </tr>
                <tr>
                  <td>9.    Was the environment you took the experiment in distraction free?                                    </td>
                  <td>No </td>
                  <td> 11.43</td>
                  <td> 3.57</td>
                </tr>
                <tr>
                  <td>9.    Was the environment you took the experiment in distraction free?                                    </td>
                  <td>Yes</td>
                  <td> 88.57</td>
                  <td>96.43</td>
                </tr>
        </tbody>
      </table>
    </div>
<!-- hack to avoid NA insertion in last line -->
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="limitations" class="level2">
<h2 data-anchor-id="limitations">Limitations</h2>
<section id="recruitment-of-l2-speakers" class="level3">
<h3 data-anchor-id="recruitment-of-l2-speakers">Recruitment of L2 Speakers</h3>
<p>In this study, we used the Prolific platform to recruit L2 Spanish speakers. We specified criteria requiring participants to be native English speakers who were also proficient in Spanish, reside in the United States, and be between the ages of 18 and 36. These criteria yielded a potential recruitment pool of approximately 1,000 participants. While this number is larger than what is typically available for in-lab studies, it is still relatively limited given the overall size of the platform. Notably, English native speakers who are L2 learners of Spanish in the U.S. are not usually considered a particularly niche population, which highlights the extent of the recruitment difficulty. Participant pools are likely to be even more limited when targeting speakers of less commonly studied languages or with specific language backgrounds (e.g.&nbsp;heritage speakers). Moreover, Prolific currently supports only an English user interface, which makes it harder to recruit non-English speakers <span class="citation" data-cites="niedermann2024 patterson2023">(Niedermann et al., 2024; Patterson &amp; Nicklin, 2023)</span>. For second language research in particular, researchers should be aware of these and other constraints (such as the limited filtering options to control for proficiency) and consider incorporating language background questionnaires and/or proficiency tasks directly into the study design. Ultimately, 181 participants signed up for the study, and recruitment proved to be more challenging than expected. Researchers considering similar studies should be aware of these limitations when targeting niche populations, even on large online platforms. Despite these challenges, the final sample was sufficient for our planned analyses and opened up the possibility to target populations you would be unable to capture otherwise.</p>
</section>
<section id="generalizability-to-other-platforms" class="level3">
<h3 data-anchor-id="generalizability-to-other-platforms">Generalizability to Other Platforms</h3>
<p>We demonstrated how to analyze webcam eye-tracking data collected via the Gorilla platform using WebGazer.js. Although we did not validate this pipeline on other platforms that support WebGazer.js—such as PCIbex <span class="citation" data-cites="zehr2018penncontroller">(Zehr &amp; Schwarz, 2018)</span>, jsPsych <span class="citation" data-cites="deleeuw2015">(Leeuw, 2015)</span>, or PsychoPy <span class="citation" data-cites="peirce2019">(Peirce et al., 2019)</span>—we believe the pipeline is generalizable to these and to platforms that use other gaze estimation logarithms, such as Labvanced <span class="citation" data-cites="kaduk2024">(Kaduk et al., 2024)</span>. To support broader compatibility, the functions in the webgazeR package are designed to work with a variety of file types—including .csv, .tsv, and .xlsx – and work with any dataset that includes five essential columns: subject, trial, x, y, and time. We also provide a helper function, <code>make_webgazer()</code>, to assist in renaming columns so your dataset can be adapted to the expected format.</p>
<p>We encourage researchers to test this pipeline in their own studies and report any issues or suggestions on our GitHub repository. We are committed to improving <code>webgazeR</code> and welcome feedback that will make the package more flexible, user-friendly, and adaptable to a wider range of experimental platforms.</p>
</section>
<section id="power" class="level3">
<h3 data-anchor-id="power">Power</h3>
<p>While we successfully demonstrated competition effects similar to Sarrett’s study, we did not conduct an a priori power analysis nor was it our intention. With webcam eye-tracking, it has been recommended running twice the number of participants from the original sample, or powering the study to detect an effect size half as large as the original <span class="citation" data-cites="slim2023 vandercruyssen2024">(Slim &amp; Hartsuiker, 2023; Van der Cruyssen et al., 2024)</span>. We did attempt to increase our sample size 2x, but were unable to recruit enough participants through Prolific. However, our sample size is similar to the lab based study. Regardless, researchers should be aware of this and plan accordingly.</p>
<p>We strongly urge researchers to perform power analyses and justify their sample sizes <span class="citation" data-cites="lakens2022">(Lakens, 2022)</span>. While tools like G*Power <span class="citation" data-cites="faul2007">(Faul et al., 2007)</span> are available for this purpose, we recommend power simulations using Monte Carlo or resampling methods on pilot or sample data <span class="citation" data-cites="prystauka2024 slim2023">(see Prystauka et al., 2024; Slim &amp; Hartsuiker, 2023)</span>. Several excellent R packages, such as <code>mixedpower</code> <span class="citation" data-cites="kumle2021">(Kumle et al., 2021)</span> and <code>SIMR</code> <span class="citation" data-cites="green2016">(Green &amp; MacLeod, 2016)</span> make such simulations straightforward and accessible.</p>
</section>
</section>
<section id="recommendations-and-ways-forward" class="level2">
<h2 data-anchor-id="recommendations-and-ways-forward">Recommendations and Ways Forward</h2>
<p>While our findings support the promise of webcam eye-tracking for language research, several challenges remain that researchers should consider. One of the most significant issues is data loss due to poor calibration. In our study, we excluded approximately 75% of participants due to calibration failure. These attrition rates are in line with some previous reports <span class="citation" data-cites="slim2023">(e.g., Slim &amp; Hartsuiker, 2023)</span>, though others have found substantially lower rates <span class="citation" data-cites="prystauka2024 bramlett2025">(Bramlett &amp; Wiener, 2025; Prystauka et al., 2024)</span>. With this valuation, it is important to understand the factors that lead to better quality data.</p>
<p>To address this, we included a post-task questionnaire assessing participants’ setups and their experiences with the experiment. These questions, included in <a href="#tbl-question" class="quarto-xref" aria-expanded="false">Table&nbsp;8</a>, provide insights that informed the following recommendations, which we also base on our experimental design and personal experience.</p>
<p>In our experimental design, participants were branched based on whether they successfully completed the experiment or failed calibration at any point. <a href="#tbl-goodbad" class="quarto-xref" aria-expanded="false">Table&nbsp;9</a> highlights the comparisons between good and poor calibrators. For the sake of brevity, we will discuss some recommendations based on questionnaire responses and personal experience that will hopefully improve research using webcam eye-tracking.</p>
<section id="prioritize-external-webcams" class="level3">
<h3 data-anchor-id="prioritize-external-webcams">Prioritize External Webcams</h3>
<p>Our data suggest that participants using external webcams were significantly more likely to complete the calibration successfully than those using built-in laptop cameras. External webcams typically offer higher resolution and frame rates—both critical for accurate gaze estimation <span class="citation" data-cites="slim2023">(Slim &amp; Hartsuiker, 2023)</span> Researchers should, whenever possible, encourage participants to use external webcams and may consider administering a brief pre-experiment questionnaire to screen for webcam type and exclude low-quality setups.</p>
</section>
<section id="optimize-environmental-conditions" class="level3">
<h3 data-anchor-id="optimize-environmental-conditions">Optimize Environmental Conditions</h3>
<p>Poor calibration was often reported in environments with natural light. Ambient lighting introduces variability that can degrade tracking performance. We recommend that researchers instruct participants to complete studies in rooms with consistent artificial lighting and minimal glare or shadows.</p>
<p>In addition to lighting, head movement and distance from the screen are critical for achieving reliable eye-tracking. Excessive movement or leaning in and out of the camera’s view can disrupt the face mesh tracking used by WebGazer.js. Participants should be advised to remain still and maintain a consistent, moderate distance from the screen—approximately 50–70 cm, depending on their camera setup. We asked individuals to provide an approximate distance from their screens (arms length) but it is not clear how accurate this is. Providing clear guidance (e.g., via an instructional video) may help mitigate these issues and improve overall tracking fidelity.&nbsp;</p>
<p>A different platform, Labvanced <span class="citation" data-cites="kaduk2024">(Kaduk et al., 2024)</span> offers additional eye-tracking functionality including a virtual chinrest to ensure head movement is restricted to an acceptable range and warns users if they deviate from this range. Together this might make for a better eye-tracking experience with less data thrown out. This should be investigated further.</p>
</section>
<section id="conduct-a-priori-power-analysis" class="level3">
<h3 data-anchor-id="conduct-a-priori-power-analysis">Conduct a Priori Power Analysis</h3>
<p>To ensure adequate statistical power, researchers should conduct a priori power analyses either via GUI like GPower or perform Monte Carlo simulations/resampling on pilot data. This step is particularly important for online studies, where sample variability can be higher than in controlled lab environments. To this point, you will have to over-enroll your study due to the high attrition rate to reach your target goal, so please plan accordingly.</p>
</section>
<section id="collect-detailed-post-experiment-feedback" class="level3">
<h3 data-anchor-id="collect-detailed-post-experiment-feedback">Collect Detailed Post-Experiment Feedback</h3>
<p>Gathering detailed feedback about participants’ setups—such as webcam type, browser, lighting conditions, and perceived ease of use—can provide valuable information about what contributes to successful calibration. These insights can inform more effective participant instructions and refined inclusion criteria for future studies.</p>
<p>By implementing these strategies, researchers can improve the quality and consistency of data collected through webcam-based eye-tracking. These recommendations aim to maximize the utility and reproducibility of remote eye-tracking research, particularly in language processing contexts.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 data-anchor-id="conclusions">Conclusions</h2>
<p>This work highlights the steps required to process webcam eye-tracking data, demonstrating the potential of webcam-based eye-tracking for robust psycholinguistic experimentation. By providing a standardized pipeline for processing eye-tracking data, we aim to give researchers a clear and practical path for collecting and analyzing visual world webcam eye-tracking data. An interactive demo of the preprocessing pipeline—using data from a monolingual VWP—is available at the webgazeR website (https://jgeller112.github.io/webgazeR/vignettes/webgazeR_vignette.html), where users can explore the code and workflow firsthand.</p>
<p>Moreover, our findings demonstrate the feasibility of conducting high-quality online experiments, paving the way for future research to address more nuanced questions about L2 processing and language comprehension more broadly. Additionally, further refinement of webcam eye-tracking methodologies could enhance data precision and extend their applicability to more complex experimental designs. This is an exciting time for eye-tracking research, with its boundaries continuously expanding. We eagerly anticipate the advancements and possibilities that the future of webcam eye-tracking will bring.</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Allaire_Quarto_2024" class="csl-entry" role="listitem">
Allaire, J. J., Teague, C., Scheidegger, C., Xie, Y., Dervieux, C., &amp; Woodhull, G. (2024). <em><span>Quarto</span></em> (Version 1.6) [Computer software]. <a href="https://doi.org/10.5281/zenodo.5960048">https://doi.org/10.5281/zenodo.5960048</a>
</div>
<div id="ref-allopenna1998" class="csl-entry" role="listitem">
Allopenna, P. D., Magnuson, J. S., &amp; Tanenhaus, M. K. (1998). <em>Tracking the time c ourse of spoken word recognition using eye movements: Evidence for c ontinuous mapping models</em> (pp. 419–439).
</div>
<div id="ref-altmann1999" class="csl-entry" role="listitem">
Altmann, G. T. M., &amp; Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of subsequent reference. <em>Cognition</em>, <em>73</em>(3), 247–264. <a href="https://doi.org/10.1016/S0010-0277(99)00059-1">https://doi.org/10.1016/S0010-0277(99)00059-1</a>
</div>
<div id="ref-anderson2019" class="csl-entry" role="listitem">
Anderson, C. A., Allen, J. J., Plante, C., Quigley-McBride, A., Lovett, A., &amp; Rokkum, J. N. (2019). The MTurkification of Social and Personality Psychology. <em>Personality &amp; Social Psychology Bulletin</em>, <em>45</em>(6), 842–850. <a href="https://doi.org/10.1177/0146167218798821">https://doi.org/10.1177/0146167218798821</a>
</div>
<div id="ref-anwyl-irvine2020" class="csl-entry" role="listitem">
Anwyl-Irvine, A. L., Massonnié, J., Flitton, A., Kirkham, N., &amp; Evershed, J. K. (2020). Gorilla in our midst: An online behavioral experiment builder. <em>Behavior Research Methods</em>, <em>52</em>(1), 388–407. <a href="https://doi.org/10.3758/s13428-019-01237-x">https://doi.org/10.3758/s13428-019-01237-x</a>
</div>
<div id="ref-apfelbaum2021" class="csl-entry" role="listitem">
Apfelbaum, K. S., Klein-Packard, J., &amp; McMurray, B. (2021). The pictures who shall not be named: Empirical support for benefits of preview in the visual world paradigm. <em>Journal of Memory and Language</em>, <em>121</em>, 104279. <a href="https://doi.org/10.1016/j.jml.2021.104279">https://doi.org/10.1016/j.jml.2021.104279</a>
</div>
<div id="ref-ggokabeito" class="csl-entry" role="listitem">
Barrett, M. (2021). <em>Ggokabeito: ’Okabe-ito’ scales for ’ggplot2’ and ’ggraph’</em>. <a href="https://CRAN.R-project.org/package=ggokabeito">https://CRAN.R-project.org/package=ggokabeito</a>
</div>
<div id="ref-bianco2021" class="csl-entry" role="listitem">
Bianco, R., Mills, G., Kerangal, M. de, Rosen, S., &amp; Chait, M. (2021). Reward enhances online participants<span>’</span> engagement with a demanding auditory task. <em>Trends in Hearing</em>, <em>25</em>, 23312165211025941. <a href="https://doi.org/10.1177/23312165211025941">https://doi.org/10.1177/23312165211025941</a>
</div>
<div id="ref-blascheck2017" class="csl-entry" role="listitem">
Blascheck, T., Kurzhals, K., Raschke, M., Burch, M., Weiskopf, D., &amp; Ertl, T. (2017). Visualization of Eye Tracking Data: A Taxonomy and Survey. <em>Computer Graphics Forum</em>, <em>36</em>(8), 260–284. <a href="https://doi.org/10.1111/cgf.13079">https://doi.org/10.1111/cgf.13079</a>
</div>
<div id="ref-blasi2022" class="csl-entry" role="listitem">
Blasi, D. E., Henrich, J., Adamou, E., Kemmerer, D., &amp; Majid, A. (2022). Over-reliance on english hinders cognitive science. <em>Trends in Cognitive Sciences</em>, <em>26</em>(12), 1153–1170. <a href="https://doi.org/10.1016/j.tics.2022.09.015">https://doi.org/10.1016/j.tics.2022.09.015</a>
</div>
<div id="ref-bogdan2024" class="csl-entry" role="listitem">
Bogdan, P. C., Dolcos, S., Buetti, S., Lleras, A., &amp; Dolcos, F. (2024). Investigating the suitability of online eye tracking for psychological research: Evidence from comparisons with in-person data using emotion<span></span>attention interaction tasks. <em>Behavior Research Methods</em>, <em>56</em>(3), 2213–2226. <a href="https://doi.org/10.3758/s13428-023-02143-z">https://doi.org/10.3758/s13428-023-02143-z</a>
</div>
<div id="ref-vanboxtel2024" class="csl-entry" role="listitem">
Boxtel, W. S. van, Linge, M., Manning, R., Haven, L. N., &amp; Lee, J. (2024). Online eye tracking for aphasia: A feasibility study comparing web and lab tracking and implications for clinical use. <em>Brain and Behavior</em>, <em>14</em>(11), e70112. <a href="https://doi.org/10.1002/brb3.70112">https://doi.org/10.1002/brb3.70112</a>
</div>
<div id="ref-bramlett2024" class="csl-entry" role="listitem">
Bramlett, A. A., &amp; Wiener, S. (2024). The art of wrangling. <em>Linguistic Approaches to Bilingualism</em>. https://doi.org/<a href="https://doi.org/10.1075/lab.23071.bra">https://doi.org/10.1075/lab.23071.bra</a>
</div>
<div id="ref-bramlett2025" class="csl-entry" role="listitem">
Bramlett, A. A., &amp; Wiener, S. (2025). Individual differences modulate prediction of Italian words based on lexical stress: a close replication and LASSO extension of Sulpizio and McQueen (2012). <em>Journal of Cultural Cognitive Science</em>, <em>9</em>(1), 55–81. <a href="https://doi.org/10.1007/s41809-024-00162-6">https://doi.org/10.1007/s41809-024-00162-6</a>
</div>
<div id="ref-brysbaert2018" class="csl-entry" role="listitem">
Brysbaert, M., &amp; Stevens, M. (2018). Power analysis and effect size in mixed effects models: A tutorial. <em>Journal of Cognition</em>, <em>1</em>(1). <a href="https://doi.org/10.5334/joc.10">https://doi.org/10.5334/joc.10</a>
</div>
<div id="ref-bylund2024" class="csl-entry" role="listitem">
Bylund, E., Khafif, Z., &amp; Berghoff, R. (2024). Linguistic and geographic diversity in research on second language acquisition and multilingualism: An analysis of selected journals. <em>Applied Linguistics</em>, <em>45</em>(2), 308–329. <a href="https://doi.org/10.1093/applin/amad022">https://doi.org/10.1093/applin/amad022</a>
</div>
<div id="ref-carter2020" class="csl-entry" role="listitem">
Carter, B. T., &amp; Luke, S. G. (2020). Best practices in eye tracking research. <em>International Journal of Psychophysiology</em>, <em>155</em>, 49–62. <a href="https://doi.org/10.1016/j.ijpsycho.2020.05.010">https://doi.org/10.1016/j.ijpsycho.2020.05.010</a>
</div>
<div id="ref-chen-sankey2023" class="csl-entry" role="listitem">
Chen-Sankey, J., Elhabashy, M., Gratale, S., Geller, J., Mercincavage, M., Strasser, A. A., Delnevo, C. D., Jeong, M., &amp; Wackowski, O. A. (2023). Examining Visual Attention to Tobacco Marketing Materials Among Young Adult Smokers: Protocol for a Remote Webcam-Based Eye-Tracking Experiment. <em>JMIR Research Protocols</em>, <em>12</em>, e43512. <a href="https://doi.org/10.2196/43512">https://doi.org/10.2196/43512</a>
</div>
<div id="ref-colby2023" class="csl-entry" role="listitem">
Colby, S. E., &amp; McMurray, B. (2023). Efficiency of spoken word recognition slows across the adult lifespan. <em>Cognition</em>, <em>240</em>, 105588. <a href="https://doi.org/10.1016/j.cognition.2023.105588">https://doi.org/10.1016/j.cognition.2023.105588</a>
</div>
<div id="ref-cooper1974" class="csl-entry" role="listitem">
Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. <em>Cognitive Psychology</em>, <em>6</em>(1), 84–107. <a href="https://doi.org/10.1016/0010-0285(74)90005-X">https://doi.org/10.1016/0010-0285(74)90005-X</a>
</div>
<div id="ref-coretta2024" class="csl-entry" role="listitem">
Coretta, S., &amp; Casillas, J. V. (2024). A tutorial on generalised additive mixed effects models for bilingualism research. <em>Linguistic Approaches to Bilingualism</em>. <a href="https://doi.org/10.1075/lab.23076.cor">https://doi.org/10.1075/lab.23076.cor</a>
</div>
<div id="ref-doParallel" class="csl-entry" role="listitem">
Corporation, M., &amp; Weston, S. (2022). <em>doParallel: Foreach parallel adaptor for the ’parallel’ package</em>. <a href="https://CRAN.R-project.org/package=doParallel">https://CRAN.R-project.org/package=doParallel</a>
</div>
<div id="ref-remotes" class="csl-entry" role="listitem">
Csárdi, G., Hester, J., Wickham, H., Chang, W., Morgan, M., &amp; Tenenbaum, D. (2024). <em>Remotes: R package installation from remote repositories, including ’GitHub’</em>. <a href="https://CRAN.R-project.org/package=remotes">https://CRAN.R-project.org/package=remotes</a>
</div>
<div id="ref-dahan2001" class="csl-entry" role="listitem">
Dahan, D., Magnuson, J. S., &amp; Tanenhaus, M. K. (2001). Time course of frequency effects in spoken-word recognition: Evidence from eye movements. <em>Cognitive Psychology</em>, <em>42</em>(4), 317–367. <a href="https://doi.org/10.1006/cogp.2001.0750">https://doi.org/10.1006/cogp.2001.0750</a>
</div>
<div id="ref-degen2021" class="csl-entry" role="listitem">
Degen, J., Kursat, L., &amp; Leigh, D. D. (2021). Seeing is believing: Testing an explicit linking assumption for visual world eye-tracking in psycholinguistics. <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em>, <em>43</em>.
</div>
<div id="ref-nix" class="csl-entry" role="listitem">
Dolstra, E., &amp; contributors, T. N. (2023). <em>Nix</em> (Version 2.15.3) [Computer software]. <a href="https://nixos.org/">https://nixos.org/</a>
</div>
<div id="ref-eberhard1995" class="csl-entry" role="listitem">
Eberhard, K. M., Spivey-Knowlton, M. J., Sedivy, J. C., &amp; Tanenhaus, M. K. (1995). Eye movements as a window into real-time spoken language comprehension in natural contexts. <em>Journal of Psycholinguistic Research</em>, <em>24</em>(6), 409–436. <a href="https://doi.org/10.1007/BF02143160">https://doi.org/10.1007/BF02143160</a>
</div>
<div id="ref-faul2007" class="csl-entry" role="listitem">
Faul, F., Erdfelder, E., Lang, A.-G., &amp; Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. <em>Behavior Research Methods</em>, <em>39</em>(2), 175–191. <a href="https://doi.org/10.3758/BF03193146">https://doi.org/10.3758/BF03193146</a>
</div>
<div id="ref-fields2019" class="csl-entry" role="listitem">
Fields, E. C., &amp; Kuperberg, G. R. (2019). Having your cake and eating it too: Flexibility and power with mass univariate statistics for ERP data. <em>Psychophysiology</em>. <a href="https://doi.org/10.1111/psyp.13468">https://doi.org/10.1111/psyp.13468</a>
</div>
<div id="ref-janitor" class="csl-entry" role="listitem">
Firke, S. (2023). <em>Janitor: Simple tools for examining and cleaning dirty data</em>. <a href="https://CRAN.R-project.org/package=janitor">https://CRAN.R-project.org/package=janitor</a>
</div>
<div id="ref-permuco" class="csl-entry" role="listitem">
Frossard, J., &amp; Renaud, O. (2021). <em>Permutation tests for regression, <span></span>ANOVA<span></span>, and comparison of signals: The <span></span>permuco<span></span> package</em>. <em>99</em>. <a href="https://doi.org/10.18637/jss.v099.i15">https://doi.org/10.18637/jss.v099.i15</a>
</div>
<div id="ref-webgazeR" class="csl-entry" role="listitem">
Geller, J. (2025). <em>webgazeR: Tools for processing webcam eye tracking data</em>. <a href="https://github.com/jgeller112/webgazeR">https://github.com/jgeller112/webgazeR</a>
</div>
<div id="ref-godfroid" class="csl-entry" role="listitem">
Godfroid, A., Finch, B., &amp; Koh, J. (2024). Reporting Eye-Tracking Research in Second Language Acquisition and Bilingualism: A Synthesis and Field-Specific Guidelines. <em>Language Learning</em>, <em>n/a</em>(n/a). <a href="https://doi.org/10.1111/lang.12664">https://doi.org/10.1111/lang.12664</a>
</div>
<div id="ref-gosling2010" class="csl-entry" role="listitem">
Gosling, S. D., Sandy, C. J., John, O. P., &amp; Potter, J. (2010). Wired but not WEIRD: The promise of the Internet in reaching more diverse samples. <em>Behavioral and Brain Sciences</em>, <em>33</em>(2-3), 94–95. <a href="https://doi.org/10.1017/S0140525X10000300">https://doi.org/10.1017/S0140525X10000300</a>
</div>
<div id="ref-green2016" class="csl-entry" role="listitem">
Green, P., &amp; MacLeod, C. J. (2016). SIMR: an R package for power analysis of generalized linear mixed models by simulation. <em>Methods in Ecology and Evolution</em>, <em>7</em>(4), 493–498. <a href="https://doi.org/10.1111/2041-210X.12504">https://doi.org/10.1111/2041-210X.12504</a>
</div>
<div id="ref-henrich2010" class="csl-entry" role="listitem">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). Most people are not WEIRD. <em>Nature</em>, <em>466</em>(7302), 29. <a href="https://doi.org/10.1038/466029a">https://doi.org/10.1038/466029a</a>
</div>
<div id="ref-hooge2024" class="csl-entry" role="listitem">
Hooge, I. T. C., Hessels, R. S., Niehorster, D. C., Andersson, R., Skrok, M. K., Konklewski, R., Stremplewski, P., Nowakowski, M., Tamborski, S., Szkulmowska, A., Szkulmowski, M., &amp; Nyström, M. (2024). Eye tracker calibration: How well can humans refixate a target? <em>Behavior Research Methods</em>, <em>57</em>(1), 23. <a href="https://doi.org/10.3758/s13428-024-02564-4">https://doi.org/10.3758/s13428-024-02564-4</a>
</div>
<div id="ref-hopp2013" class="csl-entry" role="listitem">
Hopp, H. (2013). Grammatical gender in adult L2 acquisition: Relations between lexical and syntactic variability. <em>Second Language Research</em>, <em>29</em>(1), 33–56. <a href="https://doi.org/10.1177/0267658312461803">https://doi.org/10.1177/0267658312461803</a>
</div>
<div id="ref-huang2020" class="csl-entry" role="listitem">
Huang, Y., &amp; Snedeker, J. (2020). Evidence from the visual world paradigm raises questions about unaccusativity and growth curve analyses. <em>Cognition</em>, <em>200</em>, 104251. <a href="https://doi.org/10.1016/j.cognition.2020.104251">https://doi.org/10.1016/j.cognition.2020.104251</a>
</div>
<div id="ref-huettig2007" class="csl-entry" role="listitem">
Huettig, F., &amp; McQueen, J. M. (2007). The tug of war between phonological, semantic and shape information in language-mediated visual search. <em>Journal of Memory and Language</em>, <em>57</em>(4), 460–482. <a href="https://doi.org/10.1016/j.jml.2007.02.001">https://doi.org/10.1016/j.jml.2007.02.001</a>
</div>
<div id="ref-huettig2011" class="csl-entry" role="listitem">
Huettig, F., Rommers, J., &amp; Meyer, A. S. (2011). Using the visual world paradigm to study language processing: a review and critical evaluation. <em>Acta Psychologica</em>, <em>137</em>(2), 151–171. <a href="https://doi.org/10.1016/j.actpsy.2010.11.003">https://doi.org/10.1016/j.actpsy.2010.11.003</a>
</div>
<div id="ref-ito2023" class="csl-entry" role="listitem">
Ito, A., &amp; Knoeferle, P. (2023). Analysing data from the psycholinguistic visual-world paradigm: Comparison of different analysis methods. <em>Behavior Research Methods</em>, <em>55</em>(7), 3461–3493. <a href="https://doi.org/10.3758/s13428-022-01969-3">https://doi.org/10.3758/s13428-022-01969-3</a>
</div>
<div id="ref-ito2018" class="csl-entry" role="listitem">
Ito, A., Pickering, M. J., &amp; Corley, M. (2018). Investigating the time-course of phonological prediction in native and non-native speakers of english: A visual world eye-tracking study. <em>Journal of Memory and Language</em>, <em>98</em>, 1–11. <a href="https://doi.org/10.1016/j.jml.2017.09.002">https://doi.org/10.1016/j.jml.2017.09.002</a>
</div>
<div id="ref-izura2014" class="csl-entry" role="listitem">
Izura, C., Cuetos, F., &amp; Brysbaert, M. (2014). Lextale-Esp: a test to rapidly and efficiently assess the Spanish vocabulary size. <em>PSICOLOGICA</em>, <em>35</em>(1), 49–66. <a href="http://hdl.handle.net/1854/LU-5774107">http://hdl.handle.net/1854/LU-5774107</a>
</div>
<div id="ref-ju2004" class="csl-entry" role="listitem">
Ju, M., &amp; Luce, P. A. (2004). Falling on sensitive ears: Constraints on bilingual lexical activation. <em>Psychological Science</em>, <em>15</em>(5), 314–318. <a href="https://doi.org/10.1111/j.0956-7976.2004.00675.x">https://doi.org/10.1111/j.0956-7976.2004.00675.x</a>
</div>
<div id="ref-kaduk2024" class="csl-entry" role="listitem">
Kaduk, T., Goeke, C., Finger, H., &amp; König, P. (2024). Webcam eye tracking close to laboratory standards: Comparing a new webcam-based system and the EyeLink 1000. <em>Behavior Research Methods</em>, <em>56</em>(5), 5002–5022. <a href="https://doi.org/10.3758/s13428-023-02237-8">https://doi.org/10.3758/s13428-023-02237-8</a>
</div>
<div id="ref-kamide2003" class="csl-entry" role="listitem">
Kamide, Y., Altmann, G. T. M., &amp; Haywood, S. L. (2003). The time-course of prediction in incremental sentence processing: Evidence from anticipatory eye movements. <em>Journal of Memory and Language</em>, <em>49</em>(1), 133–156. <a href="https://doi.org/10.1016/S0749-596X(03)00023-8">https://doi.org/10.1016/S0749-596X(03)00023-8</a>
</div>
<div id="ref-kret2018" class="csl-entry" role="listitem">
Kret, M. E., &amp; Sjak-Shie, E. E. (2018). Preprocessing pupil size data: Guidelines and code. <em>Behavior Research Methods</em>, 1–7. <a href="https://doi.org/10.3758/s13428-018-1075-y">https://doi.org/10.3758/s13428-018-1075-y</a>
</div>
<div id="ref-kumle2021" class="csl-entry" role="listitem">
Kumle, L., Võ, M. L.-H., &amp; Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: An open introduction and tutorial in R. <em>Behavior Research Methods</em>, <em>53</em>(6), 2528–2543. <a href="https://doi.org/10.3758/s13428-021-01546-0">https://doi.org/10.3758/s13428-021-01546-0</a>
</div>
<div id="ref-lakens2022" class="csl-entry" role="listitem">
Lakens, D. (2022). Sample size justification. <em>Collabra: Psychology</em>, <em>8</em>(1). <a href="https://doi.org/10.1525/collabra.33267">https://doi.org/10.1525/collabra.33267</a>
</div>
<div id="ref-deleeuw2015" class="csl-entry" role="listitem">
Leeuw, J. R. de. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. <em>Behavior Research Methods</em>, <em>47</em>(1), 1–12. <a href="https://doi.org/10.3758/s13428-014-0458-y">https://doi.org/10.3758/s13428-014-0458-y</a>
</div>
<div id="ref-madsen2021" class="csl-entry" role="listitem">
Madsen, J., Júlio, S. U., Gucik, P. J., Steinberg, R., &amp; Parra, L. C. (2021). Synchronized eye movements predict test scores in online video education. <em>Proceedings of the National Academy of Sciences</em>, <em>118</em>(5), e2016980118. <a href="https://doi.org/10.1073/pnas.2016980118">https://doi.org/10.1073/pnas.2016980118</a>
</div>
<div id="ref-magnuson2007" class="csl-entry" role="listitem">
Magnuson, J. S., Dixon, J. A., Tanenhaus, M. K., &amp; Aslin, R. N. (2007). The Dynamics of Lexical Competition During Spoken Word Recognition. <em>Cognitive Science</em>, <em>31</em>(1), 133–156. <a href="https://doi.org/10.1080/03640210709336987">https://doi.org/10.1080/03640210709336987</a>
</div>
<div id="ref-maris2007" class="csl-entry" role="listitem">
Maris, E., &amp; Oostenveld, R. (2007). Nonparametric statistical testing of EEG- and MEG-data. <em>Journal of Neuroscience Methods</em>, <em>164</em>(1), 177–190. <a href="https://doi.org/10.1016/j.jneumeth.2007.03.024">https://doi.org/10.1016/j.jneumeth.2007.03.024</a>
</div>
<div id="ref-mcmurray2017" class="csl-entry" role="listitem">
McMurray, B., Farris-Trimble, A., &amp; Rigler, H. (2017). Waiting for lexical access: Cochlear implants or severely degraded input lead listeners to process speech less incrementally. <em>Cognition</em>, <em>169</em>, 147–164. <a href="https://doi.org/10.1016/j.cognition.2017.08.013">https://doi.org/10.1016/j.cognition.2017.08.013</a>
</div>
<div id="ref-mcmurray2010" class="csl-entry" role="listitem">
McMurray, B., Samelson, V. M., Lee, S. H., &amp; Tomblin, J. B. (2010). Individual differences in online spoken word recognition: Implications for SLI. <em>Cognitive Psychology</em>, <em>60</em>(1), 1–39. <a href="https://doi.org/10.1016/j.cogpsych.2009.06.003">https://doi.org/10.1016/j.cogpsych.2009.06.003</a>
</div>
<div id="ref-mcmurray2002" class="csl-entry" role="listitem">
McMurray, B., Tanenhaus, M. K., &amp; Aslin, R. N. (2002). Gradient effects of within-category phonetic variation on lexical access. <em>Cognition</em>, <em>86</em>(2), B33–B42. <a href="https://doi.org/10.1016/S0010-0277(02)00157-9">https://doi.org/10.1016/S0010-0277(02)00157-9</a>
</div>
<div id="ref-meyer2021" class="csl-entry" role="listitem">
Meyer, M., Lamers, D., Kayhan, E., Hunnius, S., &amp; Oostenveld, R. (2021). Enhancing reproducibility in developmental EEG research: BIDS, cluster-based permutation tests, and effect sizes. <em>Developmental Cognitive Neuroscience</em>, <em>52</em>, 101036. <a href="https://doi.org/10.1016/j.dcn.2021.101036">https://doi.org/10.1016/j.dcn.2021.101036</a>
</div>
<div id="ref-foreach" class="csl-entry" role="listitem">
Microsoft, &amp; Weston, S. (2022). <em>Foreach: Provides foreach looping construct</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>
</div>
<div id="ref-miller2023" class="csl-entry" role="listitem">
Miller, J. (2023). Outlier exclusion procedures for reaction time analysis: The cures are generally worse than the disease. <em>Journal of Experimental Psychology: General</em>, <em>152</em>(11), 3189–3217. <a href="https://doi.org/10.1037/xge0001450">https://doi.org/10.1037/xge0001450</a>
</div>
<div id="ref-milne2021" class="csl-entry" role="listitem">
Milne, A. E., Zhao, S., Tampakaki, C., Bury, G., &amp; Chait, M. (2021). Sustained pupil responses are modulated by predictability of auditory sequences. <em>The Journal of Neuroscience</em>, <em>41</em>(28), 6116–6127. <a href="https://doi.org/10.1523/JNEUROSCI.2879-20.2021">https://doi.org/10.1523/JNEUROSCI.2879-20.2021</a>
</div>
<div id="ref-mirmanGrowthCurveAnalysis" class="csl-entry" role="listitem">
Mirman, D., &amp; CRC Press. (n.d.). <em>Growth curve analysis and visualization using r</em>.
</div>
<div id="ref-mirman2012" class="csl-entry" role="listitem">
Mirman, D., &amp; Graziano, K. M. (2012). Individual differences in the strength of taxonomic versus thematic relations. <em>Journal of Experimental Psychology: General</em>, <em>141</em>(4), 601–609. <a href="https://doi.org/10.1037/a0026451">https://doi.org/10.1037/a0026451</a>
</div>
<div id="ref-here" class="csl-entry" role="listitem">
Müller, K. (2020). <em>Here: A simpler way to find your files</em>. <a href="https://CRAN.R-project.org/package=here">https://CRAN.R-project.org/package=here</a>
</div>
<div id="ref-niedermann2024" class="csl-entry" role="listitem">
Niedermann, J. P., Sucholutsky, I., Marjieh, R., Çelen, E., Griffiths, T., Jacoby, N., &amp; Rijn, P. van. (2024). Studying the Effect of Globalization on Color Perception using Multilingual Online Recruitment and Large Language Models. <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em>, <em>46</em>(0). <a href="https://escholarship.org/uc/item/4hs755zz">https://escholarship.org/uc/item/4hs755zz</a>
</div>
<div id="ref-özsoy2023" class="csl-entry" role="listitem">
Özsoy, O., Çiçek, B., Özal, Z., Gagarina, N., &amp; Sekerina, I. A. (2023). Turkish-german heritage speakers’ predictive use of case: Webcam-based vs. In-lab eye-tracking. <em>Frontiers in Psychology</em>, <em>14</em>, 1155585. <a href="https://doi.org/10.3389/fpsyg.2023.1155585">https://doi.org/10.3389/fpsyg.2023.1155585</a>
</div>
<div id="ref-papoutsaki2016" class="csl-entry" role="listitem">
Papoutsaki, A., Sangkloy, P., Laskey, J., Daskalova, N., Huang, J., &amp; Hays, J. (2016). <em>Webgazer: Scalable webcam eye tracking using user interactions</em>. 38393845.
</div>
<div id="ref-patterson2023" class="csl-entry" role="listitem">
Patterson, A. S., &amp; Nicklin, C. (2023). L2 self-paced reading data collection across three contexts: In-person, online, and crowdsourcing. <em>Research Methods in Applied Linguistics</em>, <em>2</em>(1), 100045. <a href="https://doi.org/10.1016/j.rmal.2023.100045">https://doi.org/10.1016/j.rmal.2023.100045</a>
</div>
<div id="ref-peelle2021" class="csl-entry" role="listitem">
Peelle, J. E., &amp; Van Engen, K. J. (2021). Time stand still: Effects of temporal window selection on eye tracking analysis. <em>Collabra: Psychology</em>, <em>7</em>(1), 25961. <a href="https://doi.org/10.1525/collabra.25961">https://doi.org/10.1525/collabra.25961</a>
</div>
<div id="ref-peirce2019" class="csl-entry" role="listitem">
Peirce, J., Gray, J. R., Simpson, S., MacAskill, M., Höchenberger, R., Sogo, H., Kastman, E., &amp; Lindeløv, J. K. (2019). PsychoPy2: Experiments in behavior made easy. <em>Behavior Research Methods</em>, <em>51</em>(1), 195–203. <a href="https://doi.org/10.3758/s13428-018-01193-y">https://doi.org/10.3758/s13428-018-01193-y</a>
</div>
<div id="ref-peterson2021" class="csl-entry" role="listitem">
Peterson, R. J. (2021). We need to address ableism in science. <em>Molecular Biology of the Cell</em>, <em>32</em>(7), 507–510. <a href="https://doi.org/10.1091/mbc.E20-09-0616">https://doi.org/10.1091/mbc.E20-09-0616</a>
</div>
<div id="ref-pluzyczka2018" class="csl-entry" role="listitem">
Płużyczka, M. (2018). The First Hundred Years: a History of Eye Tracking as a Research Method. <em>Applied Linguistics Papers</em>, <em>25/4</em>, 101–116. <a href="http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-98576d43-39e3-4981-8c1c-717962cf29da">http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-98576d43-39e3-4981-8c1c-717962cf29da</a>
</div>
<div id="ref-prystauka2024" class="csl-entry" role="listitem">
Prystauka, Y., Altmann, G. T. M., &amp; Rothman, J. (2024). Online eye tracking and real-time sentence processing: On opportunities and efficacy for capturing psycholinguistic effects of different magnitudes and diversity. <em>Behavior Research Methods</em>, <em>56</em>(4), 3504–3522. <a href="https://doi.org/10.3758/s13428-023-02176-4">https://doi.org/10.3758/s13428-023-02176-4</a>
</div>
<div id="ref-R" class="csl-entry" role="listitem">
R Core Team. (2024). <em>R: A language and environment for statistical computing</em> (Version 4.4.2). R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>
</div>
<div id="ref-rodd2024" class="csl-entry" role="listitem">
Rodd, J. M. (2024). Moving experimental psychology online: How to obtain high quality data when we can<span>’</span>t see our participants. <em>Journal of Memory and Language</em>, <em>134</em>, 104472. <a href="https://doi.org/10.1016/j.jml.2023.104472">https://doi.org/10.1016/j.jml.2023.104472</a>
</div>
<div id="ref-rix" class="csl-entry" role="listitem">
Rodrigues, B., &amp; Baumann, P. (2025). <em>Rix: Reproducible data science environments with ’nix’</em>. <a href="https://docs.ropensci.org/rix/">https://docs.ropensci.org/rix/</a>
</div>
<div id="ref-rossi2019" class="csl-entry" role="listitem">
Rossi, E., Krass, K., &amp; Kootstra, G. J. (2019). <em>Psycholinguistic Methods in Multilingual Research</em> (pp. 75–99). John Wiley &amp; Sons, Ltd. <a href="https://doi.org/10.1002/9781119387725.ch4">https://doi.org/10.1002/9781119387725.ch4</a>
</div>
<div id="ref-sarrett2022" class="csl-entry" role="listitem">
Sarrett, M. E., Shea, C., &amp; McMurray, B. (2022). Within- and between-language competition in adult second language learners: Implications for language proficiency. <em>Language, Cognition and Neuroscience</em>, <em>37</em>(2), 165–181. <a href="https://doi.org/10.1080/23273798.2021.1952283">https://doi.org/10.1080/23273798.2021.1952283</a>
</div>
<div id="ref-seedorff2018" class="csl-entry" role="listitem">
Seedorff, M., Oleson, J., &amp; McMurray, B. (2018). Detecting when timeseries differ: Using the bootstrapped differences of timeseries (BDOTS) to analyze visual world paradigm data (and more). <em>Journal of Memory and Language</em>, <em>102</em>, 55–67. <a href="https://doi.org/10.1016/J.JML.2018.05.004">https://doi.org/10.1016/J.JML.2018.05.004</a>
</div>
<div id="ref-semmelmann2018" class="csl-entry" role="listitem">
Semmelmann, K., &amp; Weigelt, S. (2018). Online webcam-based eye tracking in cognitive science: A first look. <em>Behavior Research Methods</em>, <em>50</em>(2), 451–465. <a href="https://doi.org/10.3758/s13428-017-0913-7">https://doi.org/10.3758/s13428-017-0913-7</a>
</div>
<div id="ref-slim2023" class="csl-entry" role="listitem">
Slim, M. S., &amp; Hartsuiker, R. J. (2023). Moving visual world experiments online? A web-based replication of Dijkgraaf, Hartsuiker, and Duyck (2017) using PCIbex and WebGazer.js. <em>Behavior Research Methods</em>, <em>55</em>(7), 3786–3804. <a href="https://doi.org/10.3758/s13428-022-01989-z">https://doi.org/10.3758/s13428-022-01989-z</a>
</div>
<div id="ref-slim2024" class="csl-entry" role="listitem">
Slim, M. S., Kandel, M., Yacovone, A., &amp; Snedeker, J. (2024). Webcams as windows to the mind? A direct comparison between in-lab and web-based eye-tracking methods. <em>Open Mind</em>, <em>8</em>, 1369–1424. <a href="https://doi.org/10.1162/opmi_a_00171">https://doi.org/10.1162/opmi_a_00171</a>
</div>
<div id="ref-spivey1999" class="csl-entry" role="listitem">
Spivey, M. J., &amp; Marian, V. (1999). Cross talk between native and second languages: Partial activation of an irrelevant lexicon. <em>Psychological Science</em>, <em>10</em>(3), 281–284. <a href="https://doi.org/10.1111/1467-9280.00151">https://doi.org/10.1111/1467-9280.00151</a>
</div>
<div id="ref-stone2021" class="csl-entry" role="listitem">
Stone, K., Lago, S., &amp; Schad, D. J. (2021). Divergence point analyses of visual world data: applications to bilingual research. <em>Bilingualism: Language and Cognition</em>, <em>24</em>(5), 833–841. <a href="https://doi.org/10.1017/S1366728920000607">https://doi.org/10.1017/S1366728920000607</a>
</div>
<div id="ref-tanenhaus1995" class="csl-entry" role="listitem">
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. <em>Science (New York, N.Y.)</em>, <em>268</em>(5217), 1632–1634. <a href="http://www.ncbi.nlm.nih.gov/pubmed/7777863">http://www.ncbi.nlm.nih.gov/pubmed/7777863</a>
</div>
<div id="ref-trueswell2008" class="csl-entry" role="listitem">
Trueswell, J. C. (2008). <em>Using eye movements as a developmental measure within psycholinguistics</em> (I. A. Sekerina, E. M. Fernández, &amp; H. Clahsen, Eds.; pp. 73–96). John Benjamins Publishing Company. <a href="https://doi.org/10.1075/lald.44.05tru">https://doi.org/10.1075/lald.44.05tru</a>
</div>
<div id="ref-vandercruyssen2024" class="csl-entry" role="listitem">
Van der Cruyssen, I., Ben-Shakhar, G., Pertzov, Y., Guy, N., Cabooter, Q., Gunschera, L. J., &amp; Verschuere, B. (2024). The validation of online webcam-based eye-tracking: The replication of the cascade effect, the novelty preference, and the visual world paradigm. <em>Behavior Research Methods</em>, <em>56</em>(5), 4836–4849. <a href="https://doi.org/10.3758/s13428-023-02221-2">https://doi.org/10.3758/s13428-023-02221-2</a>
</div>
<div id="ref-viviani1990" class="csl-entry" role="listitem">
Viviani, P. (1990). Eye movements in visual search: cognitive, perceptual and motor control aspects. <em>Reviews of Oculomotor Research</em>, <em>4</em>, 353–393.
</div>
<div id="ref-permutes" class="csl-entry" role="listitem">
Voeten, C. C. (2023). <em>Permutes: Permutation tests for time series data</em>. <a href="https://CRAN.R-project.org/package=permutes">https://CRAN.R-project.org/package=permutes</a>
</div>
<div id="ref-vos2022" class="csl-entry" role="listitem">
Vos, M., Minor, S., &amp; Ramchand, G. C. (2022). Comparing infrared and webcam eye tracking in the Visual World Paradigm. <em>Glossa Psycholinguistics</em>, <em>1</em>(1). <a href="https://doi.org/10.5070/G6011131">https://doi.org/10.5070/G6011131</a>
</div>
<div id="ref-wickham2017" class="csl-entry" role="listitem">
Wickham, H. (2017). <em>Tidyverse: Easily install and load the ’tidyverse’</em>. <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>
</div>
<div id="ref-yee2008" class="csl-entry" role="listitem">
Yee, E., Blumstein, S., &amp; Sedivy, J. C. (2008). Lexical-semantic activation in broca<span>’</span>s and wernicke<span>’</span>s aphasia: Evidence from eye movements. <em>Journal of Cognitive Neuroscience</em>, <em>20</em>(4), 592–612. <a href="https://doi.org/10.1162/jocn.2008.20056">https://doi.org/10.1162/jocn.2008.20056</a>
</div>
<div id="ref-zehr2018penncontroller" class="csl-entry" role="listitem">
Zehr, J., &amp; Schwarz, F. (2018). <em>PennController for internet based experiments (IBEX)</em>. <a href="https://doi.org/10.17605/OSF.IO/MD832">https://doi.org/10.17605/OSF.IO/MD832</a>
</div>
</div>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><strong>It is important to note that WebGazer.js is not the only method available. Other methods have been implemented by companies like Tobii (www.tobii.com) and Labvanced <span class="citation" data-cites="kaduk2024">(Kaduk et al., 2024)</span> . However, because these methods are proprietary, they are less accessible and difficult to reproduce.</strong><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The curve-fitting approach used by <span class="citation" data-cites="sarrett2022">Sarrett et al. (2022)</span> may have required a larger number of trials to obtain reliable fits. Their study included over 400 trials, while our design was more constrained.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>