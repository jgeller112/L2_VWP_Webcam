<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jason Geller">
<meta name="author" content="Yanina Prystauka">
<meta name="author" content="Sarah Colby">
<meta name="author" content="Julia Droulin">
<meta name="keywords" content="VWP, Tutorial, Webcam eye-tracking, R, Gorilla">
<meta name="description" content="VWP WEBCAM TUTORIAL">

<title>Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="L2_VWP_webcam_ET_files/libs/clipboard/clipboard.min.js"></script>
<script src="L2_VWP_webcam_ET_files/libs/quarto-html/quarto.js"></script>
<script src="L2_VWP_webcam_ET_files/libs/quarto-html/popper.min.js"></script>
<script src="L2_VWP_webcam_ET_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="L2_VWP_webcam_ET_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L2_VWP_webcam_ET_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L2_VWP_webcam_ET_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L2_VWP_webcam_ET_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L2_VWP_webcam_ET_files/libs/bootstrap/bootstrap-03e6528f281d20b9a3cfa169127af882.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="L2_VWP_webcam_ET_files/libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="L2_VWP_webcam_ET_files/libs/tabwid-1.1.3/tabwid.js"></script>


<link rel="stylesheet" href="_extensions/wjschne/apaquarto/apa.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#l2-vwp-webcam-eye-tracking" id="toc-l2-vwp-webcam-eye-tracking" class="nav-link active" data-scroll-target="#l2-vwp-webcam-eye-tracking">L2 VWP Webcam Eye-tracking</a>
  <ul class="collapse">
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a>
  <ul class="collapse">
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants">Participants</a></li>
  <li><a href="#materials" id="toc-materials" class="nav-link" data-scroll-target="#materials">Materials</a></li>
  <li><a href="#procedure" id="toc-procedure" class="nav-link" data-scroll-target="#procedure">Procedure</a></li>
  </ul></li>
  <li><a href="#preprocessing-data" id="toc-preprocessing-data" class="nav-link" data-scroll-target="#preprocessing-data">Preprocessing data</a>
  <ul class="collapse">
  <li><a href="#load-packages" id="toc-load-packages" class="nav-link" data-scroll-target="#load-packages">Load packages</a></li>
  <li><a href="#reading-in-data" id="toc-reading-in-data" class="nav-link" data-scroll-target="#reading-in-data">Reading in data</a></li>
  <li><a href="#subject-and-trial-level-data-removal" id="toc-subject-and-trial-level-data-removal" class="nav-link" data-scroll-target="#subject-and-trial-level-data-removal">Subject and trial level data removal</a></li>
  <li><a href="#eye-tracking-data-1" id="toc-eye-tracking-data-1" class="nav-link" data-scroll-target="#eye-tracking-data-1">Eye-tracking data</a></li>
  </ul></li>
  <li><a href="#areas-of-interest" id="toc-areas-of-interest" class="nav-link" data-scroll-target="#areas-of-interest">Areas of Interest</a>
  <ul class="collapse">
  <li><a href="#zone-coordinates" id="toc-zone-coordinates" class="nav-link" data-scroll-target="#zone-coordinates">Zone coordinates</a></li>
  </ul></li>
  <li><a href="#samples-to-bins" id="toc-samples-to-bins" class="nav-link" data-scroll-target="#samples-to-bins">Samples to bins</a>
  <ul class="collapse">
  <li><a href="#downsampling" id="toc-downsampling" class="nav-link" data-scroll-target="#downsampling">Downsampling</a></li>
  </ul></li>
  <li><a href="#visualizing-time-course-data" id="toc-visualizing-time-course-data" class="nav-link" data-scroll-target="#visualizing-time-course-data">Visualizing time course data</a></li>
  <li><a href="#gorilla-provided-coordinates" id="toc-gorilla-provided-coordinates" class="nav-link" data-scroll-target="#gorilla-provided-coordinates">Gorilla provided coordinates</a></li>
  <li><a href="#visualizing-time-course-data-with-gorilla-coordinates" id="toc-visualizing-time-course-data-with-gorilla-coordinates" class="nav-link" data-scroll-target="#visualizing-time-course-data-with-gorilla-coordinates">Visualizing time course data with Gorilla coordinates</a></li>
  <li><a href="#modeling-data" id="toc-modeling-data" class="nav-link" data-scroll-target="#modeling-data">Modeling data</a>
  <ul class="collapse">
  <li><a href="#cpt" id="toc-cpt" class="nav-link" data-scroll-target="#cpt">CPT</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a>
  <ul class="collapse">
  <li><a href="#poor-vs.-good-calibrators" id="toc-poor-vs.-good-calibrators" class="nav-link" data-scroll-target="#poor-vs.-good-calibrators">Poor vs.&nbsp;good calibrators</a></li>
  <li><a href="#generalizability-to-other-platforms" id="toc-generalizability-to-other-platforms" class="nav-link" data-scroll-target="#generalizability-to-other-platforms"><strong>Generalizability to other platforms</strong></a></li>
  <li><a href="#power" id="toc-power" class="nav-link" data-scroll-target="#power"><strong>Power</strong></a></li>
  </ul></li>
  <li><a href="#recommendations" id="toc-recommendations" class="nav-link" data-scroll-target="#recommendations">Recommendations</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<br>

<br>

<section id="title" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research</h1>
<div class="Author">
<br>

<p>Jason Geller<sup>1</sup>, Yanina Prystauka<sup>2</sup>, Sarah Colby<sup>3</sup>, and Julia Droulin<sup>4</sup></p>
<p><sup>1</sup>Department of Psychology and Neuroscience, Boston College</p>
<p><sup>2</sup>Department of Psychology and Neuroscience, University of Bergen</p>
<p><sup>3</sup>Department of Psychology and Neuroscience, University of Ottowa</p>
<p><sup>4</sup>Department of Psychology and Neuroscience, University of North Carolina at Chapel Hill</p>
</div>
</section>
<section id="author-note" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Author Note</h1>
<p>Jason Geller <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0002-7459-4505</p>
<p>Yanina Prystauka <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0000-0000-0002</p>
<p>Sarah Colby <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0000-0000-0003</p>
<p>Julia Droulin <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0000-0000-0003</p>
<p>This study was not preregistered. The data and code can be found at https://github.com/jgeller112/L2_VWP_Webcam. The authors have no conflicts of interest to disclose.</p>
<p>Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows: <em>Jason Geller</em><strong>: </strong>conceptualization, writing – original draft, data curation, writing – review &amp; editing, software, and formal analysis. <em>Yanina Prystauka</em><strong>: </strong>writing – original draft, writing – review &amp; editing, and formal analysis. <em>Sarah Colby</em><strong>: </strong>writing – original draft and writing – review &amp; editing. <em>Julia Droulin</em><strong>: </strong>conceptualziation, writing – original draft, writing – review &amp; editing, and funding acquisition</p>
<p>Correspondence concerning this article should be addressed to Jason Geller, Department of Psychology and Neuroscience, Boston College, Mcguinn Hall 405, Chestnut Hill, MA 02467-9991, USA, drjasongeller@gmail.com: jason.geller@bc.edu</p>
</section>
<section id="abstract" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Abstract</h1>
<div class="AbstractFirstParagraph">
<p>Eye-tracking has become a valuable tool for studying cognitive processes in second language (L2) acquisition and bilingualism (Godfroid et al., 2024). While research-grade infrared eye-trackers are commonly used, there are a number of issues that limit its wide-spread adoption. Recently, consumer-based webcam eye-tracking has emerged as an attractive alternative, requiring only internet access and a personal webcam. However, webcam eye-tracking presents unique design and preprocessing challenges that must be addressed for valid results. To help researchers overcome these challenges,we developed a comprehensive tutorial focused on visual world webcam eye-tracking for L2 langauge research. Our guide will cover all key steps, from experiment design to data preprocessing and analysis, where we highlight the R package <code>webgazeR</code>, which is open source and freely available for download and installation: https://github.com/jgeller112/webgazeR. We offer best practices for environmental conditions, participant instructions,and tips for designing visual world experiments with webcam eye-tracking. To demonstrate these steps, we analyze data collected through the Gorilla platform (Anwyl-Irvine et al., 2020) using a single word Spanish visual world paradigm (VWP) and show competeiton within and between L2/L1. This tutorial aims to empower researchers by providing a step-by-step guide to successfully conduct visual world webcam-based eye-tracking studies. To follow along with this tutorial, please download the entire manuscript and its accompaying code with data from here: https://github.com/jgeller112/L2_VWP_Webcam.</p>
</div>
<p><em>Keywords</em>: VWP, Tutorial, Webcam eye-tracking, R, Gorilla</p>
</section>
<section id="firstheader" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research</h1>
<p>Eye-tracking technology, which has a history spanning over a century, has seen remarkable advancements. In the early days, eye-tracking sometimes required the use of contact lenses fitted with search coils, often requiring anesthesia, or the attachment of suction cups to the sclera of the eyes <span class="citation" data-cites="pluzyczka2018">(<a href="#ref-pluzyczka2018" role="doc-biblioref">Płużyczka, 2018</a>)</span>. These methods were not only cumbersome for the researcher, but also uncomfortable and invasive for participants. Over time, such approaches have been replaced by non-invasive, lightweight, and user-friendly systems. Today, modern eye-tracking technology is widely accessible in laboratories worldwide, enabling researchers to tackle critical questions about cognitive processes . This evolution has had a profound impact on fields such as psycholinguistics and bilingualism opening up new possibilities for understanding how language is processed in real time <span class="citation" data-cites="godfroid">(<a href="#ref-godfroid" role="doc-biblioref">Godfroid et al., 2024</a>)</span>.</p>
<p>Despite its widespread usage, eye-tracking technology faces several obstacles that can limit its accessibility. One significant challenge is the specialized expertise required to operate research-grade eye-trackers. Proper usage often demands many hours of training, meaning most research must be conducted in a lab by a trained student or faculty member. Another major limitation is the cost. Eye-trackers can be prohibitively expensive, ranging from a few thousand dollars (e.g., Gazepoint; www.gazept.com) to tens of thousands of dollars (e.g., Tobii (www.tobii.com; SR Research (www.sr-research.com). As a result, not everyone possess the resources, or the time, to incorporate eye-tracking into their research program.</p>
<p>In addition, eye-tracking research often requires participants to visit a laboratory, which significantly limits the diversity of the sample or population researchers can recruit. Behavioral science research, in general, frequently suffers from a lack of diversity, relying heavily on participants who are predominantly Western, Educated, Industrialized, Rich, Democratic, and able-bodied (WEIRDA). This focus often excludes individuals from geographically dispersed areas, those from lower socioeconomic backgrounds, and people with disabilities who may face barriers to accessing research facilities. In language research, this issue is particularly evident, as it often prioritizes English-speaking, monolingual, populations <span class="citation" data-cites="blasi2022 bylund2024">(<a href="#ref-blasi2022" role="doc-biblioref">Blasi et al., 2022</a>; <a href="#ref-bylund2024" role="doc-biblioref">Bylund et al., 2024</a>)</span> and largely includes individuals with normal developing language abilities <span class="citation" data-cites="mcmurray2010">(<a href="#ref-mcmurray2010" role="doc-biblioref">McMurray et al., 2010</a>)</span>. These limitations not only narrow the populations available for study but also compromise the generalizability and applicability of research findings.</p>
<section id="eye-tracking-outside-the-lab" class="level2">
<h2 data-anchor-id="eye-tracking-outside-the-lab">Eye-tracking outside the lab</h2>
<p>Methods that allow participants to use their own equipment from anywhere in the world offer a potential solution to the issues outlined above, enabling researchers to recruit more diverse and disadvantaged samples and explore a broader range of questions <span class="citation" data-cites="gosling2010">(<a href="#ref-gosling2010" role="doc-biblioref">Gosling et al., 2010</a>)</span>. The shift toward online behavioral experiments has been gradually increasing in the behavioral sciences and has become every more important since the 2020 pandemic, which forced many of us to run studies online <span class="citation" data-cites="anderson2019 rodd2024">(<a href="#ref-anderson2019" role="doc-biblioref">Anderson et al., 2019</a>; <a href="#ref-rodd2024" role="doc-biblioref">Rodd, 2024</a>)</span>. The <em>onlineification</em> of behavioral research has prompted the development of eye-tracking methods that do not rely on traditional lab settings.</p>
<p>One method, manual eye-tracking <span class="citation" data-cites="trueswell2008">(<a href="#ref-trueswell2008" role="doc-biblioref">Trueswell, 2008</a>)</span>, involves using video recordings of participants, which can be collected through online teleconferencing platforms such as Zoom (www.zoom.com). Here eye gaze (direction) is manually analyzed post-hoc frame by frame from these recordings.</p>
<p>Another method, which is the focus of this tutorial, is automated eye-tracking or webcam eye-tracking. Webcam eye-tracking requires three things: 1. A personal computer. 2. An internet connection and 3. A purchased or pre-installed webcamera. Gaze information can be collected via a web browser. One common method to perform webcam eye-tracking is through an open source, free, and actively maintained JavaScript library plugin called WebGazer.js <span class="citation" data-cites="papoutsaki2016">(<a href="#ref-papoutsaki2016" role="doc-biblioref">Papoutsaki et al., 2016</a>)</span>. This plugin is already incorporated into several popular experimental platforms [e.g., <em>Gorilla</em>, <em>jsPsych</em>, <em>PsychoPy</em>, and <em>PCIbex</em>; <span class="citation" data-cites="anwyl-irvine2020">Anwyl-Irvine et al. (<a href="#ref-anwyl-irvine2020" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="peirce2019">Peirce et al. (<a href="#ref-peirce2019" role="doc-biblioref">2019</a>)</span>; <span class="citation" data-cites="deleeuw2015">Leeuw (<a href="#ref-deleeuw2015" role="doc-biblioref">2015</a>)</span>; <span class="citation" data-cites="zehr2018penncontroller">Zehr and Schwarz (<a href="#ref-zehr2018penncontroller" role="doc-biblioref">2018</a>)</span>]. A benefit of WebGazer.js is that it does not require users to download any software, and is fully integrated in the browser, making it extremely easy to start webcam eye-tracking.</p>
<p>WebGazer.js uses facial feature detection to dynamically estimate gaze positions in real time via webcam. For every time point (based on sampling rate), x and y coordinates are recorded. WebGazer.js leverages machine learning to analyze the relative movement of the eyes and infer gaze location on a screen. To imporve accuracy, a calibration process is used in which users interaction with visual stimuli, such as looking at and clicking random dots or tracking a moving dot. This mapping process enhances the precision of the gaze-to-screen-coordinate relationship</p>
<p>It is important to note that WebGazer.js is not the only method available. Other methods have been implemented by companies like Tobii (www.tobii.com) and Labvanced <span class="citation" data-cites="kaduk2024">(<a href="#ref-kaduk2024" role="doc-biblioref">Kaduk et al., 2024</a>)</span> . However, because these methods are proprietary, it is unclear what they are doing under the hood.</p>
<p>The algorithms underlying webcam-based eye tracking differ significantly from those used in research-grade eye trackers. Research-grade systems employ video-based recording and rely on the pupil-corneal reflection (P-CR) method to track gaze with high precision <span class="citation" data-cites="carter2020">(<a href="#ref-carter2020" role="doc-biblioref">Carter &amp; Luke, 2020</a>)</span>. This method utilizes infrared light to illuminate the eyes, capturing reflections (known as glints) from the cornea and pupil. High-speed cameras simultaneously capture images at rates of hundreds or thousands of frames per second to measure eye position. By combining data from the corneal reflections and pupil location, these systems calculate gaze direction and position. Proprietary algorithms then map this information to specific locations on the screen.</p>
<p>This leads to an important question: how does consumer-grade webcam eye tracking compare to research-grade systems? While validation studies are ongoing, webcam-based eye trackers generally exhibit reduced spatiotemporal accuracy. Studies have reported that these systems achieve spatial accuracy and precision exceeding 1° of visual angle, with latencies ranging from 200 ms to 1000 ms <span class="citation" data-cites="semmelmann2018 slim2023 slim2024 kaduk2024">(<a href="#ref-kaduk2024" role="doc-biblioref">Kaduk et al., 2024</a>; <a href="#ref-semmelmann2018" role="doc-biblioref">Semmelmann &amp; Weigelt, 2018</a>; <a href="#ref-slim2024" role="doc-biblioref">Slim et al., 2024</a>; <a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span>. Furthermore, the sampling rate of webcam-based systems is much lower, typically capped at 60 Hz, with most studies reporting average or median rates around 30 Hz <span class="citation" data-cites="prystauka2024 bramlett2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>; <a href="#ref-prystauka2024" role="doc-biblioref">Prystauka et al., 2024</a>)</span>. Unlike research-grade systems, webcam eye trackers do not use infrared light; instead, they rely on ambient light from the participant’s environment. This dependency introduces additional variability in tracking performance.</p>
<p>To compare, research-grade systems like the Tobii Pro Spectrum provides spatial precision of 0.03°–0.06° RMS, spatial accuracy of &lt;0.3°, and latency of less than 2.5 ms, with a sampling rate of up to 1200 Hz <span class="citation" data-cites="nyström2021 tobii2024">(<a href="#ref-tobii2024" role="doc-biblioref">AB, 2024</a>; <a href="#ref-nyström2021" role="doc-biblioref">Nyström et al., 2021</a>)</span>. These advanced metrics make research-grade systems ideal for studies requiring high temporal and spatial resolution.</p>
</section>
<section id="bringing-the-visual-world-paradigm-online" class="level2">
<h2 data-anchor-id="bringing-the-visual-world-paradigm-online">Bringing the visual world paradigm online</h2>
<p>Despite the differences between research-grade and consumer grade eye-tracking, a number of studies have begun to look at if lab-based results replicate online using webcam eye-tracking. Most relevant to this tutorial are online replications using the VWP <span class="citation" data-cites="tanenhaus1995 cooper1974">(<a href="#ref-tanenhaus1995" role="doc-biblioref">Tanenhaus et al., 1995</a>; cf. <a href="#ref-cooper1974" role="doc-biblioref">Cooper, 1974</a>)</span>. For the past 25 years, the VWP has been a dominant force in language research, helping researchers tackle a wide range of topics, including sentence processing <span class="citation" data-cites="eberhard1995">(<a href="#ref-eberhard1995" role="doc-biblioref">Eberhard et al., 1995</a>)</span>, word recognition <span class="citation" data-cites="allopenna1998">(<a href="#ref-allopenna1998" role="doc-biblioref">Allopenna et al., 1998</a>)</span>, bilingualism <span class="citation" data-cites="ito2018">(<a href="#ref-ito2018" role="doc-biblioref">Ito et al., 2018</a>)</span>, and the effects of brain damage on language <span class="citation" data-cites="mirman2012">(<a href="#ref-mirman2012" role="doc-biblioref">Mirman &amp; Graziano, 2012</a>)</span>.</p>
<p>What makes the widespread use of the VWP even more remarkable is the simplicity of the task. In a typical VWP experiment, participants view a display containing several objects and are asked to select one of them by pointing or clicking. While they listen to a spoken word or phrase that identifies the target object, their eye movements are recorded. Remarkably, looks to each object align very closely—and with precise timing—with the mental activation of the word or concept it represents. This provides a unique and detailed view of how cognitive processes unfold in real time.</p>
<p>Most research on visual world eye-tracking has been conducted in laboratory settings using research-grade eye-trackers. However, several attempts have been made to conduct these experiments online using webcam-based eye-tracking. Most online VWP replications have focused on sentence-based language processing. These studies have looked at effects of set size and determiners <span class="citation" data-cites="degen2021">(<a href="#ref-degen2021" role="doc-biblioref">Degen et al., 2021</a>)</span>, verb semantic constraint <span class="citation" data-cites="prystauka2024 slim2023">(<a href="#ref-prystauka2024" role="doc-biblioref">Prystauka et al., 2024</a>; <a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span>, grammatical aspect and event comprehension <span class="citation" data-cites="vos2022">(<a href="#ref-vos2022" role="doc-biblioref">Vos et al., 2022</a>)</span>, and lexical interference <span class="citation" data-cites="prystauka2024">(<a href="#ref-prystauka2024" role="doc-biblioref">Prystauka et al., 2024</a>)</span>.</p>
<p>More relevant to the current paper are findings from single-word VWP studies conducted online. To date, only one study has investigated visual world webcam eye-tracking with single words. <span class="citation" data-cites="slim2024">Slim et al. (<a href="#ref-slim2024" role="doc-biblioref">2024</a>)</span> examined a phonemic cohort task. In the cohort task, pictures were displayed randomly in one of four quadrants, and participants were instructed to fixate on the target based on the auditory cue. On each trial, one of the pictures was phonemically similar to the target in onset (e.g., <em>MILK</em> – <em>MITTEN</em>).</p>
<p>They were able to observe significant fixations to the cohort compared to the control condition, replicating lab-based single word VWP experiments with research grade eye-trackers <span class="citation" data-cites="allopenna1998">(e.g., <a href="#ref-allopenna1998" role="doc-biblioref">Allopenna et al., 1998</a>)</span>. However, <span class="citation" data-cites="slim2024">Slim et al. (<a href="#ref-slim2024" role="doc-biblioref">2024</a>)</span> only observed these competition effects in a later time window compared to remote eye-tracking.</p>
<p>It is important to note, however, that while these studies represent successful replication attempts, there is an important caveat. Most notably, some studies <span class="citation" data-cites="slim2023 slim2024 degen2021">(<a href="#ref-degen2021" role="doc-biblioref">Degen et al., 2021</a>; <a href="#ref-slim2024" role="doc-biblioref">Slim et al., 2024</a>; e.g., <a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span> reported considerable delays in the temporal onset of effects. Several factors likely contribute to these delays, including reduced spatial precision, computational demands, the size of areas of interest (AOIs), and the number of calibrations performed <span class="citation" data-cites="degen2021">(<a href="#ref-degen2021" role="doc-biblioref">Degen et al., 2021</a>)</span>.</p>
<p>More recent work has addressed these limitations by utilizing am updated version of WebGazer.js and using different experimental platforms. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>For instance, <span class="citation" data-cites="vos2022">Vos et al. (<a href="#ref-vos2022" role="doc-biblioref">2022</a>)</span> demonstrated a significant reduction in delays—approximately 50 ms—when comparing lab-based and online versions of the VWP using an updated version of WebGazer within the jsPsych framework <span class="citation" data-cites="deleeuw2015">(<a href="#ref-deleeuw2015" role="doc-biblioref">Leeuw, 2015</a>)</span>. Furthermore, studies by <span class="citation" data-cites="prystauka2024">Prystauka et al. (<a href="#ref-prystauka2024" role="doc-biblioref">2024</a>)</span> and <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (<a href="#ref-bramlett2024" role="doc-biblioref">2024</a>)</span>, which leveraged the Gorilla platform alongside the improved WebGazer algorithm, reported effects comparable to those observed in traditional lab-based VWP studies.</p>
<p>These findings underscore the potential of the online version of the VWP, powered by webcam eye-tracking, to achieve results similar to those of traditional lab-based methods. Importantly, they demonstrate that this approach can effectively be used to study competition effects in single-word speech perception</p>
</section>
<section id="tutorial" class="level2">
<h2 data-anchor-id="tutorial">Tutorial</h2>
<p>Taken together, it seems that webcam eye-tracking is viable alternative to lab-based eye-tracking. Given this, we aimed to support researchers in their efforts to conduct high-quality webcam eye-tracking studies with the VWP. While a valuable tutorial on webcam eye-tracking in the VWP already exists <span class="citation" data-cites="bramlett2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>)</span>, we believe there is value in having multiple resources available to researchers. To this end, we sought to expand on the tutorial by <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (<a href="#ref-bramlett2024" role="doc-biblioref">2024</a>)</span> by incorporating many of their useful recommendations, but also offering an R package to help streamline data pre-processing.</p>
<p>The purpose of this tutorial is to provide an overview of the basic set-up and design features of an online VWP task using the Gorilla platform <span class="citation" data-cites="anwyl-irvine2020">(<a href="#ref-anwyl-irvine2020" role="doc-biblioref">Anwyl-Irvine et al., 2020</a>)</span> and to highlight the pre-processing steps needed to analyze webcam eye-tracking data. Here we use the popular open source programming language R and introduce the <code>webgazeR</code> package <span class="citation" data-cites="webgazeR">(<a href="#ref-webgazeR" role="doc-biblioref">Geller &amp; Prystauka, 2024</a>)</span> to facilitate pre-processing of webcam data. To highlight the steps needed to process webcam eye-tracking data we present data from a Spanish spoken word VWP with L2 Spanish speakers. To our knowledge, L2 processing and competitor effects have not been looked at in the online version of the VWP.</p>
<p>The structure of the tutorial will be as follows. We first outline the general methods used to conduct a visual world webcam eye-tracking experiment. Next, we detail the data preprocessing steps required to prepare the data for analysis. Finally, we demonstrate one statistical approach for analyzing our preprocessed data, highlighting its application and implications.</p>
</section>
</section>
<section id="l2-vwp-webcam-eye-tracking" class="level1">
<h1>L2 VWP Webcam Eye-tracking</h1>
<p>To highlight the preprocessing steps required to analyze webcam eye-tracking data, we examined the competitive dynamics of second-language (L2) learners of Spanish during spoken word recognition. Specifically, we investigated both within-language and cross-language (L2/L1) competition using webcam-based eye-tracking.</p>
<p>It is well established that competition plays a critical role in language processing <span class="citation" data-cites="magnuson2007">(<a href="#ref-magnuson2007" role="doc-biblioref">Magnuson et al., 2007</a>)</span>. In speech perception, as the auditory signal unfolds over time, competitors (or cohorts)—phonological neighbors that differ from the target by an initial phoneme—become activated. To successfully recognize the spoken word, these competitors must be inhibited or suppressed. For example, as the word <em>wizard</em> is spoken, cohorts like <em>whistle</em> might also be briefly activated and in order for wizard to be recognized, <em>whistle</em> must be suppressed.</p>
<p>A key question in the L2 literature is whether competition can occur cross-linguistically, with interactions between a speaker’s first language (L1) and second language (L2). A recent study by <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> explored this question using carefully designed stimuli to examine within- and between linguistic (L2/L1) competition in adult L2 Spanish speakers learners using a Spanish VWP. Their study included two key conditions:</p>
<ol type="1">
<li><p>Spanish-Spanish condition: A Spanish competitor was presented alongside the target word. For example, if the target word spoken was “cielo” (sky), the Spanish competitor was “ciencia” (science).</p></li>
<li><p>Spanish-English (cross-linguistic) condition: An English competitor was presented for the Spanish target word. For example, if the target word spoken was “botas” (boots), the English competitor was “border.”</p></li>
</ol>
<p><span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> also included a no competition condition where the Spanish-English pairs were not cross-linguistic competitors (e.g., <em>frontera</em> as the target word and <em>botas/boots</em> as an unrelated item in the pair). They observed competition effects in both of the critical conditions: within-Spanish competition (e.g., <em>cielo</em> - <em>ciencia</em>) and cross-linguistic competition (e.g., <em>botas</em> - <em>border</em>). For this tutorial, we collected data to conceptually replicate their pattern of findings.</p>
<p>There are two key differences between our dataset and the original study by <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> worth noting. First, <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> focused on adult L2 Spanish speakers and posed more fine-grained questions about the time course of competition and resolution and its relationship with L2 language acquisition. Second, unlike McCall et al., who measured Spanish proficiency objectively (e.g., using LexTALE-esp; <span class="citation" data-cites="izura2014">Izura et al. (<a href="#ref-izura2014" role="doc-biblioref">2014</a>)</span> ), we relied on Prolific’s filters to recruit L2 Spanish speakers.</p>
<p>Our primary goal here was to demonstrate the pre-processing steps required to analyze webcam-based eye-tracking data. A secondary goal was to provide evidence of L2 competition within and between or cross-linguistically using this methodology. To our knowledge, no papers have looked at spoken word recognition and competition using online methods. It is our hope that researchers can use this to test more detailed questions about L2 processing using webcam-based eye-tracking.</p>
<section id="method" class="level2">
<h2 data-anchor-id="method">Method</h2>
<p>All tasks herein can be previewed here (<a href="https://app.gorilla.sc/openmaterials/953693" class="uri">https://app.gorilla.sc/openmaterials/953693</a>). The manuscript, data, and R code can be found on Github (<a href="https://github.com/jgeller112/webcam_gazeR_VWP" class="uri">https://github.com/jgeller112/webcam_gazeR_VWP</a>).</p>
<section id="participants" class="level3">
<h3 data-anchor-id="participants">Participants</h3>
<p>We recruited participants from Prolific, a participant recruitment platform, who where: (1) between the ages of 18 and 36 years old, (2) native English speakers, (3) were also fluent in Spanish, and (4) residents of the US. All participants were taken to the Gorilla hosting and experiment platform (www.gorilla.sc; <span class="citation" data-cites="anwyl-irvine2020">(<a href="#ref-anwyl-irvine2020" role="doc-biblioref">Anwyl-Irvine et al., 2020</a>)</span>. The participant flow is shown in <a href="#fig-sankey" class="quarto-xref" aria-expanded="false">Figure&nbsp;1</a>. A total of 187 participants consented t participate in the study. Of these, 111 passed the headphone screener checkpoint and proceeded to the VWP. Among these, 32 participants successfully completed the Visual World Paradigm (VWP) task with at least 100 trials, while 79 participants failed calibration. Ninety-one participants completed the entire experiment, including the final questionnaires. <a href="#tbl-demo2" class="quarto-xref" aria-expanded="false">Table&nbsp;1</a> provides basic demographic information about the participants who completed the full experiment. After applying additional exclusion criteria (low accuracy (&lt; 80%) and excessive missing eye-data (&gt; 30%) , the final sample consisted of 28 participants with usable eye-tracking data.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-sankey" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="1" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-sankey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant flow through the experiment</p>
</div>
</figcaption>
<div aria-describedby="fig-sankey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/sankey_plot.svg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
</figure>
</div>
</div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="1" data-custom-style="FigureWithoutNote">
<div id="tbl-demo2" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="1">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-demo2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant demographic variables</p>
</div>
</figcaption>
<div aria-describedby="tbl-demo2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/my_demo.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="materials" class="level3">
<h3 data-anchor-id="materials">Materials</h3>
<section id="vwp" class="level4">
<h4 data-anchor-id="vwp">VWP.</h4>
<section id="items" class="level5">
<h5 data-anchor-id="items">Items.</h5>
<p>We adapted materials from <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span>. In their cross-linguistic VWP, participants were presented with four pictures and a spoken Spanish word and had to select the image that matched the spoken word by clicking on it. The word stimuli for the experiment were chosen from textbooks used by students in their first and second year college Spanish courses.</p>
<p>The item sets consisted of two types of phonologically-related word pairs: one pair of Spanish-Spanish words and another of Spanish-English words. The Spanish-Spanish pairs were unrelated to the Spanish-English pairs. All the word pairs were carefully controlled on a number of dimensions (see <span class="citation" data-cites="sarrett2022">(<a href="#ref-sarrett2022" role="doc-biblioref">Sarrett et al., 2022</a>)</span>).</p>
<p>There were three experimental conditions: (1) the Spanish-Spanish condition, where one of the Spanish words was the target and the other was the competitor; (2) the Spanish-English condition, where a Spanish word was the target and its English phonological cohort served as the competitor; and (3) the No Competitor condition, where the Spanish word did not overlap with any other word in the set. The Spanish-Spanish condition had twice as many trials as the other conditions due to the interchangeable nature of the target and competitor words in that pair.</p>
<p>There were 15 sets of 4 items (half the number of sets used in <span class="citation" data-cites="sarrett2022">(<a href="#ref-sarrett2022" role="doc-biblioref">Sarrett et al., 2022</a>)</span>). Each item within a set was repeated 4 times as the target word. This yielded 240 trials (15 sets × 4 items per set × 4 repetitions). Each item set consisted of one Spanish-Spanish cohort pair and one Spanish-English cohort pair. Both items in a Spanish-Spanish pair had a“reciprocal” competitor relationship (that is, we could test activation for <em>cielo</em> given <em>ciencia</em>, and for <em>ciencia</em> given <em>cielo</em>). Consequently, there were 120 trials in the Spanish-Spanish condition. In contrast, only one item from the Spanish-English pair had the speciﬁed competitor relationship (we could test activation for frontera <em>border</em>, given <em>botas</em>, but when hearing <em>frontera</em>, there was no competitor). Thus, there were only 60 trials for each the Spanish-English competition as well as the No Competitor condition. Items occurred in each of the four corners of the screen on an equal numbers of trials.</p>
</section>
<section id="stimuli" class="level5">
<h5 data-anchor-id="stimuli">Stimuli.</h5>
<p>In <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> all auditory stimuli were recorded by a female bilingual speaker whose native language was Mexican Spanish and also spoke English. Stimuli were recorded in a sound-attenuated room sampled at 44.1 kHz. Auditory tokens were edited to reduce noise and remove clicks. The auditory tokens were then amplitude normalized to 70 dB SPL. For each target word, there were four separate recordings so each instance was unique.</p>
<p>Visual stimuli were images from a commercial clipart database that were selected by a consensus method involving a small group of students. All .wav files were converted to .mp3 for online data collection. All stimuli can be found here: <a href="https://osf.io/mgkd2/" class="uri">https://osf.io/mgkd2/</a>.</p>
</section>
</section>
<section id="headphone-screener" class="level4">
<h4 data-anchor-id="headphone-screener">Headphone screener.</h4>
<p>Headphones were required for all participants. To ensure this, we used a six-trial task taken from <span class="citation" data-cites="woods2017">Woods et al. (<a href="#ref-woods2017" role="doc-biblioref">2017</a>)</span>. On each trial, three tones of the same frequency and duration were presented sequentially. One tone had a lower amplitude than the other two tones. Tones were presented in stereo, but the tones in the left and right channels were 180 out of phase across stereo channels—in free field, these sounds should cancel out or create distortion, whereas they will be perfectly clear over headphones. The listener picked which of the three tones was the quietest. Performance is generally at the ceiling when wearing headphones but poor when listening in the free field (due to phase cancellation).</p>
</section>
<section id="demographics-questionnaire" class="level4">
<h4 data-anchor-id="demographics-questionnaire">Demographics questionnaire.</h4>
<p>Participants completed a demographic questionnaire as part of the study. The questions covered basic demographic information, including age, gender, spoken dialect, ethnicity, and race.</p>
<p>Participants also answered a series of questions related to their personal health and environmental conditions during the experiment. These questions addressed any history of vision problems (e.g., corrected vision, eye disease, or drooping eyelids) and whether they were currently taking medications that might impair judgment. Participants also indicated if they were wearing eyeglasses, contacts, makeup, false eyelashes, or hats.</p>
<p>The questionnaire inquired about their environment, asking if there was natural light in the room, if they were using a built-in camera or an external one (with an option to specify the brand), and their estimated distance from the camera. Participants were asked to estimate how many times they looked at their phone or got up during the experiment and whether their environment was distraction-free.</p>
<p>Additional questions assessed the clarity of calibration instructions, allowing participants to suggest improvements, and asked if they were wearing a mask during the session. These questions aimed to gather insights into personal and environmental factors that could impact data quality and participant comfort during the experiment.</p>
<p>To gauge L2 experience, we asked participants when they started speaking Spanish, how many years of Spanish speaking experience they had, and to provide, on a scale between 0-100, how often they use Spanish in their daily lives.</p>
</section>
</section>
<section id="procedure" class="level3">
<h3 data-anchor-id="procedure">Procedure</h3>
<p>All tasks were completed in a single session, lasting approximately 45 minutes. The tasks were presented in a fixed order: consent, headphone screener, spoken word VWP, and questionnaire items.</p>
<p>The experiment was programmed in the Gorilla Experiment Platform (Anwyl-Irvine et al., 2019), with personal computers as the only permitted device type. Upon entering the online study, participants received general information to decide if they wished to participate, after which they provided informed consent. Participants were then instructed to adjust the volume to a comfortable level while noise played.</p>
<p>Next, participants completed a headphone screening test. They had three attempts to pass this test. If unsuccessful by the third attempt, participants were directed to an early exit screen, followed by the questionnaire.</p>
<p>For those who passed the screening, the next task was the VWP. This began with instructional videos providing specific guidance on the ideal experiment setup for eye-tracking and calibration procedures. Participants were then required to enter full-screen mode before calibration. A 9 point calibration procedure was used. Calibration occurred every 60 trials for a total of 3 calibrations. Participants had three attempts to successfully complete each calibration phase. If calibration was unsuccessful, participants were directed to an early exit screen, followed by the questionnaire.</p>
<p>In the main VWP task, each trial began with a 500 ms fixation cross at the center of the screen. This was followed by a preview screen displaying four images, each positioned in a corner of the screen. After 1500 ms, a start button appeared in the center. Participants clicked the button to confirm they were focused on the center before the audio played. Once clicked, the audio was played, and the images remained visible. Participants were instructed to click the image that best matched the spoken target word, while their eye movements were recorded. Eye movements were only recorded on that screen. <a href="#fig-vwptrial" class="quarto-xref" aria-expanded="false">Figure&nbsp;2</a> displays the VWP trial sequence.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-vwptrial" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="2" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-vwptrial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;2</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>VWP trial schematic</p>
</div>
</figcaption>
<div aria-describedby="fig-vwptrial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/trial_descrip.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>After completing the main VWP task, participants proceeded to the final questionnaire, which included questions about the eye-tracking task and basic demographic information. Participants were then thanked for their participation.</p>
</div>
</section>
</section>
<section id="preprocessing-data" class="level2">
<h2 data-anchor-id="preprocessing-data">Preprocessing data</h2>
<p>After the data is collected you can begin preprocessing your data. Below we highlight the steps needed to preprocess your webcam eye-tracking data and get it ready for analysis. For some of this preprocessing we will use the newly created <code>webgazeR</code>package (v. 0.1.0) which is an extension of the <code>gazeR</code> package (Geller et al., 2020) which was created to analyze VWP data in lab-based studies.</p>
<p>For preprocessing visual world webcam eye data, we follow six general steps:</p>
<ol type="1">
<li>Reading in data</li>
<li>Data Exclusion</li>
<li>Combining trial- and eye-level data</li>
<li>Assigning areas of interest</li>
<li>Time Binning</li>
<li>Aggregating (optional)</li>
</ol>
<p>For each of these steps, we will display R code chunks demonstrating how to perform each step with helper functions (if applicable) from the <code>webgazeR</code> <span class="citation" data-cites="webgazeR">(<a href="#ref-webgazeR" role="doc-biblioref">Geller &amp; Prystauka, 2024</a>)</span> package in R.</p>
<section id="load-packages" class="level3">
<h3 data-anchor-id="load-packages">Load packages</h3>
<section id="package-installation-and-setup" class="level5">
<h5 data-anchor-id="package-installation-and-setup">Package Installation and Setup.</h5>
<p>Before turning to the pre-processing code below, we will need to make sure all the necessary packages are installed. The code will not run if the packages are not installed properly. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
</section>
<section id="webgazer-installation" class="level5">
<h5 data-anchor-id="webgazer-installation">webgazeR installation.</h5>
<p>The <code>webgazeR</code> package is installed from the Github repository using the <code>remotes</code> package.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(remotes) <span class="co"># install github repo</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">"jgeller112/webgazeR"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once this is installed, <code>webgazeR</code> can be loaded along with additional useful packages. The following code will load the required packages or install them if you do not have them on your system.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>) <span class="co"># no automatic data transformation</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="st">"scipen"</span> <span class="ot">=</span> <span class="dv">100</span>, <span class="st">"digits"</span> <span class="ot">=</span> <span class="dv">10</span>) <span class="co"># suppress math annotation</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># List of required packages</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>required_packages <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"tidyverse"</span>,      <span class="co"># data wrangling</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">"here"</span>,           <span class="co"># relative paths instead of absolute aids in reproducibility</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">"tinytable"</span>,      <span class="co"># nice tables</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">"janitor"</span>,        <span class="co"># functions for cleaning up your column names</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="st">"webgazeR"</span>,       <span class="co"># has webcam functions</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="st">"readxl"</span>,         <span class="co"># read in Excel files</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">"ggokabeito"</span>,     <span class="co"># color-blind friendly palettes</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="st">"flextable"</span>,      <span class="co"># Word tables</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"permuco"</span>,        <span class="co"># permutation analysis</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">"foreach"</span>,        <span class="co"># permutation analysis</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">"geomtextpath"</span>,   <span class="co"># for plotting labels on lines of ggplot figures</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  <span class="st">"cowplot"</span>         <span class="co"># combine ggplot figures</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Install and load each package</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (pkg <span class="cf">in</span> required_packages) {</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(pkg, <span class="at">character.only =</span> <span class="cn">TRUE</span>)) {</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">install.packages</span>(pkg, <span class="at">dependencies =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(pkg, <span class="at">character.only =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once <code>webgazeR</code> and other helper packages have been installed and loaded the user is ready to start cleaning your data.</p>
</section>
</section>
<section id="reading-in-data" class="level3">
<h3 data-anchor-id="reading-in-data">Reading in data</h3>
<section id="behavioral-trial-level-data" class="level4">
<h4 data-anchor-id="behavioral-trial-level-data">Behavioral, trial-level, data.</h4>
<p>To process eye-tracking data you will need to make sure you have both the behavioral data and the eye-tracking data files. We have all the data needed in the repository by navigating to the L2 subfolder in the data folder (data -&gt; L2). For the behavioral data, Gorilla produces a <code>.csv</code> file that includes trial-level information (here contained in the object <code>L2_data)</code>. The files needed are called <code>data_exp_196386-v5_task-scf6.csv</code>. and <code>data_exp_196386-v6_task-scf6.csv</code>. We have two files because we ran a modified version of the experiment.</p>
<p>The .csv files contain meta-data for each each trial, such as what picture were presented on each trial, which object was the target, reaction times, audio presentation times, what object was clicked on, etc. To load our data files into our R environment, we use the <code>here</code> package to set a relative rather than an absolute path to our files. We read in the data files from the repositroy for both versions of the task and merge the files together. <code>L2_data</code> merges both <code>data_exp_196386-v5_task-scf6.csv</code> and <code>data_exp_196386-v6_task-scf6.csv</code> into one object.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in trial level data </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># combine data from version 5 and 6 of the task</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>L2_1 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"data_exp_196386-v5_task-scf6.csv"</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>L2_2 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"data_exp_196386-v6_task-scf6.csv"</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>L2_data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(L2_1, L2_2) <span class="co"># bind the two objects together </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="eye-tracking-data" class="level4">
<h4 data-anchor-id="eye-tracking-data">Eye-tracking data.</h4>
<p>Gorilla currently saves each participant’s eye-tracking data trial by trial. The <code>raw</code> subfolder in the data folder in the project repository contains the eye-tracking files by participant for each trial individually. Contained in those files, we have information pertaining to each trial such as participant id,time since trial started, x and y coordinates of looks, convergence (the model’s confidence in finding a face (and accurately predicting eye movements), face confidence (represents the support vector machine (SVM) classifier score for the face model fit), and information pertaining to the the AOI screen coordinates (standardized and user-specific). The <code>vwp_files_L2</code> object below contains a list of all the files contained in the folder. Because <code>vwp_files_L2</code> contains trial data as well as calibration data, we remove the calibration trials and save the files to <code>vwp_paths_filtered_L2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the list of all files in the folder</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>vwp_files_L2  <span class="ot">&lt;-</span> <span class="fu">list.files</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"raw"</span>), <span class="at">pattern =</span> <span class="st">"</span><span class="sc">\\</span><span class="st">.xlsx$"</span>, <span class="at">full.names =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Exclude files that contain "calibration" in their filename</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>vwp_paths_filtered_L2 <span class="ot">&lt;-</span> vwp_files_L2[<span class="sc">!</span><span class="fu">grepl</span>(<span class="st">"calibration"</span>, vwp_files_L2)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When data is generated from Gorilla, each trial in your experiment is saved as an individual file. Because of this, we need some way to take all the individual files and merge them together. The <code>merge_webcam_files()</code>function merges each trial from each participant into a single tibble or data frame. Before running the <code>merge_webcam_files()</code> function, ensure that your working directory is set to where the files are stored. <code>merge_webcam_files()</code> reads in all the .xlsx files from the raw subfolder, binds them together into one dataframe, and cleans up the column names. The function then filters the data to include only rows where the type is “prediction” and the <code>screen_index</code> matches the specified value (in our case, screen 4 is where we collected eye-tracking data). If you recorded across multiple screens the <code>screen_index</code> argument can take multiple values (e.g., <code>screen_index</code>= c(1, 4, 5), will take eye-tacking information from screens, 1, 4, and 5)). <code>merge_webcam_files()</code> also renames the <code>spreadsheet_row</code> column to trial and sets both <code>trial</code> and <code>subject</code> as factors for further analysis in our pipeline. As a note, all steps should be followed in order due to the renaming of column names. If you encounter an error it might be because column names have not been changed.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setwd</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">"data"</span>, <span class="st">"L2"</span>, <span class="st">"raw"</span>)) <span class="co"># set working directory to raw data folder</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>edat_L2 <span class="ot">&lt;-</span> <span class="fu">merge_webcam_files</span>(vwp_paths_filtered_L2, <span class="at">screen_index=</span><span class="dv">4</span>) <span class="co"># eye tracking occured ons creen index 4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="subject-and-trial-level-data-removal" class="level3">
<h3 data-anchor-id="subject-and-trial-level-data-removal">Subject and trial level data removal</h3>
<p>To ensure high-quality data, it is essential to filter out unreliable data based on both behavioral and eye-tracking criteria before merging datasets. In our dataset, participants will be excluded if they meet any of the following conditions: failure to successfully calibrate throughout the experiment (less than 100 trials), low accuracy ( &lt; 80%) , low sampling rates ( &lt; 5), and a high proportion of gaze data outside the screen coordinates ( &gt; 30%). Successful calibration is crucial for capturing accurate eye-tracking measurements, so participants who could not maintain proper calibration may have inaccurate gaze data. Similarly, low accuracy may indicate poor engagement or task difficulty, which can reduce the reliability of the behavioral data and suggest that eye-tracking data may be less precise.</p>
<p>First, we will create a cleaned up version of our use the behavioral, trial-level, data <code>L2_data</code> by creating an object named <code>eye_behav_L2</code> that selects useful columns from that file and renames stimuli to make them more intuitive. Because most of this will be user-specific, no function is called here. Below we describe the preprocessing done on the behavioral data file. The below code processes and transforms the <code>L2_data</code> dataset into a cleaned and structured format for further analysis. First, the code renames several columns for easier access using <code>janitor::clean_names()</code> <span class="citation" data-cites="janitor">(<a href="#ref-janitor" role="doc-biblioref">Firke, 2023</a>)</span> function. We then select only the columns we need and filter the dataset to include only rows where <code>zone_type</code> is “response_button_image”, representing the picture selected for that trial. Afterward, the function renames additional columns (<code>tlpic</code> to <code>TL</code>, <code>trpic</code> to <code>TR</code>, etc.). We also renamed <code>participant_private_id</code> to <code>subject</code>, <code>spreadsheet_row</code> to <code>trial</code>, and <code>reaction_time</code> to <code>RT</code>. This makes our columns consistent with the <code>edat</code> above for merging later on. Lastly, <code>reaction time</code> (RT) is converted to a numeric format for further numerical analysis.</p>
<p>It is important to note here that what the behavioral spreadsheet denotes as trial is not in fact the trial number used in the eye-tracking files. Thus it is imperative you use <code>spreadhseet row</code> as trial number to merge the two files successfully.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#|message: false</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#|echo: true</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>eye_behav_L2 <span class="ot">&lt;-</span> L2_data <span class="sc">%&gt;%</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  janitor<span class="sc">::</span><span class="fu">clean_names</span>() <span class="sc">%&gt;%</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Select specific columns to keep in the dataset</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(participant_private_id,  correct, tlpic, trpic, blpic, brpic, condition, eng_targetword, targetword, typetl, typetr, typebl, typebr, zone_name, zone_type,reaction_time, spreadsheet_row,  response) <span class="sc">%&gt;%</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Filter the rows where 'Zone.Type' equals "response_button_image"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(zone_type <span class="sc">==</span> <span class="st">"response_button_image"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Rename columns for easier use and readability</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TL"</span> <span class="ot">=</span> <span class="st">"tlpic"</span>,              <span class="co"># Rename 'tlpic' to 'TL'</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TR"</span> <span class="ot">=</span> <span class="st">"trpic"</span>,             <span class="co"># Rename 'trpic' to 'TR'</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"BL"</span> <span class="ot">=</span> <span class="st">"blpic"</span>,            <span class="co"># Rename 'blpic' to 'BL'</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"BR"</span> <span class="ot">=</span> <span class="st">"brpic"</span>,                <span class="co"># Rename 'brpic' to 'BR'</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"targ_loc"</span> <span class="ot">=</span> <span class="st">"zone_name"</span>,       <span class="co"># Rename 'Zone.Name' to 'targ_loc'</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"subject"</span> <span class="ot">=</span> <span class="st">"participant_private_id"</span>,  <span class="co"># Rename 'Participant.Private.ID' to 'subject'</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trial"</span> <span class="ot">=</span> <span class="st">"spreadsheet_row"</span>,    <span class="co"># Rename 'spreadsheet_row' to 'trial'</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"acc"</span> <span class="ot">=</span> <span class="st">"correct"</span>,              <span class="co"># Rename 'Correct' to 'acc' (accuracy)</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RT"</span> <span class="ot">=</span> <span class="st">"reaction_time"</span>          <span class="co"># Rename 'Reaction.Time' to 'RT'</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Convert the 'RT' (Reaction Time) column to numeric type</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">RT =</span> <span class="fu">as.numeric</span>(RT),</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>         <span class="at">subject=</span><span class="fu">as.factor</span>(subject),</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>         <span class="at">trial=</span><span class="fu">as.factor</span>(trial))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="audio-onset" class="level4">
<h4 data-anchor-id="audio-onset">Audio onset.</h4>
<p>Because we are using spoken audio on each trial and running this experiment from the browser, audio onset is never going to to consistent across participants. In Gorilla there is an option to collect advanced audio features (you must make sure you select this when designing the study) such as when the audio play was requested, fired (played) and when the audio ended. To do so you must click on advanced settings and select 1 (see <a href="#fig-audiopres" class="quarto-xref" aria-expanded="false">Figure&nbsp;3</a>). We will want to incorporate this timing information into our analysis pipeline. Gorilla records the onset of the audio which varies by participant. We are extracting that in the <code>audio_rt_L2</code> object by filtering <code>zone_type</code> to <code>content_web_audio</code> and response equal to “AUDIO PLAY EVENT FIRED”. This will tell us when the audio was triggered in the experiment. We are creating a column called (<code>RT_audio</code>) which we will use later on to correct for audio delays.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-audiopres" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="3" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-audiopres-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;3</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Advanced audio settings in Gorilla</p>
</div>
</figcaption>
<div aria-describedby="fig-audiopres-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/audio_settings.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>audio_rt_L2 <span class="ot">&lt;-</span> L2_data <span class="sc">%&gt;%</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  janitor<span class="sc">::</span><span class="fu">clean_names</span>()<span class="sc">%&gt;%</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">select</span>(participant_private_id,zone_type, spreadsheet_row, reaction_time, response) <span class="sc">%&gt;%</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(zone_type<span class="sc">==</span><span class="st">"content_web_audio"</span>, response<span class="sc">==</span><span class="st">"AUDIO PLAY EVENT FIRED"</span>)<span class="sc">%&gt;%</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>() <span class="sc">%&gt;%</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="st">"subject"</span> <span class="ot">=</span> <span class="st">"participant_private_id"</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>       <span class="st">"trial"</span> <span class="ot">=</span><span class="st">"spreadsheet_row"</span>,  </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>       <span class="st">"RT_audio"</span> <span class="ot">=</span> <span class="st">"reaction_time"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="fu">select</span>(<span class="sc">-</span>zone_type) <span class="sc">%&gt;%</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="fu">mutate</span>(<span class="at">RT_audio=</span><span class="fu">as.numeric</span>(RT_audio))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then merge this information with <code>eye_behav_L2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># merge the audio Rt data to the trial level object</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>trial_data_rt_L2 <span class="ot">&lt;-</span> <span class="fu">merge</span>(eye_behav_L2, audio_rt_L2, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"trial"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="trial-removal" class="level4">
<h4 data-anchor-id="trial-removal">Trial removal.</h4>
<p>As stated above, participants who did not successfully calibrate 3 times or less were rejected from the experiment. Let’s take a look at how many trials each participant had using the <code>trial_data_rt_L2</code> object. Deciding to remove trials is ultimately up to the researcher. In our case, we removed participants with less than 100 trials. In <a href="#tbl-partL2" class="quarto-xref" aria-expanded="false">Table&nbsp;2</a> we can see several participants failed some of the calibration attempts and do not have an adequate number of trials. Again we make no strong recommendations here. If you to decide to do this, we recommend pre-registering this decision.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find out how many trials each participant had</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>edatntrials_L2 <span class="ot">&lt;-</span>trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">group_by</span>(subject)<span class="sc">%&gt;%</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">ntrials=</span><span class="fu">length</span>(<span class="fu">unique</span>(trial)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="2" data-custom-style="FigureWithoutNote">
<div id="tbl-partL2" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="2">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-partL2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;2</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participants with less than 100 trials</p>
</div>
</figcaption>
<div aria-describedby="tbl-partL2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-0ac43b1e{}.cl-0abe0460{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0ac0a2ec{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-0ac0bfde{width:1.063in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0ac0bfe8{width:0.763in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0ac0bff2{width:1.063in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0ac0bff3{width:0.763in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0ac0bffc{width:1.063in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0ac0c006{width:0.763in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-0ac43b1e"><thead><tr style="overflow-wrap:break-word;"><th class="cl-0ac0bfde"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">subject</span></p></th><th class="cl-0ac0bfe8"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">ntrials</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12102265</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">2</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12110638</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">55</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12110829</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">59</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12110878</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">59</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12110897</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">60</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111234</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">57</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111244</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">58</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111363</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">58</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111663</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">57</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111703</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">58</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111869</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">60</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12111960</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">46</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12112152</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">59</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12212113</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">56</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bff2"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12213826</span></p></td><td class="cl-0ac0bff3"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">99</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0ac0bffc"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">12213965</span></p></td><td class="cl-0ac0c006"><p class="cl-0ac0a2ec"><span class="cl-0abe0460">59</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>Let’s remove them from the analysis using the below code.</p>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>trial_data_rt_L2 <span class="ot">&lt;-</span> trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(subject <span class="sc">%in%</span> edatntrials_bad_L2<span class="sc">$</span>subject)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="low-accuracy" class="level4">
<h4 data-anchor-id="low-accuracy">Low accuracy.</h4>
<p>In our experiment, we want to make sure accuracy is high (&gt; 80%). Again, we want participants that are fully attentive in the experiment. In the below code, we keep participants with accuracy equal to or above 80% and only include correct trials and save it to <code>trial_data_acc_clean_L2</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Calculate mean accuracy per subject and filter out subjects with mean accuracy &lt; 0.8</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>subject_mean_acc_L2 <span class="ot">&lt;-</span> trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(subject) <span class="sc">%&gt;%</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">mean_acc =</span> <span class="fu">mean</span>(acc, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(mean_acc <span class="sc">&gt;</span> <span class="fl">0.8</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Join the mean accuracy back to the main dataset and exclude trials with accuracy &lt; 0.8</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>trial_data_acc_clean_L2 <span class="ot">&lt;-</span> trial_data_rt_L2 <span class="sc">%&gt;%</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(subject_mean_acc_L2, <span class="at">by =</span> <span class="st">"subject"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(acc<span class="sc">==</span><span class="dv">1</span>) <span class="co"># only use accurate responses for fixation analysis</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rts" class="level4">
<h4 data-anchor-id="rts">RTs.</h4>
<p>There is much debate on how to handle RT data <span class="citation" data-cites="miller2023">(see <a href="#ref-miller2023" role="doc-biblioref">Miller, 2023</a>)</span>. Because of this. we leave it up to the reader and researcher to decide what to do with RTs. In the current example we ignore RTs.</p>
</section>
<section id="sampling-rate" class="level4">
<h4 data-anchor-id="sampling-rate">Sampling rate.</h4>
<p>While most commercial eye-trackers sample at a constant rate, data captured by webcams are widely inconsistent. Below is some code to calculate the sampling rate of each participant. Ideally, you should not have a sampling rate less than 5 Hz. It has been recommended you drop those values <span class="citation" data-cites="bramlett2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>)</span> The below function <code>analyze_sample_rate()</code> calculates calculates the sampling rate for each subject and each trial in our eye-tracking dataset (<code>edat_L2</code>). The function provides overall statistics, including the median (used by <span class="citation" data-cites="bramlett2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>)</span>) and standard deviation of sampling rates in your experiment,and also generates a histogram of median sampling rates by subject. Looking at <a href="#fig-samprate-L2" class="quarto-xref" aria-expanded="false">Figure&nbsp;4</a>, the sampling rate ranges from 5 to 35 Hz with a median sampling rate of 21.56. This corresponds to previous webcam eye-tracking work (e.g., <span class="citation" data-cites="prystauka2024 bramlett2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>; <a href="#ref-prystauka2024" role="doc-biblioref">Prystauka et al., 2024</a>)</span>)</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-fig-heigh="8" data-custom-style="FigureWithoutNote">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>samp_rate_L2 <span class="ot">&lt;-</span> <span class="fu">analyze_sampling_rate</span>(edat_L2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overall Median Sampling Rate (Hz): 21.56171771 
Overall Standard Deviation of Sampling Rate (Hz): 7.399937723 

Sampling Rate by Trial:
# A tibble: 10,665 × 5
# Groups:   subject [60]
   subject  trial max_time n_times    SR
   &lt;fct&gt;    &lt;fct&gt;    &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;
 1 12102265 8        4895      108  22.1
 2 12102265 11       4920.     112  22.8
 3 12102265 15       4911.      79  16.1
 4 12102265 17       4916.     113  23.0
 5 12102265 20       4903.     112  22.8
 6 12102265 21       1826.      40  21.9
 7 12102265 28       4917.     114  23.2
 8 12102265 31       4913.      79  16.1
 9 12102265 34       4948.      88  17.8
10 12102265 35       4901.      93  19.0
# ℹ 10,655 more rows

Median Sampling Rate by Subject:
# A tibble: 60 × 2
   subject  med_SR
   &lt;fct&gt;     &lt;dbl&gt;
 1 12102265  21.9 
 2 12102286  30.6 
 3 12102530  19.9 
 4 12110559  29.3 
 5 12110579  13.3 
 6 12110585  30.1 
 7 12110586  14.8 
 8 12110600   2.47
 9 12110638  29.0 
10 12110685  19.5 
# ℹ 50 more rows</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-samprate-L2" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="4" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-samprate-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;4</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant sampling-rate for L2 experiment. A histogram and overlayed density plot shows median sampling rate by participant. the overall median and SD is highlighted in red.</p>
</div>
</figcaption>
<div aria-describedby="fig-samprate-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-samprate-L2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>When using the above function, separate dataframes are produced by-participants and by-trial. These can be added to the behavioral dataframe using the below code.</p>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract by-subject and by-trial sampling rates from the result</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>subject_sampling_rate_L2 <span class="ot">&lt;-</span> samp_rate_L2<span class="sc">$</span>median_SR_by_subject  <span class="co"># Sampling rate by subject</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>trial_sampling_rate_L2 <span class="ot">&lt;-</span> samp_rate_L2<span class="sc">$</span>SR_by_trial  <span class="co"># Sampling rate by trial</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>trial_sampling_rate_L2<span class="sc">$</span>subject<span class="ot">&lt;-</span><span class="fu">as.factor</span>(trial_sampling_rate_L2<span class="sc">$</span>subject)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming target_data is your other dataset that contains subject and trial information</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Append the by-subject sampling rate to target_data (based on subject)</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>subject_sampling_rate_L2<span class="sc">$</span>subject <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(subject_sampling_rate_L2<span class="sc">$</span>subject)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming target_data is your other dataset that contains subject and trial information</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Append the by-subject sampling rate to target_data (based on subject)</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>trial_sampling_rate_L2<span class="sc">$</span>subject<span class="ot">&lt;-</span><span class="fu">as.factor</span>(trial_sampling_rate_L2<span class="sc">$</span>subject)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming target_data is your other dataset that contains subject and trial information</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Append the by-subject sampling rate to target_data (based on subject)</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>subject_sampling_rate_L2<span class="sc">$</span>subject <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(subject_sampling_rate_L2<span class="sc">$</span>subject)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>trial_data_acc_clean_L2<span class="sc">$</span>subject <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(trial_data_acc_clean_L2<span class="sc">$</span>subject)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>target_data_with_subject_SR_L2 <span class="ot">&lt;-</span> trial_data_acc_clean_L2 <span class="sc">%&gt;%</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(subject_sampling_rate_L2, <span class="at">by =</span> <span class="st">"subject"</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>target_data_with_subject_SR_L2<span class="sc">$</span>trial <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(target_data_with_subject_SR_L2<span class="sc">$</span>trial)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Append the by-trial sampling rate to target_data (based on subject and trial)</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>target_data_with_full_SR_L2 <span class="ot">&lt;-</span> target_data_with_subject_SR_L2 <span class="sc">%&gt;%</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(subject, trial, med_SR)<span class="sc">%&gt;%</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(trial_sampling_rate_L2, <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"trial"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>trial_data_L2 <span class="ot">&lt;-</span> <span class="fu">left_join</span>(trial_data_acc_clean_L2, target_data_with_full_SR_L2, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"trial"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use this information to filter out data with poor sampling rates. Users can use the <code>filter_sampling_rate()</code> function to either (1) throw out data, by-participant, by-trial, or both, or (2) label sampling rates below a certain threshold as bad (TRUE or FALSE). Let’s use the <code>filter_sampling_rate()</code> function to do this. We will use our <code>trial_data_L2</code> object.</p>
<p>We leave it up to the user to decide what to do with low sampling rates and make no specific recommendations here. In our case we are going to remove the data by-participant and by-trial (setting <code>action</code> = “both” ) if sampling frequency is below 5hz (<code>threshold</code>=5). The <code>filter_sampling_rate()</code> function is designed to process a dataset containing participant-level and trial-level sampling rates. It allows the user to either filter out data that falls below a certain sampling rate threshold or simply label it as “bad”. The function gives flexibility by allowing the threshold to be applied at the participant-level, trial-level, or both. It also lets the user decide whether to remove the data or flag it as below the threshold without removing it. If <code>action</code> = remove, the function will output how many subjects and trials were removed by on the threshold.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>filter_edat_L2 <span class="ot">&lt;-</span> <span class="fu">filter_sampling_rate</span>(trial_data_L2,<span class="at">threshold =</span> <span class="dv">5</span>,</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">action =</span> <span class="st">"remove"</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">by =</span> <span class="st">"both"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The message produced states that 1 subject is thrown out along with 107 trials (trials associated with the 1 subject).</p>
</section>
<section id="out-of-bounds-outside-of-screen" class="level4">
<h4 data-anchor-id="out-of-bounds-outside-of-screen">Out-of-bounds (outside of screen).</h4>
<p>It is important that we do not include points that fall outside the standardized coordinates (0,1). The <code>gaze_oob()</code> function calculates how many of the data points fall outside the standardized range. Here we need our eye-tracking data (<code>edat_L2</code>). Running the <code>gaze_oob()</code> function returns a table listing how many data points fall outside this range (total, X and Y), and also provides percentages (see <a href="#tbl-oob-L2" class="quarto-xref" aria-expanded="false">Table&nbsp;3</a>). This information would be useful to include in the final paper.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>oob_data_L2 <span class="ot">&lt;-</span> <span class="fu">gaze_oob</span>(edat_L2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="3" data-custom-style="FigureWithoutNote">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "/home/runner/work/L2_VWP_Webcam/L2_VWP_Webcam/_manuscript/Figures/oob_data_L2.png"</code></pre>
</div>
<div id="tbl-oob-L2" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="3">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-oob-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;3</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Out of bounds gaze statistics</p>
</div>
</figcaption>
<div aria-describedby="tbl-oob-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/oob_data_L2.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>We can also add add by-participant and by-trial out of bounds data to our behavioral, trial-level, data (<code>filter_edat_L2</code>) and finally exclude participants and trials with more than 30% missing data. The value of 30 is just a suggestion and should not be used as a rule of thumb for all studies nor are we endorsing this value.</p>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>remove_missing <span class="ot">&lt;-</span> oob_data_L2 <span class="sc">%&gt;%</span>                             <span class="co"># Start with the `oob_data` dataset and assign the result to `remove_missing`</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(subject, total_missing_percentage) <span class="sc">%&gt;%</span>            <span class="co"># Select only the `subject` and `total_missing_percentage` columns from `oob_data`</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(filter_edat_L2, <span class="at">by =</span> <span class="st">"subject"</span>) <span class="sc">%&gt;%</span>               <span class="co"># Perform a left join with `filter_edat` on the `subject` column, keeping all rows from `oob_data`</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(total_missing_percentage <span class="sc">&lt;</span> <span class="dv">30</span>)  <span class="sc">%&gt;%</span>                   <span class="co"># Filter the data to keep only rows where `total_missing_percentage` is less than 30 %&gt;%</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">na.omit</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="eye-tracking-data-1" class="level3">
<h3 data-anchor-id="eye-tracking-data-1">Eye-tracking data</h3>
<section id="convergence-and-confidence" class="level4">
<h4 data-anchor-id="convergence-and-confidence">Convergence and confidence.</h4>
<p>In the eye-tracking data we need to remove rows with poor convergence and confidence scores in our eye-tracking data. The <code>convergence</code> column refers to WebGazer.js confidence in finding a face (and accurately predicting eye movements). Confidence values vary from 0 to 1, and numbers less than 0.5 suggest that the model has probably converged. <code>face_conf</code> represents the support vector machine (SVM) classifier score for the face model fit. This score indicates how strongly the image under the model resembles a face. Values vary from 0 to 1, and here numbers greater than 0.5 are indicative of a good model fit. In our <code>edat_L2</code> object we filter out convergence less than 0.5 and face confidence greater than 0.5 and save it to <code>edat_1_L2</code></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>edat_1_L2 <span class="ot">&lt;-</span> edat_L2 <span class="sc">%&gt;%</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">filter</span>(convergence <span class="sc">&lt;=</span> .<span class="dv">5</span>, face_conf <span class="sc">&gt;=</span> .<span class="dv">5</span>) <span class="co"># remove poor convergnce and face confidence</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="combining-eye-and-trial-level-data" class="level4">
<h4 data-anchor-id="combining-eye-and-trial-level-data">Combining eye and trial-level data.</h4>
<p>Next, we will combine the eye-tracking data and behavioral data. In this case, we’ll use right_join to add the behavioral data to the eye-tracking data. This ensures that all rows from the eye-tracking data are preserved, even if there isn’t a matching entry in the behavioral data (missing values will be filled with NA). The resulting object is called dat_L2. We use the <code>distinct()</code> function afterward to remove any duplicate rows that may arise during the join</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>dat_L2 <span class="ot">&lt;-</span> <span class="fu">right_join</span>(edat_1_L2,remove_missing,  <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"subject"</span>,<span class="st">"trial"</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>dat_L2 <span class="ot">&lt;-</span> dat_L2 <span class="sc">%&gt;%</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>() <span class="co"># make sure to remove duplicate rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="areas-of-interest" class="level2">
<h2 data-anchor-id="areas-of-interest">Areas of Interest</h2>
<section id="zone-coordinates" class="level3">
<h3 data-anchor-id="zone-coordinates">Zone coordinates</h3>
<p>In the lab, we can control every aspect of the experiment. Online we cant do this. Participants are going to be completing the experiment under a variety of conditions. This includes using different computers, with very different screen dimensions. To control for this, Gorilla outputs standardized zone coordinates (labeled as <code>x_pred_normalised</code> and <code>y_pred_normalised</code> in the eye-tracking file) . As discussed in the Gorilla documentation, the Gorilla lays everything out in a 4:3 frame and makes that frame as big as possible. The normalized coordinates are then expressed relative to this frame; for example, the coordinate 0.5, 0.5 will always be the center of the screen, regardless of the size of the participant’s screen. We used the normalized coordinates in our analysis (in general, you should always use normalized coordinates). However, there are a few different ways to specify the four coordinates of the screen, which I think are worth highlighting here.</p>
<section id="quadrant-approach" class="level4">
<h4 data-anchor-id="quadrant-approach">Quadrant approach.</h4>
<p>One way is to make the AOIs as big as possible, dividing the screen into four quadrants. This approach has been used in several studies (e.g., <span class="citation" data-cites="bramlett2024 prystauka2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>; <a href="#ref-prystauka2024" role="doc-biblioref">Prystauka et al., 2024</a>)</span> ).@tbl-quadcor lists coordinates for the quadrant approach and <a href="#fig-quads" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a> shows how each quadrant looks in standardized space.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="4" data-custom-style="FigureWithoutNote">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "/home/runner/work/L2_VWP_Webcam/L2_VWP_Webcam/_manuscript/Figures/aoi-quad.png"</code></pre>
</div>
<div id="tbl-quadcor" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="4">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-quadcor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;4</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Quandrant coordinates in standardized space</p>
</div>
</figcaption>
<div aria-describedby="tbl-quadcor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/aoi-quad.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>We plot all the fixations in each of the quadrants highlighted in different colors (<a href="#fig-quads" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a>), removing points outside the standardized screen space.</p>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-quads" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="5" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-quads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;5</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>AOI coordiantes in standardized space using the quandrant appraoch</p>
</div>
</figcaption>
<div aria-describedby="fig-quads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-quads-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-fixquads" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="6" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-fixquads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;6</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>All looks in each of the screen quadrants</p>
</div>
</figcaption>
<div aria-describedby="fig-fixquads-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-fixquads-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>We plot all the fixations in each of the quadrants highlighted in different colors (<a href="#fig-quads" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a>), removing points outside the standardized screen space. As a note, we have decided to use an outer edge approach here (eliminating eye fixations that extend beyond the screen coordinates). <span class="citation" data-cites="bramlett2024">Bramlett and Wiener (<a href="#ref-bramlett2024" role="doc-biblioref">2024</a>)</span> have suggested an inner-edge approach and we may add this functionality once more testing is done. For now, we believe that the otter edge approach leads to the least amount of bias in the eye-tracking pipeline.</p>
</div>
<section id="matching-conditions-with-screen-locations" class="level5">
<h5 data-anchor-id="matching-conditions-with-screen-locations">Matching conditions with screen locations.</h5>
<p>The goal of the provided code is to assign condition codes (e.g., Target, Unrelated, Unrelated2, and Cohort) to each image in the dataset based on the screen location where the image is displayed (e.g., TL, TR, BL, BR).</p>
<p>For each trial, the images are dynamically placed at different screen locations, and the code maps each image to its corresponding condition based on these locations.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming your data is in a data frame called dat_L2</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>dat_L2 <span class="ot">&lt;-</span> dat_L2 <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Target =</span> <span class="fu">case_when</span>(</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> TL,</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> TR,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> BL,</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"target"</span> <span class="sc">~</span> BR,</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span>  <span class="co"># Default to NA if no match</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">Unrelated =</span> <span class="fu">case_when</span>(</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> TL,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> TR,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> BL,</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"unrelated1"</span> <span class="sc">~</span> BR,</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">Unrelated2 =</span> <span class="fu">case_when</span>(</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> TL,</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> TR,</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> BL,</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"unrelated2"</span> <span class="sc">~</span> BR,</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">Cohort =</span> <span class="fu">case_when</span>(</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>      typetl <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> TL,</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>      typetr <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> TR,</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>      typebl <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> BL,</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>      typebr <span class="sc">==</span> <span class="st">"cohort"</span> <span class="sc">~</span> BR,</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA_character_</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In addition to tracking the condition of each image during randomized trials, a custom function, <code>find_location()</code>, determines the specific screen location of each image by comparing it against the list of possible locations. This function ensures that the appropriate location is identified or returns NA if no match exists. Specifically, <code>find_location()</code> first checks if the image is NA (missing). If the image is NA, the function returns NA, meaning that there’s no location to find for this image. If the image is not NA, the function creates a vector called loc_names that lists the names of the possible locations. It then attempts to match the given image with the locations. If a match is found, it returns the name of the location (e.g., TL, TR, BL, or BR) of the image.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the function to each of the targ, cohort, rhyme, and unrelated columns</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>dat_colnames_L2 <span class="ot">&lt;-</span> dat_L2 <span class="sc">%&gt;%</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">targ_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(TL, TR, BL, BR), Target),</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">cohort_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(TL, TR, BL, BR), Cohort),</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">unrelated_loc =</span> <span class="fu">find_location</span>(<span class="fu">c</span>(TL, TR, BL, BR), Unrelated), </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">unrealted2_loc=</span> <span class="fu">find_location</span>(<span class="fu">c</span>(TL, TR, BL, BR), Unrelated2), </span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we do this we can use <code>assign_aoi()</code> to loop through our object called <code>dat_colnames_L2</code> and assign locations (i.e., TR, TL, BL, BR) to where participants looked at on the screen. This requires the <code>x</code> and <code>y</code> coordinates and the location of our aois <code>aoi_loc</code>. Here we are using the quadrant approach. This function will label non-looks and off screen coordinates with NA. To make it easier to read we change the numerals assigned by the function to actual screen locations (e.g., TL, TR, BL, BR).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>assign_L2 <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">assign_aoi</span>(dat_colnames_L2,<span class="at">X=</span><span class="st">"x_pred_normalised"</span>, <span class="at">Y=</span><span class="st">"y_pred_normalised"</span>,<span class="at">aoi_loc =</span> aoi_loc)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>AOI_L2 <span class="ot">&lt;-</span> assign_L2 <span class="sc">%&gt;%</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">loc1 =</span> <span class="fu">case_when</span>(</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">1</span> <span class="sc">~</span> <span class="st">"TL"</span>, </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">2</span> <span class="sc">~</span> <span class="st">"TR"</span>, </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">3</span> <span class="sc">~</span> <span class="st">"BL"</span>, </span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    AOI<span class="sc">==</span><span class="dv">4</span> <span class="sc">~</span> <span class="st">"BR"</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <code>AOI_L2</code> we label looks to Targets, Unrelated, and Cohort items with 1 (looked) and 0 (no look).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>AOI_L2<span class="sc">$</span>target <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(AOI_L2<span class="sc">$</span>loc1<span class="sc">==</span>AOI_L2<span class="sc">$</span>targ_loc, <span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># if in coordinates 1, if not 0. </span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>AOI_L2<span class="sc">$</span>unrelated <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(AOI_L2<span class="sc">$</span>loc1 <span class="sc">==</span> AOI_L2<span class="sc">$</span>unrelated_loc, <span class="dv">1</span>, <span class="dv">0</span>)<span class="co"># if in coordinates 1, if not 0. </span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>AOI_L2<span class="sc">$</span>unrelated2 <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(AOI_L2<span class="sc">$</span>loc1 <span class="sc">==</span> AOI_L2<span class="sc">$</span>unrealted2_loc, <span class="dv">1</span>, <span class="dv">0</span>)<span class="co"># if in coordinates 1, if not 0. </span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>AOI_L2<span class="sc">$</span>cohort <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(AOI_L2<span class="sc">$</span>loc1 <span class="sc">==</span> AOI_L2<span class="sc">$</span>cohort_loc, <span class="dv">1</span>, <span class="dv">0</span>)<span class="co"># if in coordinates 1, if not 0. </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The locations of looks need to be “gathered” or pivoted into long format—that is, converted from separate columns into a single column. This transformation makes the data easier to visualize and analyze. We use the <code>pivot_longer()</code> function from the <code>tidyverse</code> to combine the columns (Target, Unrelated, Unrelated2, and Cohort) into a single column called <code>condition1</code>. Additionally, we create another column called <code>Looks</code>, which contains the values from the original columns (e.g., 0 or 1 for whether the area was looked at).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>dat_long_aoi_me_L2 <span class="ot">&lt;-</span> AOI_L2 <span class="sc">%&gt;%</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(subject, trial, condition, target, cohort, unrelated, unrelated2, time, x_pred_normalised, y_pred_normalised, RT_audio) <span class="sc">%&gt;%</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cols =</span> <span class="fu">c</span>(target, unrelated, unrelated2, cohort),</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">names_to =</span> <span class="st">"condition1"</span>,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">values_to =</span> <span class="st">"Looks"</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We further clean up the object by first cleaning up the condition codes. They have a numeral appended to them and that should be removed. We then adjust the timing in the <code>gaze_sub_L2_comp</code> object by aligning time to the actual audio onset. To achieve this, we subtract <code>RT_audio</code> from time for each trial. In addition, we subtract 300 ms from this to account for the 100 ms of silence at the beginning of each audio clip and 200 ms to account for the oculomotor delay when planning an eye movement <span class="citation" data-cites="viviani1990">Viviani (<a href="#ref-viviani1990" role="doc-biblioref">1990</a>)</span>. Additionally, we set our interest period between 0 ms (audio onset) and 2000 ms. This was chosen based on the time course figures in <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> . It is important that you choose your interest area carefully and preferably you preregister it. The interest period you choose can have drastic We also filter out gaze coordinates that fall outside the standardized window, ensuring only valid data points are retained. The resulting object <code>gaze_sub_long_L2</code> provides the corrected time column spanning from -200 ms to 2000 ms relative to stimulus onset with looks outside the screen removed.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># repalce the numbers appended to conditions that somehow got added </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>dat_long_aoi_me_comp <span class="ot">&lt;-</span> dat_long_aoi_me_L2 <span class="sc">%&gt;%</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">condition =</span> <span class="fu">str_replace</span>(condition, <span class="st">"TCUU-SPENG</span><span class="sc">\\</span><span class="st">d*"</span>, <span class="st">"TCUU-SPENG"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">condition =</span> <span class="fu">str_replace</span>(condition, <span class="st">"TCUU-SPSP</span><span class="sc">\\</span><span class="st">d*"</span>, <span class="st">"TCUU-SPSP"</span>))<span class="sc">%&gt;%</span> </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dat_long_aoi_me_comp has condition corrected </span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2_long <span class="ot">&lt;-</span>dat_long_aoi_me_comp<span class="sc">%&gt;%</span> </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="fu">group_by</span>(subject, trial, condition) <span class="sc">%&gt;%</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">time =</span> (time<span class="sc">-</span>RT_audio)<span class="sc">-</span><span class="dv">300</span>) <span class="sc">%&gt;%</span> <span class="co"># subtract audio rt onset and account for occ motor planning and silence in audio</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">filter</span>(time <span class="sc">&gt;=</span> <span class="sc">-</span><span class="dv">200</span>, time <span class="sc">&lt;</span> <span class="dv">2000</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>   dplyr<span class="sc">::</span><span class="fu">filter</span>(x_pred_normalised <span class="sc">&gt;</span> <span class="dv">0</span>,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>                x_pred_normalised <span class="sc">&lt;</span> <span class="dv">1</span>,</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>                y_pred_normalised <span class="sc">&gt;</span> <span class="dv">0</span>,</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>                y_pred_normalised <span class="sc">&lt;</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
</section>
<section id="samples-to-bins" class="level2">
<h2 data-anchor-id="samples-to-bins">Samples to bins</h2>
<section id="downsampling" class="level3">
<h3 data-anchor-id="downsampling">Downsampling</h3>
<p>Downsampling into smaller time bins is a common practice in gaze data analysis, as it helps create a more manageable dataset and reduces noise. When using research grade eye-trackers, downsampling is often not needed. However, with consumer-based webcam eye-tracking it is recommended you downsample your data so participants have consistent bin sizes (e.g., <span class="citation" data-cites="slim2023">(<a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span>). In <code>webgazeR</code> we included the <code>downsample_gaze()</code> function to assist with this process. We apply this function to the <code>gaze_sub_L2_long</code> object,and set the <code>bin.length</code> argument to 100, which groups the data into 100-millisecond intervals. This adjustment means that each bin now represents a 100 ms passage of time. We specify time as the variable to base these bins on, allowing us to focus on broader patterns over time rather than individual millisecond fluctuations.There is no agreed upon downsampling value, but with webcam data larger bins are preferred <span class="citation" data-cites="slim2023">(<a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span>.</p>
<p>In addition, the <code>downsample_gaze()</code> allows you to aggregate across other variables, such as condition, condition1, and use the newly created timebins variable, which represents the time intervals over which we aggregate data. The resulting downsampled dataset, output as <a href="#tbl-agg-sub" class="quarto-xref" aria-expanded="false">Table&nbsp;5</a>, provides a simplified and more concise view of gaze patterns, making it easier to analyze and interpret broader trends.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2 <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">downsample_gaze</span>(gaze_sub_L2_long, <span class="at">bin.length=</span><span class="dv">100</span>, <span class="at">timevar=</span><span class="st">"time"</span>, <span class="at">aggvars=</span><span class="fu">c</span>(<span class="st">"condition"</span>, <span class="st">"condition1"</span>, <span class="st">"time_bin"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="5" data-custom-style="FigureWithoutNote">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "/home/runner/work/L2_VWP_Webcam/L2_VWP_Webcam/_manuscript/Figures/downsample_table.png"</code></pre>
</div>
<div id="tbl-agg-sub" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="5">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-agg-sub-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;5</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Aggregated proportion looks for each condition in each 100 ms time bin</p>
</div>
</figcaption>
<div aria-describedby="tbl-agg-sub-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/downsample_table.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>To simplify the analysis, we combine the two unrelated conditions and average them (this is for the proportional plots).</p>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Average Fix for unrelated and unrelated2, then combine with the rest</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2_avg <span class="ot">&lt;-</span> gaze_sub_L2 <span class="sc">%&gt;%</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(condition, time_bin) <span class="sc">%&gt;%</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">Fix =</span> <span class="fu">mean</span>(Fix[condition1 <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"unrelated"</span>, <span class="st">"unrelated2"</span>)], <span class="at">na.rm =</span> <span class="cn">TRUE</span>),</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">condition1 =</span> <span class="st">"unrelated"</span>,  <span class="co"># Assign the combined label</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">.groups =</span> <span class="st">"drop"</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine with rows that do not include unrelated or unrelated2</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_rows</span>(gaze_sub_L2 <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="sc">!</span>condition1 <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"unrelated"</span>, <span class="st">"unrelated2"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above will not include the subject variable. If you want to keep participant-level data we need to add <code>subject</code> to the <code>aggvars</code> argument.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add subject-level data</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_L2_id <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">downsample_gaze</span>(gaze_sub_L2_long, <span class="at">bin.length=</span><span class="dv">100</span>, <span class="at">timevar=</span><span class="st">"time"</span>, <span class="at">aggvars=</span><span class="fu">c</span>(<span class="st">"subject"</span>, <span class="st">"condition"</span>, <span class="st">"condition1"</span>, <span class="st">"time_bin"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Aggregation is an optional step. If you do not plan to analyze proportion data, and instead what time binned data with binary outcomes preserved please set the <code>aggvars</code> argument to “none.” This will return a time binned column, but will not aggregate over other variables.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get back trial level data with no aggregation</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>gaze_sub_id <span class="ot">&lt;-</span> <span class="fu">downsample_gaze</span>(gaze_sub_L2_long, <span class="at">bin.length=</span><span class="dv">100</span>, <span class="at">timevar=</span><span class="st">"time"</span>, <span class="at">aggvars=</span><span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We need to make sure we only have one unrelated value.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>gaze_sub_id <span class="ot">&lt;-</span> gaze_sub_id <span class="sc">%&gt;%</span> </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">condition1 =</span> <span class="fu">ifelse</span>(condition1<span class="sc">==</span><span class="st">"unrelated2"</span>, <span class="st">"unrelated"</span>, condition1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="visualizing-time-course-data" class="level2">
<h2 data-anchor-id="visualizing-time-course-data">Visualizing time course data</h2>
<p>To simplify plotting your time-course data, we have created the <code>plot_IA_proportions()</code> function. This function takes several arguments. The <code>ia_column</code> argument specifies the column containing your Interest Area (IA) labels. The <code>time_column</code> argument requires the name of your time bin column, and the <code>proportion_column</code> argument specifies the column containing fixation or look proportions. Additional arguments allow you to specify custom names for each IA in the <code>ia_mapping</code> argument, enabling you to label them as desired. In order to use this function, you must use the <code>downsample_gaze()</code> function.</p>
<p>Below we have plotted the time course data in <a href="#fig-L2comp" class="quarto-xref" aria-expanded="false">Figure&nbsp;7</a> for each condition. By default the graphs are using a color-blind friendly palette from the <code>ggokabeito</code> package <span class="citation" data-cites="ggokabeito">(<a href="#ref-ggokabeito" role="doc-biblioref">Barrett, 2021</a>)</span>. Because these are <code>ggplot</code> objects, you can further modify if you choose.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-L2comp" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="7" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-L2comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;7</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Comparison of L2 competition effect in the Spanish-Spanish condition, the Spanish-English condition, and the no competition condition.</p>
</div>
</figcaption>
<div aria-describedby="fig-L2comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-L2comp-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="gorilla-provided-coordinates" class="level2">
<h2 data-anchor-id="gorilla-provided-coordinates">Gorilla provided coordinates</h2>
<p>Thus far, we have used the coordinates representing the four quadrants of the screen. However, Gorilla provides their own quadrants representing image location on the screen. To the authors’ knowledge, these quadrants have not been looked at in any studies reporting eye-tracking results. Let’s examine how reasonable our results are with the Gorilla provided coordinates.</p>
<p>We will use the function <code>extract_aois()</code> to get the standardized coordinates for each quadrant on screen. You can use the <code>zone_names</code> argument to get the zones you want to use. In our example, we want the <code>TL</code>, <code>BR</code>, <code>BL</code> <code>TR</code> coordinates. We input the object from above <code>vwp_paths_filtered_L2</code> that contains all our eye-tracking files and extract the coordinates we want. These are labeled in <a href="#tbl-gorgaze" class="quarto-xref" aria-expanded="false">Table&nbsp;6</a>. In <a href="#fig-gor-L2" class="quarto-xref" aria-expanded="false">Figure&nbsp;8</a> we can see that the AOIs are a bit smaller than then when using the quadrant approach. We can take these coordinates and use them in our analysis.</p>
<p>We are not going to highlight the steps here as they are the same as above. we are just replacing the coordinates.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the extract_aois fucntion</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>aois_L2 <span class="ot">&lt;-</span> <span class="fu">extract_aois</span>(vwp_paths_filtered_L2, <span class="at">zone_names =</span>  <span class="fu">c</span>(<span class="st">"TL"</span>, <span class="st">"BR"</span>, <span class="st">"TR"</span>, <span class="st">"BL"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="6" data-custom-style="FigureWithoutNote">
<div id="tbl-gorgaze" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="6">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-gorgaze-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;6</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Gorilla provided gaze coordinates</p>
</div>
</figcaption>
<div aria-describedby="tbl-gorgaze-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/gorilla_cords.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-gor-L2" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="8" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-gor-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;8</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Gorilla provided standardized coordinates for the four qudrants on the screen</p>
</div>
</figcaption>
<div aria-describedby="fig-gor-L2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-gor-L2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>assign_L2_gor <span class="ot">&lt;-</span> webgazeR<span class="sc">::</span><span class="fu">assign_aoi</span>(dat_colnames_L2,<span class="at">X=</span><span class="st">"x_pred_normalised"</span>, <span class="at">Y=</span><span class="st">"y_pred_normalised"</span>,<span class="at">aoi_loc =</span> aois_L2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualizing-time-course-data-with-gorilla-coordinates" class="level2">
<h2 data-anchor-id="visualizing-time-course-data-with-gorilla-coordinates">Visualizing time course data with Gorilla coordinates</h2>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-L2comp-gor" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="9" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-L2comp-gor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;9</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Comparison of competition effects with Gorilla standardized cooridnates</p>
</div>
</figcaption>
<div aria-describedby="fig-L2comp-gor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-L2comp-gor-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>The Gorilla provided coordinates show a similar pattern to the quadrant approach. However, the time course looks a bit nosier given the smaller AOIs.</p>
</div>
</section>
<section id="modeling-data" class="level2">
<h2 data-anchor-id="modeling-data">Modeling data</h2>
<p>When analyzing VWP data there are many analytic approaches to choose from (e.g., growth curve analysis (GCA), cluster permutation tests (CPT), generalized additive mixed models (GAMMS), logistic multilevel models, divergent point analysis, etc.), and a lot has already been written describing these methods and applying them to visual world fixation data from the lab (see <span class="citation" data-cites="stone2021 ito2023 mcmurray">(<a href="#ref-ito2023" role="doc-biblioref">Ito &amp; Knoeferle, 2023</a>; <a href="#ref-mcmurray" role="doc-biblioref">McMurray &amp; Kutlu, n.d.</a>; <a href="#ref-stone2021" role="doc-biblioref">Stone et al., 2021</a>)</span>) and online <span class="citation" data-cites="bramlett2024">(<a href="#ref-bramlett2024" role="doc-biblioref">Bramlett &amp; Wiener, 2024</a>)</span>. This tutorial’s goal, however, is to not evaluate different analytic approaches and tell readers what they should use. All methods have there strengths and weaknesses <span class="citation" data-cites="ito2023">(see <a href="#ref-ito2023" role="doc-biblioref">Ito &amp; Knoeferle, 2023</a>)</span>. Nevertheless, statistical modeling should be guided by the questions researchers have and thus serious thought needs to be given to the proper analysis. In the VWP, there are two general questions one might be interested in: (1) Are there any overall difference in fixations between conditions and (2) Are there any time course differences in fixations between conditions.</p>
<p>With our data, one question we might want to answer is if there are any fixation differences between the cohort and unrelated conditions across the time course. One statistical approach we chose to highlight to answer this question is a cluster permutation analysis (CPA). The CPA is suitable for testing differences between two conditions or groups over an interest period while controlling for multiple comparisons and autocorrelation.</p>
<section id="cpt" class="level3">
<h3 data-anchor-id="cpt">CPT</h3>
<p>CPA is a technique that has become increasingly popular, particularly in the field of cognitive neuropsychology, for analyzing MEG and EEG data <span class="citation" data-cites="maris2007">(<a href="#ref-maris2007" role="doc-biblioref">Maris &amp; Oostenveld, 2007</a>)</span>. While its adoption in VWP studies has been relatively slow, it is now beginning to appear more frequently <span class="citation" data-cites="ito2023 huang2020">(<a href="#ref-huang2020" role="doc-biblioref">Huang &amp; Snedeker, 2020</a>; <a href="#ref-ito2023" role="doc-biblioref">Ito &amp; Knoeferle, 2023</a>)</span>. Notably, its use is growing in online eye-tracking studies (see <span class="citation" data-cites="slim2023 slim2024 vos2022">(<a href="#ref-slim2024" role="doc-biblioref">Slim et al., 2024</a>; <a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>; <a href="#ref-vos2022" role="doc-biblioref">Vos et al., 2022</a>)</span>).</p>
<p>Before I show you how to apply this method to the current dataset, I want to briefly explain what CPT is. The CPT is a data-driven approach that increases statistical power while controlling for Type I errors across multiple comparisons—exactly what we need when analyzing fixations across the time course.</p>
<p>The clustering procedure involves three main steps:</p>
<p>1. Cluster Formation: With our data, a multilevel logistic models is conducted for every data point (condition by time). Adjacent data points that surpass the mass univariate significance threshold (e.g., p &lt; .05) are combined into clusters. The cluster-level statistic, typically the sum of the t-values (or F-values) within the cluster, is computed. By clustering adjacent significant data points, this step accounts for autocorrelation by considering temporal dependencies rather than treating each data point as independent.</p>
<p>2. Null Distribution Creation: A surrogate null distribution is generated by randomly permuting the conditions within subjects. This randomization is repeated n times (here 1000), and the cluster-level statistic is computed for each permutation. This step addresses multiple comparisons by constructing a distribution of cluster statistics under the null hypothesis, ensuring that family-wise error rates (FWER) are controlled.</p>
<p>3. Significance Testing: The cluster-level statistic from the observed (real) comparison is compared to the null distribution. Clusters with statistics falling in the highest or lowest 2.5% of the null distribution are considered significant (e.g., <em>p</em> &lt; 0.05).</p>
<p>To preform CPT, we will load in the <code>permutaes</code> <span class="citation" data-cites="permutes">(<a href="#ref-permutes" role="doc-biblioref">Voeten, 2023</a>)</span>, <code>permuco</code> <span class="citation" data-cites="permuco">(<a href="#ref-permuco" role="doc-biblioref">Frossard &amp; Renaud, 2021</a>)</span>, and <code>foreach</code> <span class="citation" data-cites="foreach">(<a href="#ref-foreach" role="doc-biblioref"> &amp; Weston, 2022</a>)</span> packages in R so we can use the <code>cluster.glmer</code>() function to run a cluster permutation model with our <code>gaze_sub_id</code> dataset where each row in <code>Looks</code> denotes whether the AOI was fixated, with values of zero (not fixated) or one (fixated).</p>
<p>Below you find sample code to perform multilevel CPA in R (please see the Github repository for further code).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(permutes) <span class="co"># cpa</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(permuco) <span class="co"># cpa</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreach) <span class="co"># for par processing </span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>cpa.lme <span class="ot">=</span> permutes<span class="sc">::</span><span class="fu">clusterperm.glmer</span>(Looks<span class="sc">~</span> condition1_code <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>subject) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>trial), <span class="at">data=</span>gaze_sub_L2_cp1, <span class="at">series.var=</span><span class="sc">~</span>time_bin, <span class="at">nperm =</span> <span class="dv">1000</span>, <span class="at">parallel=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="7" data-custom-style="FigureWithoutNote">
<div id="tbl-clustermass" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="7">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;7</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Clustermass statistics for the Spanish-Spanish condition</p>
</div>
</figcaption>
<div aria-describedby="tbl-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-89ea1756{}.cl-89e4af78{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-89e706a6{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-89e71f06{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-89e71f10{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-89ea1756"><thead><tr style="overflow-wrap:break-word;"><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">cluster</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">cluster_mass</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">p.cluster_mass</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">bin_start</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">bin_end</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">t</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">sign</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">time_start</span></p></th><th class="cl-89e71f06"><p class="cl-89e706a6"><span class="cl-89e4af78">time_end</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">1.00</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">210.03</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">0.00</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">7.00</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">13.00</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">5.14</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">1</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">500.00</span></p></td><td class="cl-89e71f10"><p class="cl-89e706a6"><span class="cl-89e4af78">1,100.00</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>For the Spanish-Spanish condition, we observed one significant cluster from 500-1000 ms (see <a href="#tbl-clustermass" class="quarto-xref" aria-expanded="false">Table&nbsp;7</a>). <a href="#fig-clustermass" class="quarto-xref" aria-expanded="false">Figure&nbsp;10</a> highlights the significant cluster (shaded) for both the Spanish-Spanish and Spanish-English conditions. We see there is one significant cluster in both conditions.</p>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-clustermass" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="10" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;10</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Average looks in the cross-lingustic VWP task over time for the Spanish-Spanish condition (a) and the Spanish-English condition (b). The shaded rectangles indicate when cohort looks were greater than chance baed on the CPA.</p>
</div>
</figcaption>
<div aria-describedby="fig-clustermass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="L2_VWP_webcam_ET_files/figure-html/fig-clustermass-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Webcam eye-tracking is a relatively nascent technology, and as such, there is limited guidance available for researchers. To ameliorate this, we created a tutorial to assist new users of visual world webcam eye-tracking, using some of the best practices available [e.g.,<span class="citation" data-cites="bramlett2024">Bramlett and Wiener (<a href="#ref-bramlett2024" role="doc-biblioref">2024</a>)</span>]. To further facilitate this process, we created the <code>webgazeR</code> package, which contains several helper functions designed to streamline data preprocessing, analysis, and visualization.</p>
<p>In this tutorial, we covered the basic steps of running a visual world webcam-based eye-tracking experiment. We highlighted these steps by using data from a cross-linguistic VWP looking at competitive processes in L2 speakers of Spanish. Specifically, we attempted to replicate the experiment by <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> where they observed within- and between L2/L1 competition using carefully crafted materials.</p>
<p>While the main purpose of this tutorial was to highlight the steps needed to analyze webcam eye-tracking data, replicating <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> allowed us to not only assess whether within and between L2/L1 competition can be found in a spoken word recognition VWP experiment online (one of the first studies to do so), but also provide insight in how to run VWP studies online and the issues associated with it.</p>
<p>Our conceptual replication findings are highly encouraging, demonstrating competition effects both within (Spanish-Spanish condition) and across languages (Spanish-English condition), closely paralleling the results reported by <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span>. However, several important methodological and sample differences warrant discussion.</p>
<p>A key methodological difference between our study and <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> lies in the approach used to analyze the time course of competition. While <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> employed a non-linear curve-fitting method <span class="citation" data-cites="mcmurray2010">(<a href="#ref-mcmurray2010" role="doc-biblioref">McMurray et al., 2010</a>)</span>, we used CPA. This methodological distinction limits our ability to address similar temporal questions. Nonetheless, the overall temporal patterns are strikingly similar. For instance, our CPA revealed a significant cluster starting at 500 ms, whereas <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> identified competition effects emerging at approximately 400 ms. This indicates a delay of about 100 ms in competition onset between lab-based and online eye-tracking data. This delay, while notable, reflects a significant improvement over previous webcam-based studies <span class="citation" data-cites="slim2023 slim2024">(<a href="#ref-slim2024" role="doc-biblioref">Slim et al., 2024</a>; e.g., <a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span>. It is important to emphasize, however, that CPA clusters cannot reliably be used to make temporal inferences about the onset/offset of effects <span class="citation" data-cites="fields2019 ito2023">(<a href="#ref-fields2019" role="doc-biblioref">Fields &amp; Kuperberg, 2019</a>; <a href="#ref-ito2023" role="doc-biblioref">Ito &amp; Knoeferle, 2023</a>)</span>.</p>
<p>Our study also employed a truncated stimulus set, with only 250 trials compared to the 450 trials in <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span>. Despite this reduction, the number of trials in our study remains larger than most existing webcam-based studies. Even with the smaller set, we observed a similar pattern of competition effects in both the Spanish-Spanish and Spanish-English conditions, demonstrating the robustness of our findings.</p>
<p>Another notable difference is the recruitment strategy and participant screening. <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> recruited participants from a Spanish college course and used the LexTALE-Spanish assessment <span class="citation" data-cites="izura2014">(<a href="#ref-izura2014" role="doc-biblioref">Izura et al., 2014</a>)</span> to evaluate Spanish proficiency. In contrast, our data were collected via Prolific with limited filters, which only allowed us to screen for native language and experience with another language. This constraint limited our ability to refine participant selection further and likely contributed to differences in participant profiles. While <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span> focused on adult L2 learners with known language proficiency levels, our sample included a broader range of L2 speakers with limited checks on their language abilities. This may help explain why we did not observe a cohort competition effect that persisted across the time course as reported by <span class="citation" data-cites="sarrett2022">Sarrett et al. (<a href="#ref-sarrett2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>Overall, while the methodological and sample differences between the two studies are notable, the similarities in the competition effects observed within and across languages reinforce the robustness of these findings across different research settings. While we do not wish to downplay our findings, a more systematic study is needed to ensure there generalizability.</p>
<section id="limitations" class="level2">
<h2 data-anchor-id="limitations">Limitations</h2>
<p>While the above suggests that webcam eye-tracking is a promising avenue for language research, there are some issues that we ran into that need to be addressed. One issue is data loss due to poor calibration. In our study, we had to throw out ~40% of our data due to poor calibration. Other studies have shown numbers much higher [e.g., 73%; <span class="citation" data-cites="slim2023">Slim and Hartsuiker (<a href="#ref-slim2023" role="doc-biblioref">2023</a>)</span>] and lower [e.g., 20%; <span class="citation" data-cites="prystauka2024">Prystauka et al. (<a href="#ref-prystauka2024" role="doc-biblioref">2024</a>)</span>]. Given this, it is still an open question as to what contributes to better vs.&nbsp;poor data quality in webcam eye-tracking. To this end, we included an assessment after the VWP that included questions on the participants’ experimental set-ups and overall experiences with the eye-tracking experiment. All questions are included <a href="#tbl-question" class="quarto-xref" aria-expanded="false">Table&nbsp;8</a>.</p>
<section id="poor-vs.-good-calibrators" class="level3">
<h3 data-anchor-id="poor-vs.-good-calibrators">Poor vs.&nbsp;good calibrators</h3>
<p>In our experimental design, participants were branched based on whether they successfully completed the experiment or failed calibration at any point. Table 2 highlights the comparisons between good and poor calibrators. For the sake of brevity, we do not include responses to all questions. You can look at all the responses at our repo. However, two key differences emerge that may provide insight into factors influencing successful calibration.</p>
<p>One notable difference is the type of webcam used. Participants who failed calibration predominantly reported using built-in webcams, whereas those who successfully calibrated reported using a variety of external webcams. This suggests that built-in webcams may not provide sufficient resolution for calibration in the experiment. <span class="citation" data-cites="slim2023">Slim and Hartsuiker (<a href="#ref-slim2023" role="doc-biblioref">2023</a>)</span> performed some correlations looking at calibration score and webcam quality and noticed that high frame rate correlated with a calibration scores.</p>
<p>Another difference lies in the participants’ environmental setup. Individuals who failed calibration were more likely to be in environments with natural light. Since natural light is known to interfere with eye-tracking, it may have contributed to their inability to calibrate successfully.</p>
<p>We did not notice any other differences between those that successful calibrated vs.&nbsp;those who did not. For researchers wanting to use webcam eye-tracking, they should try to make sure participants are in rooms without natural light, and use good web cameras. While we tried to emphasize this in our instructional videos, more explicit instruction may be needed. An avenue for research research would be to compare lab based webcam eye-tracking to online based webcam eye tracking to see if control of the environment can produce better results.</p>
<p>It is important to note here that Gorilla uses WebGazer.js <span class="citation" data-cites="papoutsaki2016">(<a href="#ref-papoutsaki2016" role="doc-biblioref">Papoutsaki et al., 2016</a>)</span> to perform it’s eye tracking. It is unclear if poor calibration results from the noise introduced by participants’ environments/equipments or if it is a function of the method itself, or both. We have listed some equipment and environmental factors that may contribute to the poor performance; however it could be the algorithm itself that is poor. There are other experimental platforms out there that use different eye-tracking ML algorithms to perform webcam eye-tracking (e.g., labvanced; <span class="citation" data-cites="kaduk2024">(<a href="#ref-kaduk2024" role="doc-biblioref">Kaduk et al., 2024</a>)</span>). In labvanced, comapred to Gorilla they use head motion tracking that measures the distance of the participant in front the screen to ensure head movement is restricted to an acceptable range. Together this might make for a better eye-tracking experience with less data thrown out. This should be investigated further.&nbsp;</p>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="8" data-custom-style="FigureWithoutNote">
<div id="tbl-question" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="8">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-question-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;8</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Eye-tracking questionaire items</p>
</div>
</figcaption>
<div aria-describedby="tbl-question-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/Quest.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" prefix="" data-tblnum="9" data-custom-style="FigureWithoutNote">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "/home/runner/work/L2_VWP_Webcam/L2_VWP_Webcam/_manuscript/Figures/poor_good_table.png"</code></pre>
</div>
<div id="tbl-goodbad" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-layout-align="center" data-tblnum="9">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-goodbad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;9</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Responses to eye-tracking questions for participants who successfully calibrated vs.&nbsp;participants who had trouble calibratrin</p>
</div>
</figcaption>
<div aria-describedby="tbl-goodbad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/poor_good_table.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="generalizability-to-other-platforms" class="level3">
<h3 data-anchor-id="generalizability-to-other-platforms"><strong>Generalizability to other platforms</strong></h3>
<p>We demonstrated how to analyze webcam eye-tracking data from a Gorilla experiment using WebGazer.js. While we were unable to validate this pipeline on other experimental platforms using WebGazer.js, such as PCIbex <span class="citation" data-cites="zehr2018penncontroller">(<a href="#ref-zehr2018penncontroller" role="doc-biblioref">Zehr &amp; Schwarz, 2018</a>)</span> or jsPsych <span class="citation" data-cites="deleeuw2015">(<a href="#ref-deleeuw2015" role="doc-biblioref">Leeuw, 2015</a>)</span>, we believe that this basic pipeline will generalize to those platforms, as WebGazer.js underlies them all and provides consistent output. We encourage researchers to test this pipeline in their own studies and report any issues on our GitHub repository. We are committed to continuing improvements to <code>webgazeR</code>, ensuring that users can effectively analyze webcam eye-tracking data with our package.</p>
</section>
<section id="power" class="level3">
<h3 data-anchor-id="power"><strong>Power</strong></h3>
<p>While we successfully demonstrated competition effects similar to Sarrett’s study, we did not conduct an a priori power analysis. With webcam eye-tracking, it has been recommended running twice the number of participants from the original sample, or powering the study to detect an effect size half as large as the original <span class="citation" data-cites="slim2023 simonsohn2015">(<a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>; also see <a href="#ref-simonsohn2015" role="doc-biblioref">Simonsohn, 2015</a>)</span> . We did attempt to increae our sample size 2x, but were unable to recruit enough participants through Prolific. However, our sample size is similar to the lab based studies.</p>
<p>We strongly urge researchers to perform power analyses and justify their sample sizes <span class="citation" data-cites="lakens">(<a href="#ref-lakens" role="doc-biblioref">Lakens, n.d.</a>)</span>. While tools like G*Power <span class="citation" data-cites="faul2007">(<a href="#ref-faul2007" role="doc-biblioref">Faul et al., 2007</a>)</span> are available for this purpose, we recommend power simulations using Monte Carlo or resampling methods on pilot or sample data <span class="citation" data-cites="slim2023 prystauka2024">(<a href="#ref-prystauka2024" role="doc-biblioref">Prystauka et al., 2024</a>; see <a href="#ref-slim2023" role="doc-biblioref">Slim &amp; Hartsuiker, 2023</a>)</span>. Several excellent R packages, such as <code>mixedpower</code> <span class="citation" data-cites="kumle2021">(<a href="#ref-kumle2021" role="doc-biblioref">Kumle et al., 2021</a>)</span> and SIMR <span class="citation" data-cites="green2016">(<a href="#ref-green2016" role="doc-biblioref">Green &amp; MacLeod, 2016</a>)</span> make such simulations straightforward and accessible.</p>
</section>
</section>
<section id="recommendations" class="level2">
<h2 data-anchor-id="recommendations">Recommendations</h2>
<p>Based on our findings and limitations, we propose the following recommendations for researchers conducting visual world webcam eye-tracking experiments.</p>
<ol type="1">
<li><p><strong>Prioritize external webcams<br>
</strong>Our questionnaire suggested that participants using external webcams had significantly better calibration success compared to those relying on built-in webcams. External webcams generally provide higher resolution and frame rates, which are critical for accurate eye-tracking. Researchers should encourage participants to use external webcams whenever possible.</p></li>
<li><p><strong>Optimize environmental conditions<br>
</strong>Natural light was a common factor in environments where calibration failed. Researchers should advise participants to conduct experiments in rooms with controlled lighting—ideally, artificial lighting with minimal glare or shadows—to reduce interference with eye-tracking accuracy.</p></li>
<li><p><strong>Conduct a priori power analysis<br>
</strong>To ensure adequate statistical power, researchers should conduct a priori power analyses either via GUI like GPower or perform Monte Carlo simulations/resampling on pilot data. This step is particularly important for online studies, where sample variability can be higher than in controlled lab environments.</p></li>
<li><p><strong>Collect detailed post-experiment feedback<br>
</strong>Including post-experiment questionnaires about participants’ setups (e.g., webcam type, browser, lighting conditions) can provide valuable insights into calibration success factors. These data can help refine participant instructions and inclusion criteria for future studies.</p></li>
</ol>
<p>By adhering to these recommendations, researchers can enhance the reliability and generalizability of their webcam eye-tracking studies, ensuring the potential of this technology is fully realized.</p>
</section>
<section id="conclusions" class="level2">
<h2 data-anchor-id="conclusions">Conclusions</h2>
<p>This work highlighted the steps required to process webcam eye-tracking data collected via Gorilla, showcasing the potential of webcam-based eye-tracking for robust psycholinguistic experimentation. With a standardized pipeline for processing eye-tracking data we hope we have given researchers a clear path forward when collecting and analyzing visual word webcam eye-tracking data.</p>
<p>Moreover, our findings demonstrate the feasibility of conducting high-quality online experiments, paving the way for future research to address more nuanced questions about L2 processing and language comprehension more broadly. Additionally, further refinement of webcam eye-tracking methodologies could enhance data precision and extend their applicability to more complex experimental designs. This is an exciting time for eye-tracking research, with its boundaries continuously expanding. We eagerly anticipate the advancements and possibilities that the future of webcam eye-tracking will bring.</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-tobii2024" class="csl-entry" role="listitem">
AB, T. (2024). <em>Tobii pro spectrum: Technical specifications</em>. <a href="https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-spectrum">https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-spectrum</a>
</div>
<div id="ref-allopenna1998" class="csl-entry" role="listitem">
Allopenna, P. D., Magnuson, J. S., &amp; Tanenhaus, M. K. (1998). <em>Tracking the time c ourse of spoken word recognition using eye movements: Evidence for c ontinuous mapping models</em> (pp. 419–439).
</div>
<div id="ref-anderson2019" class="csl-entry" role="listitem">
Anderson, C. A., Allen, J. J., Plante, C., Quigley-McBride, A., Lovett, A., &amp; Rokkum, J. N. (2019). The MTurkification of Social and Personality Psychology. <em>Personality &amp; Social Psychology Bulletin</em>, <em>45</em>(6), 842–850. <a href="https://doi.org/10.1177/0146167218798821">https://doi.org/10.1177/0146167218798821</a>
</div>
<div id="ref-anwyl-irvine2020" class="csl-entry" role="listitem">
Anwyl-Irvine, A. L., Massonnié, J., Flitton, A., Kirkham, N., &amp; Evershed, J. K. (2020). Gorilla in our midst: An online behavioral experiment builder. <em>Behavior Research Methods</em>, <em>52</em>(1), 388–407. <a href="https://doi.org/10.3758/s13428-019-01237-x">https://doi.org/10.3758/s13428-019-01237-x</a>
</div>
<div id="ref-ggokabeito" class="csl-entry" role="listitem">
Barrett, M. (2021). <em>Ggokabeito: ’Okabe-ito’ scales for ’ggplot2’ and ’ggraph’</em>. <a href="https://CRAN.R-project.org/package=ggokabeito">https://CRAN.R-project.org/package=ggokabeito</a>
</div>
<div id="ref-blasi2022" class="csl-entry" role="listitem">
Blasi, D. E., Henrich, J., Adamou, E., Kemmerer, D., &amp; Majid, A. (2022). Over-reliance on english hinders cognitive science. <em>Trends in Cognitive Sciences</em>, <em>26</em>(12), 1153–1170. <a href="https://doi.org/10.1016/j.tics.2022.09.015">https://doi.org/10.1016/j.tics.2022.09.015</a>
</div>
<div id="ref-bramlett2024" class="csl-entry" role="listitem">
Bramlett, A. A., &amp; Wiener, S. (2024). The art of wrangling. <em>Linguistic Approaches to Bilingualism</em>. https://doi.org/<a href="https://doi.org/10.1075/lab.23071.bra">https://doi.org/10.1075/lab.23071.bra</a>
</div>
<div id="ref-bylund2024" class="csl-entry" role="listitem">
Bylund, E., Khafif, Z., &amp; Berghoff, R. (2024). Linguistic and geographic diversity in research on second language acquisition and multilingualism: An analysis of selected journals. <em>Applied Linguistics</em>, <em>45</em>(2), 308–329. <a href="https://doi.org/10.1093/applin/amad022">https://doi.org/10.1093/applin/amad022</a>
</div>
<div id="ref-carter2020" class="csl-entry" role="listitem">
Carter, B. T., &amp; Luke, S. G. (2020). Best practices in eye tracking research. <em>International Journal of Psychophysiology</em>, <em>155</em>, 49–62. <a href="https://doi.org/10.1016/j.ijpsycho.2020.05.010">https://doi.org/10.1016/j.ijpsycho.2020.05.010</a>
</div>
<div id="ref-cooper1974" class="csl-entry" role="listitem">
Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. <em>Cognitive Psychology</em>, <em>6</em>(1), 84–107. <a href="https://doi.org/10.1016/0010-0285(74)90005-X">https://doi.org/10.1016/0010-0285(74)90005-X</a>
</div>
<div id="ref-degen2021" class="csl-entry" role="listitem">
Degen, J., Kursat, L., &amp; Leigh, D. D. (2021). Seeing is believing: Testing an explicit linking assumption for visual world eye-tracking in psycholinguistics. <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em>, <em>43</em>.
</div>
<div id="ref-eberhard1995" class="csl-entry" role="listitem">
Eberhard, K. M., Spivey-Knowlton, M. J., Sedivy, J. C., &amp; Tanenhaus, M. K. (1995). Eye movements as a window into real-time spoken language comprehension in natural contexts. <em>Journal of Psycholinguistic Research</em>, <em>24</em>(6), 409–436. <a href="https://doi.org/10.1007/BF02143160">https://doi.org/10.1007/BF02143160</a>
</div>
<div id="ref-faul2007" class="csl-entry" role="listitem">
Faul, F., Erdfelder, E., Lang, A.-G., &amp; Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. <em>Behavior Research Methods</em>, <em>39</em>(2), 175–191. <a href="https://doi.org/10.3758/BF03193146">https://doi.org/10.3758/BF03193146</a>
</div>
<div id="ref-fields2019" class="csl-entry" role="listitem">
Fields, E. C., &amp; Kuperberg, G. R. (2019). Having your cake and eating it too: Flexibility and power with mass univariate statistics for ERP data. <em>Psychophysiology</em>. <a href="https://doi.org/10.1111/psyp.13468">https://doi.org/10.1111/psyp.13468</a>
</div>
<div id="ref-janitor" class="csl-entry" role="listitem">
Firke, S. (2023). <em>Janitor: Simple tools for examining and cleaning dirty data</em>. <a href="https://CRAN.R-project.org/package=janitor">https://CRAN.R-project.org/package=janitor</a>
</div>
<div id="ref-permuco" class="csl-entry" role="listitem">
Frossard, J., &amp; Renaud, O. (2021). <em>Permutation tests for regression, <span></span>ANOVA<span></span>, and comparison of signals: The <span></span>permuco<span></span> package</em>. <em>99</em>. <a href="https://doi.org/10.18637/jss.v099.i15">https://doi.org/10.18637/jss.v099.i15</a>
</div>
<div id="ref-webgazeR" class="csl-entry" role="listitem">
Geller, J., &amp; Prystauka, Y. (2024). <em>webgazeR: Tools for processing webcam eye tracking data</em>. <a href="https://github.com/jgeller112/webgazeR">https://github.com/jgeller112/webgazeR</a>
</div>
<div id="ref-godfroid" class="csl-entry" role="listitem">
Godfroid, A., Finch, B., &amp; Koh, J. (2024). Reporting Eye-Tracking Research in Second Language Acquisition and Bilingualism: A Synthesis and Field-Specific Guidelines. <em>Language Learning</em>, <em>n/a</em>(n/a). <a href="https://doi.org/10.1111/lang.12664">https://doi.org/10.1111/lang.12664</a>
</div>
<div id="ref-gosling2010" class="csl-entry" role="listitem">
Gosling, S. D., Sandy, C. J., John, O. P., &amp; Potter, J. (2010). Wired but not WEIRD: The promise of the Internet in reaching more diverse samples. <em>Behavioral and Brain Sciences</em>, <em>33</em>(2-3), 94–95. <a href="https://doi.org/10.1017/S0140525X10000300">https://doi.org/10.1017/S0140525X10000300</a>
</div>
<div id="ref-green2016" class="csl-entry" role="listitem">
Green, P., &amp; MacLeod, C. J. (2016). SIMR: an R package for power analysis of generalized linear mixed models by simulation. <em>Methods in Ecology and Evolution</em>, <em>7</em>(4), 493–498. <a href="https://doi.org/10.1111/2041-210X.12504">https://doi.org/10.1111/2041-210X.12504</a>
</div>
<div id="ref-huang2020" class="csl-entry" role="listitem">
Huang, Y., &amp; Snedeker, J. (2020). Evidence from the visual world paradigm raises questions about unaccusativity and growth curve analyses. <em>Cognition</em>, <em>200</em>, 104251. <a href="https://doi.org/10.1016/j.cognition.2020.104251">https://doi.org/10.1016/j.cognition.2020.104251</a>
</div>
<div id="ref-ito2023" class="csl-entry" role="listitem">
Ito, A., &amp; Knoeferle, P. (2023). Analysing data from the psycholinguistic visual-world paradigm: Comparison of different analysis methods. <em>Behavior Research Methods</em>, <em>55</em>(7), 3461–3493. <a href="https://doi.org/10.3758/s13428-022-01969-3">https://doi.org/10.3758/s13428-022-01969-3</a>
</div>
<div id="ref-ito2018" class="csl-entry" role="listitem">
Ito, A., Pickering, M. J., &amp; Corley, M. (2018). Investigating the time-course of phonological prediction in native and non-native speakers of english: A visual world eye-tracking study. <em>Journal of Memory and Language</em>, <em>98</em>, 1–11. <a href="https://doi.org/10.1016/j.jml.2017.09.002">https://doi.org/10.1016/j.jml.2017.09.002</a>
</div>
<div id="ref-izura2014" class="csl-entry" role="listitem">
Izura, C., Cuetos, F., &amp; Brysbaert, M. (2014). Lextale-Esp: a test to rapidly and efficiently assess the Spanish vocabulary size. <em>PSICOLOGICA</em>, <em>35</em>(1), 49–66. <a href="http://hdl.handle.net/1854/LU-5774107">http://hdl.handle.net/1854/LU-5774107</a>
</div>
<div id="ref-kaduk2024" class="csl-entry" role="listitem">
Kaduk, T., Goeke, C., Finger, H., &amp; König, P. (2024). Webcam eye tracking close to laboratory standards: Comparing a new webcam-based system and the EyeLink 1000. <em>Behavior Research Methods</em>, <em>56</em>(5), 5002–5022. <a href="https://doi.org/10.3758/s13428-023-02237-8">https://doi.org/10.3758/s13428-023-02237-8</a>
</div>
<div id="ref-kumle2021" class="csl-entry" role="listitem">
Kumle, L., Võ, M. L.-H., &amp; Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: An open introduction and tutorial in R. <em>Behavior Research Methods</em>, <em>53</em>(6), 2528–2543. <a href="https://doi.org/10.3758/s13428-021-01546-0">https://doi.org/10.3758/s13428-021-01546-0</a>
</div>
<div id="ref-lakens" class="csl-entry" role="listitem">
Lakens, D. (n.d.). <em>Sample Size Justification</em>. <a href="https://online.ucpress.edu/collabra/article/8/1/33267/120491/Sample-Size-Justification">https://online.ucpress.edu/collabra/article/8/1/33267/120491/Sample-Size-Justification</a>
</div>
<div id="ref-deleeuw2015" class="csl-entry" role="listitem">
Leeuw, J. R. de. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. <em>Behavior Research Methods</em>, <em>47</em>(1), 1–12. <a href="https://doi.org/10.3758/s13428-014-0458-y">https://doi.org/10.3758/s13428-014-0458-y</a>
</div>
<div id="ref-magnuson2007" class="csl-entry" role="listitem">
Magnuson, J. S., Dixon, J. A., Tanenhaus, M. K., &amp; Aslin, R. N. (2007). The Dynamics of Lexical Competition During Spoken Word Recognition. <em>Cognitive Science</em>, <em>31</em>(1), 133–156. <a href="https://doi.org/10.1080/03640210709336987">https://doi.org/10.1080/03640210709336987</a>
</div>
<div id="ref-maris2007" class="csl-entry" role="listitem">
Maris, E., &amp; Oostenveld, R. (2007). Nonparametric statistical testing of EEG- and MEG-data. <em>Journal of Neuroscience Methods</em>, <em>164</em>(1), 177–190. <a href="https://doi.org/10.1016/j.jneumeth.2007.03.024">https://doi.org/10.1016/j.jneumeth.2007.03.024</a>
</div>
<div id="ref-mcmurray" class="csl-entry" role="listitem">
McMurray, B., &amp; Kutlu, E. (n.d.). <em>From real-time measures to real world differences new [and old] statistical approaches to individual differences in real-time language processing</em>. <a href="https://doi.org/10.31234/osf.io/2c5b6">https://doi.org/10.31234/osf.io/2c5b6</a>
</div>
<div id="ref-mcmurray2010" class="csl-entry" role="listitem">
McMurray, B., Samelson, V. M., Lee, S. H., &amp; Tomblin, J. B. (2010). Individual differences in online spoken word recognition: Implications for SLI. <em>Cognitive Psychology</em>, <em>60</em>(1), 1–39. <a href="https://doi.org/10.1016/j.cogpsych.2009.06.003">https://doi.org/10.1016/j.cogpsych.2009.06.003</a>
</div>
<div id="ref-foreach" class="csl-entry" role="listitem">
Microsoft, &amp; Weston, S. (2022). <em>Foreach: Provides foreach looping construct</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>
</div>
<div id="ref-miller2023" class="csl-entry" role="listitem">
Miller, J. (2023). Outlier exclusion procedures for reaction time analysis: The cures are generally worse than the disease. <em>Journal of Experimental Psychology: General</em>, <em>152</em>(11), 3189–3217. <a href="https://doi.org/10.1037/xge0001450">https://doi.org/10.1037/xge0001450</a>
</div>
<div id="ref-mirman2012" class="csl-entry" role="listitem">
Mirman, D., &amp; Graziano, K. M. (2012). Individual differences in the strength of taxonomic versus thematic relations. <em>Journal of Experimental Psychology: General</em>, <em>141</em>(4), 601–609. <a href="https://doi.org/10.1037/a0026451">https://doi.org/10.1037/a0026451</a>
</div>
<div id="ref-nyström2021" class="csl-entry" role="listitem">
Nyström, M., Niehorster, D. C., Andersson, R., &amp; Hooge, I. (2021). The Tobii Pro Spectrum: A useful tool for studying microsaccades? <em>Behavior Research Methods</em>, <em>53</em>(1), 335–353. <a href="https://doi.org/10.3758/s13428-020-01430-3">https://doi.org/10.3758/s13428-020-01430-3</a>
</div>
<div id="ref-papoutsaki2016" class="csl-entry" role="listitem">
Papoutsaki, A., Sangkloy, P., Laskey, J., Daskalova, N., Huang, J., &amp; Hays, J. (2016). <em>Webgazer: Scalable webcam eye tracking using user interactions</em>. 38393845.
</div>
<div id="ref-peirce2019" class="csl-entry" role="listitem">
Peirce, J., Gray, J. R., Simpson, S., MacAskill, M., Höchenberger, R., Sogo, H., Kastman, E., &amp; Lindeløv, J. K. (2019). PsychoPy2: Experiments in behavior made easy. <em>Behavior Research Methods</em>, <em>51</em>(1), 195–203. <a href="https://doi.org/10.3758/s13428-018-01193-y">https://doi.org/10.3758/s13428-018-01193-y</a>
</div>
<div id="ref-pluzyczka2018" class="csl-entry" role="listitem">
Płużyczka, M. (2018). The First Hundred Years: a History of Eye Tracking as a Research Method. <em>Applied Linguistics Papers</em>, <em>25/4</em>, 101–116. <a href="http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-98576d43-39e3-4981-8c1c-717962cf29da">http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-98576d43-39e3-4981-8c1c-717962cf29da</a>
</div>
<div id="ref-prystauka2024" class="csl-entry" role="listitem">
Prystauka, Y., Altmann, G. T. M., &amp; Rothman, J. (2024). Online eye tracking and real-time sentence processing: On opportunities and efficacy for capturing psycholinguistic effects of different magnitudes and diversity. <em>Behavior Research Methods</em>, <em>56</em>(4), 3504–3522. <a href="https://doi.org/10.3758/s13428-023-02176-4">https://doi.org/10.3758/s13428-023-02176-4</a>
</div>
<div id="ref-rodd2024" class="csl-entry" role="listitem">
Rodd, J. M. (2024). Moving experimental psychology online: How to obtain high quality data when we can<span>’</span>t see our participants. <em>Journal of Memory and Language</em>, <em>134</em>, 104472. <a href="https://doi.org/10.1016/j.jml.2023.104472">https://doi.org/10.1016/j.jml.2023.104472</a>
</div>
<div id="ref-sarrett2022" class="csl-entry" role="listitem">
Sarrett, M. E., Shea, C., &amp; McMurray, B. (2022). Within- and between-language competition in adult second language learners: Implications for language proficiency. <em>Language, Cognition and Neuroscience</em>, <em>37</em>(2), 165–181. <a href="https://doi.org/10.1080/23273798.2021.1952283">https://doi.org/10.1080/23273798.2021.1952283</a>
</div>
<div id="ref-semmelmann2018" class="csl-entry" role="listitem">
Semmelmann, K., &amp; Weigelt, S. (2018). Online webcam-based eye tracking in cognitive science: A first look. <em>Behavior Research Methods</em>, <em>50</em>(2), 451–465. <a href="https://doi.org/10.3758/s13428-017-0913-7">https://doi.org/10.3758/s13428-017-0913-7</a>
</div>
<div id="ref-simonsohn2015" class="csl-entry" role="listitem">
Simonsohn, U. (2015). Small telescopes. <em>Psychological Science</em>, <em>26</em>(5), 559–569. <a href="https://doi.org/10.1177/0956797614567341">https://doi.org/10.1177/0956797614567341</a>
</div>
<div id="ref-slim2023" class="csl-entry" role="listitem">
Slim, M. S., &amp; Hartsuiker, R. J. (2023). Moving visual world experiments online? A web-based replication of Dijkgraaf, Hartsuiker, and Duyck (2017) using PCIbex and WebGazer.js. <em>Behavior Research Methods</em>, <em>55</em>(7), 3786–3804. <a href="https://doi.org/10.3758/s13428-022-01989-z">https://doi.org/10.3758/s13428-022-01989-z</a>
</div>
<div id="ref-slim2024" class="csl-entry" role="listitem">
Slim, M. S., Kandel, M., Yacovone, A., &amp; Snedeker, J. (2024). Webcams as windows to the mind? A direct comparison between in-lab and web-based eye-tracking methods. <em>Open Mind</em>, <em>8</em>, 1369–1424. <a href="https://doi.org/10.1162/opmi_a_00171">https://doi.org/10.1162/opmi_a_00171</a>
</div>
<div id="ref-stone2021" class="csl-entry" role="listitem">
Stone, K., Lago, S., &amp; Schad, D. J. (2021). Divergence point analyses of visual world data: applications to bilingual research. <em>Bilingualism: Language and Cognition</em>, <em>24</em>(5), 833–841. <a href="https://doi.org/10.1017/S1366728920000607">https://doi.org/10.1017/S1366728920000607</a>
</div>
<div id="ref-tanenhaus1995" class="csl-entry" role="listitem">
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. <em>Science (New York, N.Y.)</em>, <em>268</em>(5217), 1632–1634. <a href="http://www.ncbi.nlm.nih.gov/pubmed/7777863">http://www.ncbi.nlm.nih.gov/pubmed/7777863</a>
</div>
<div id="ref-trueswell2008" class="csl-entry" role="listitem">
Trueswell, J. C. (2008). <em>Using eye movements as a developmental measure within psycholinguistics</em> (I. A. Sekerina, E. M. Fernández, &amp; H. Clahsen, Eds.; pp. 73–96). John Benjamins Publishing Company. <a href="https://doi.org/10.1075/lald.44.05tru">https://doi.org/10.1075/lald.44.05tru</a>
</div>
<div id="ref-viviani1990" class="csl-entry" role="listitem">
Viviani, P. (1990). Eye movements in visual search: cognitive, perceptual and motor control aspects. <em>Reviews of Oculomotor Research</em>, <em>4</em>, 353–393.
</div>
<div id="ref-permutes" class="csl-entry" role="listitem">
Voeten, C. C. (2023). <em>Permutes: Permutation tests for time series data</em>. <a href="https://CRAN.R-project.org/package=permutes">https://CRAN.R-project.org/package=permutes</a>
</div>
<div id="ref-vos2022" class="csl-entry" role="listitem">
Vos, M., Minor, S., &amp; Ramchand, G. C. (2022). Comparing infrared and webcam eye tracking in the Visual World Paradigm. <em>Glossa Psycholinguistics</em>, <em>1</em>(1). <a href="https://doi.org/10.5070/G6011131">https://doi.org/10.5070/G6011131</a>
</div>
<div id="ref-woods2017" class="csl-entry" role="listitem">
Woods, K. J. P., Siegel, M. H., Traer, J., &amp; McDermott, J. H. (2017). Headphone screening to facilitate web-based auditory experiments. <em>Attention, Perception, and Psychophysics</em>, <em>79</em>(7), 2064–2072. <a href="https://doi.org/10.3758/s13414-017-1361-2">https://doi.org/10.3758/s13414-017-1361-2</a>
</div>
<div id="ref-zehr2018penncontroller" class="csl-entry" role="listitem">
Zehr, J., &amp; Schwarz, F. (2018). <em>PennController for internet based experiments (IBEX)</em>. <a href="https://doi.org/10.17605/OSF.IO/MD832">https://doi.org/10.17605/OSF.IO/MD832</a>
</div>
</div>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Studies showing this delay used IPCbex<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>