---
title: "Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research"
# If blank, the running header is the title in 
shorttitle: "VWP Webcam Tutorial"
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Jason Geller
    corresponding: true
    orcid: 0000-0002-7459-4505
    email: jason.geller@bc.edu
    url: www.drjasongeller.com
    # Roles are optional. 
    # Select from the CRediT: Contributor Roles Taxonomy https://credit.niso.org/
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    roles:
      - conceptualization
      - writing
      - data curation
      - editing
      - software
      - formal analysis
    affiliations:
      - id: id1
        name: "Boston College"
        department: Department of Psychology and Neuroscience
        address: Mcguinn Hall 405
        city: Chestnut Hill
        region: MA
        country: USA
        postal-code: 02467-9991

  - name: Yanina Prystauka
    orcid: 0000-0001-8258-2339
    roles:
      - methodology
      - editing
      - formal analysis
    affiliations: 
      - id: id2
        name: "University of Bergen"
        department: Department of Linguistic, Literary and Aesthetic Studies
  - name: Sarah E. Colby
    orcid: 0000-0002-2956-3072
    roles: 
      - methodology
      - editing
    affiliations:
     - id: id3
       name: "University of Ottawa"
       department: Department of Linguistics
  - name: Julia R. Drouin
    orcid: 0000-0003-0798-3268
    roles: 
      - methodology
      - conceptualziation
      - editing
      - funding acquisition
    affiliations:
     - id: id4
       name: "University of North Carolina at Chapel Hill"
       department: Division of Speech and Hearing Sciences
author-note:
  status-changes: 
    # Example: [Author name] is now at [affiliation].
    affiliation-change: ~
    # Example: [Author name] is deceased.
    deceased: ~
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: This study was not preregistered. 
    # Acknowledge and cite data/materials to be shared.
    data-sharing: The data and code for this manuscript can be found at  https://github.com/jgeller112/L2_VWP_Webcam. 
    # Example: This article is based on data published in [Reference].
    # Example: This article is based on the dissertation completed by [citation].  
    related-report: ~
    # Example: [Author name] has been a paid consultant for Corporation X, which funded this study.
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    # Example: This study was supported by Grant [Grant Number] from [Funding Source].
    financial-support: This work was supported by research start-up funds to JRD.

    authorship-agreements: ~
abstract: "Eye-tracking has become a valuable tool for studying cognitive processes in second language (L2) acquisition and bilingualism (Godfroid et al., 2024). While research-grade infrared eye-trackers are commonly used, there are a number of issues that limit its wide-spread adoption. Recently, consumer-based webcam eye-tracking has emerged as an attractive alternative, requiring only internet access and a personal webcam. However, webcam eye-tracking presents unique design and preprocessing challenges that must be addressed for valid results. To help researchers overcome these challenges, we developed a comprehensive tutorial focused on visual world  webcam eye-tracking for L2 language research. Our guide will cover all key steps, from design to data preprocessing and analysis, where we highlight the R package `webgazeR`, which is open source and freely available for download and installation: https://github.com/jgeller112/webgazeR. We offer best practices for environmental conditions, participant instructions, and tips for designing visual world experiments with webcam eye-tracking. To demonstrate these steps, we analyze data collected through the Gorilla platform (Anwyl-Irvine et al., 2020) using a single word Spanish visual world paradigm (VWP) and show competition within and between L2/L1. This tutorial aims to empower researchers by providing a step-by-step guide to successfully conduct visual world  webcam-based eye-tracking studies. To follow along with this tutorial, please download the entire manuscript and its accompanying code with data from here: https://github.com/jgeller112/L2_VWP_Webcam."

# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [VWP, Tutorial, Webcam eye-tracking, R, Gorilla, Spoken word recognition, L2 processing]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: true
# File with references
bibliography: references.bib
# Suppress title page
suppress-title-page: false
# Link citations to references
link-citations: false
# Masks references that appear in the masked-citations list
mask: false
masked-citations:
# If true, adds today's date below author affiliations. If text, can be any value.
# This is not standard APA format, but it is convenient.
# Works with docx, html, and typst. 
draft-date: false
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "drjasongeller@gmail.com"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format: 
  apaquarto-pdf: 
    documentmode: doc
    include-in-header:
      - text: |
          \usepackage{fvextra} % Advanced verbatim environment for better wrapping
          \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
          \renewcommand{\baselinestretch}{1.2} % Adjust line spacing for readability
          \setlength{\parskip}{0.5em} % Paragraph spacing for better readability
          \usepackage{geometry} % Control margins
          \geometry{margin=1in} % Ensure text does not overflow page
          \usepackage{setspace} % Allows flexible line spacing
          \setstretch{1.2} % Slightly stretch lines for better readability
execute: 
  echo: true
  warning: false
  message: false
  fig-align: center
  tbl-align: center
  keep-with-next: true
  code-overflow: wrap
  out-width: 50%
  out-height: 50%
filters:
  - highlight-text
---

Eye-tracking technology, which has a history spanning over a century, has seen remarkable advancements. In the early days, eye-tracking sometimes required the use of contact lenses fitted with search coils, often requiring anesthesia, or the attachment of suction cups to the sclera of the eyes [@pluzyczka2018]. These methods were not only cumbersome for the researcher, but also uncomfortable and invasive for participants. Over time, such approaches have been replaced by non-invasive, lightweight, and user-friendly systems. Today, modern eye-tracking technology is widely accessible in laboratories worldwide, enabling researchers to tackle critical questions about cognitive processes . This evolution has had a profound impact on fields such as psycholinguistics and bilingualism, opening up new possibilities for understanding how language is processed in real time [@godfroid].

[In the last decade, there has been a gradual shift toward conducting more behavioral experiments online [@anderson2019; @rodd2024]. This “onlineification” of behavioral research has driven the development of remote eye-tracking methods that do not rely on traditional laboratory settings. Allowing participants to use their own equipment from anywhere in the world opens the door to recruiting more diverse and historically underrepresented populations [@gosling2010]. Behavioral science research has long struggled with a lack of diversity, relying heavily on participants who are predominantly Western, Educated, Industrialized, Rich, Democratic, and able-bodied (WEIRD-A) [@henrich2010]. This reliance often excludes individuals from geographically dispersed regions, lower socioeconomic backgrounds, and people with disabilities who may face barriers to accessing research facilities. In language research, this issue is especially pronounced, as studies often focus on “modal” listeners and speakers—typically young, monolingual, and neurotypical individuals [@blasi2022; @bylund2024; @mcmurray2010].] {bg-colour="#b22222"}

One online method that is increasing in popularity and hopefully mitigate these limitations by increasing the inclusivity and representativeness of participant samples is automated eye-tracking or webcam eye-tracking. Webcam eye-tracking typically requires three things: 1. A personal computer. 2. An internet connection and 3. A purchased or pre-installed webcamera. Gaze information can be collected via a web browser. One common method to perform webcam eye-tracking is through an open source, free, and actively maintained JavaScript library plugin called WebGazer.js [@papoutsaki2016]. [^1]This plugin is already incorporated into several popular experimental platforms (e.g., Gorilla, *jsPsych*, PsychoPy, Labvanced, PCIbex; [@anwyl-irvine2020; @peirce2019; @deleeuw2015; @zehr2018penncontroller; @kaduk2024]. WebGazer.js runs locally through a person's personal computer via a browser and does not require users to download any software, making it extremely easy to start webcam eye-tracking. In addition, videos taken from webcams are not recorded and saved which eliminates some of the ethical and privacy concerns.

[^1]: It is important to note that WebGazer.js is not the only method available. Other methods have been implemented by companies like Tobii (www.tobii.com) and Labvanced [@kaduk2024] . However, because these methods are proprietary, they are less accessible and difficult to reproduce.

While research-grade eye-trackers commonly employ video-based recording and rely on one or more cameras and the pupil-corneal reflection (P-CR) method [@carter2020], WebGazer.js utilizes facial feature detection to estimate gaze positions in real time through a webcam. At each time point, determined by the sampling rate, x and y coordinates of the gaze are recorded. The system employs machine learning to analyze the relative movement of the eyes and infer the gaze location on the screen. To enhance accuracy, calibration and validation procedures are implemented, during which participants fixate on markers with known positions on the screen.

This leads to an important question: how does consumer-grade webcam eye tracking compare to research-grade systems? A study of 15 research-grade eye-trackers by @hooge2024 found that precision ranged from 0.1° to 0.35°, while accuracy ranged from 0.3° to 0.75°. Additionally, research-grade eye-trackers have low latency and can achieve high sampling rates—for example, the SR EyeLink 1000 Plus can sample at 2,000 Hz. These advanced capabilities make research-grade systems ideal for studies requiring high temporal and spatial resolution.

While validation studies are ongoing, webcam-based eye trackers generally exhibit reduced spatiotemporal accuracy. Studies have reported that these systems achieve spatial accuracy and precision exceeding 1° of visual angle, with latencies ranging from 200 ms to 1000 ms [@semmelmann2018; @slim2023; @slim2024; @kaduk2024]. Furthermore, the sampling rate of webcam-based systems is much lower, typically capped at 60 Hz, with most studies reporting average or median rates around 30 Hz [@prystauka2024; @bramlett2024]. Unlike research-grade systems, webcam eye trackers do not use infrared light; instead, they rely on ambient light from the participant’s environment. This dependency introduces additional variability in tracking performance. As a result of this, some [e.g., @slim2024] have shown reduced effect sizes when comparing webcam eye-tracking to research-grade trackers on the exact same task.

## Bringing the visual world paradigm (VWP) online

In language research, few methods have had as enduring an impact as the Visual World Paradigm (VWP) [@tanenhaus1995; cf. @cooper1974]. For the past 25 years, the VWP has helped researchers tackle a wide range of topics, including sentence processing [@altmann1999; @huettig2011; @kamide2003], word recognition [@allopenna1998; @dahan2001; @huettig2007; @mcmurray2002], bilingualism [@hopp2013; @ito2018; @rossi2019], and the effects of brain damage on language [@mirman2012; @yee2008].

What makes the widespread use of the VWP even more remarkable is the simplicity of the task. In a typical VWP experiment, participants view a display containing several objects (typically in the form of pictures) and are asked to select one—either by pointing, clicking, or simply looking—\[\@altmannIncrementalInterpretationVerbs1999\]. As they listen to a spoken word or phrase, or in some cases read a word \[\@huettigTugWarPhonological2007\], their eye movements are recorded in real time. A robust finding in VWP research is that listeners reliably direct their gaze to the picture representing the spoken word, often *before* the word has been fully articulated, revealing anticipatory or predictive processing. Eye movements align closely—both in content and timing—with the mental activation of the corresponding word or concept. This offers a uniquely detailed window into how cognitive and linguistic processes unfold moment-to-moment.

Most research on visual world eye-tracking has been conducted in laboratory settings using research-grade eye-trackers. However, several attempts have been made to conduct these experiments online using webcam-based eye-tracking. Most online VWP replications have focused on sentence-based language processing. These studies have looked at effects of set size and determiners [@degen2021], verb semantic constraint [@prystauka2024; @slim2023], grammatical aspect and event comprehension [@vos2022], and lexical interference [@prystauka2024].

More relevant to the current paper are findings from single-word VWP studies conducted online. To date, only one study has investigated visual world webcam eye-tracking with single words. @slim2024 examined a phonemic cohort task. In the cohort task, pictures were displayed randomly in one of four quadrants, and participants were instructed to fixate on the target based on the auditory cue. On each trial, one of the pictures was phonemically similar to the target in onset (e.g., *MILK* – *MITTEN*). @slim2024 were able to observe significant fixations to the cohort compared to the control condition, replicating lab-based single word VWP experiments with research grade eye-trackers [e.g., @allopenna1998]. However, @slim2024 only observed these competition effects in a later time window compared to traditional, lab-based eye-tracking.

It is important to note, however, that while these studies represent successful replication attempts, there a few caveats. Most notably, some studies [e.g., @degen2021; @slim2023; @slim2024] reported considerable delays in the temporal onset of effects. Several factors likely contribute to these delays, including reduced spatial precision, computational demands induced by the webgazer.JS algorithm, the size of areas of interest (AOIs), and the number of calibrations performed [@degen2021; @slim2024].

More recent work has addressed these limitations by utilizing an updated version of WebGazer.js and using different experimental platforms. For instance, @vos2022 demonstrated a significant reduction in delays—approximately 50 ms—when comparing lab-based and online versions of the VWP using an updated version of WebGazer.js within the jsPsych framework [@deleeuw2015]. Furthermore, studies by @prystauka2024 and @bramlett2024, which leveraged the Gorilla platform alongside the improved WebGazer algorithm, reported effects comparable to those observed in traditional lab-based VWP studies.

These findings underscore the potential of the online version of the VWP, powered by webcam eye-tracking, to achieve results similar to those of traditional lab-based methods. Importantly, they demonstrate that this approach can effectively be used to study competition effects in single-word speech perception.

## Tutorial

Taken together, it seems that webcam eye-tracking is a viable alternative to lab-based eye-tracking. Given this, we aimed to support researchers in their efforts to conduct high-quality webcam eye-tracking studies with the VWP. While a valuable tutorial on webcam eye-tracking in the VWP already exists [@bramlett2024], we believe there is value in having multiple resources available to researchers. To this end, we sought to expand on the tutorial by @bramlett2024 by incorporating many of their useful recommendations, but also offering an R package to help streamline data pre-processing.

The purpose of this tutorial is to provide an overview of the basic set-up and design features of an online VWP task using the Gorilla platform [@anwyl-irvine2020] and to highlight the pre-processing steps needed to analyze webcam eye-tracking data. Here we use the popular open source programming language R and introduce the `webgazeR` package [@webgazeR] to facilitate pre-processing of webcam data. To highlight the steps needed to process webcam eye-tracking data we present data from a Spanish spoken word VWP with L2 Spanish speakers. To our knowledge, L2 processing and competitor effects have not been looked at in the online version of the VWP.

The structure of the tutorial will be as follows. We first outline the general methods used to conduct a visual world webcam eye-tracking experiment. Next, we detail the data preprocessing steps required to prepare the data for analysis. Finally, we demonstrate one statistical approach for analyzing our preprocessed data, highlighting its application and implications.

To promote transparency and reproducibility, this tutorial was written in R [@R] using Quarto [@Allaire_Quarto_2024], an open-source publishing system that allows for dynamic and static documents. This allows figures, tables, and text to be programmatically included directly in the manuscript, ensuring that all results are seamlessly integrated into the document.To increase computational reproducibility we use the rix [@rix] package which harnesses the power of the nix [@nix] ecosystem to help with computational reproducibility. Not only does this give us a snapshot of the packages used to create the current manuscript, but it also takes a snapshot of system dependencies used at run-time. This way reproducers can easily re-use the exact same environment by installing the nix package manager and using the included default.nix file to set up the right environment. The README file in the GitHub repository contains detailed information on how to set this up to reproduce the contents of the current manuscript. We have also included a video tutorial.

# L2 VWP Webcam Eye-tracking

To highlight the preprocessing steps required to analyze webcam eye-tracking data, we examined the competitive dynamics of second-language (L2) learners of Spanish, whose first language is English, during spoken word recognition. Specifically, we investigated both within-language and cross-language (L2/L1) competition using webcam-based eye-tracking.

It is well established that competition plays a critical role in language processing [@magnuson2007]. In speech perception, as the auditory signal unfolds over time, competitors (or cohorts)—phonological neighbors that differ from the target by an initial phoneme—become activated. To successfully recognize the spoken word, these competitors must be inhibited or suppressed. For example, as the word *wizard* is spoken, cohorts like *whistle* might also be briefly activated and in order for wizard to be recognized, *whistle* must be suppressed. A key question in the L2 literature is whether competition can occur cross-linguistically, with interactions between a speaker’s first language (L1) and second language (L2). Recent work by @sarrett2022 explored this question using carefully designed stimuli to examine within- and between linguistic (L2/L1) competition in adult L2 Spanish learners using a Spanish VWP. Their study included two key conditions:

1.  Spanish-Spanish (within) condition: A Spanish competitor was presented alongside the target word. For example, if the target word spoken was *cielo* (sky), the Spanish competitor was *ciencia* (science).

2.  Spanish-English (cross-ligustic) condition: An English competitor was presented for the Spanish target word. For example, if the target word spoken was *botas* (boots), the English competitor was *border*.

@sarrett2022 also included a no competition condition where the Spanish-English pairs were not cross-linguistic competitors (e.g., *frontera* as the target word and *botas* - *boots* as an unrelated item in the pair). They observed competition effects in both of the critical conditions: within (e.g., *cielo* - *ciencia*) and between (e.g., *botas* - *border*). For this tutorial, we collected data to conceptually replicate their pattern of findings.

There are two key differences between our dataset and the original study by @sarrett2022 worth noting. First, @sarrett2022 focused on adult L2 Spanish speakers and posed more fine-grained questions about the time course of competition and resolution and its relationship with L2 language acquisition. Second, unlike @sarrett2022 , who measured Spanish proficiency objectively using LexTALE-esp [@izura2014]), we relied on Prolific’s filters to recruit L2 Spanish speakers.

Our primary goal here was to demonstrate the pre-processing steps required to analyze webcam-based eye-tracking data. A secondary goal was to provide evidence of L2 competition within and between or cross-linguistically using this methodology. To our knowledge, no papers have looked at spoken word recognition and competition using online methods. It is our hope that researchers can use this to test more detailed questions about L2 processing using webcam-based eye-tracking.\]