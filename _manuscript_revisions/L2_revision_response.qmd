---
title: Responses to editor and reviewer concerns
format:
  hikmah-response-typst: default
bibliography: references.bib
---

Thank you for giving us the opportunity to revise and resubmit our manuscript, "Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research" (Manuscript ID `RMAL-D-25-00055`). We found the Editor's and the Reviewers' comments to be very insightful and helpful and we have used these comments to rewrite and improve the manuscript in a number of ways. I would like to say that I really appreciate the time Reviewer 1 took to provide constructive comments and edits, including providing code. In this memo, we describe how we have revised the paper in response to these comments. The Editor's and Reviewers' comments are cited or summarized in [red italicized text]{.memo-reviewer-inline}, and our responses are in black Roman text. We include excerpts from our revised manuscript in [blue]{.memo-excerpt-inline}.

We have made a several changes to the webgazeR package to make it robust and flexible to different types of data. It is not perfect, but we hope users will post issues so we can make the package better. 

# Reviewer 1

## Borader concerns

The authors noted overlap in the wording of some of the methods discussed in the paper.

::: memo-reviewer
***Reviewer 1***: There are some places of plagiarism. We do not think this was intentional, but it is a serious concern and one that the authors should be aware of and remedy.

-   Two extensive chunks copied from: Prystauka, Y., Altmann, G. T., & Rothman, J. (2024). Online eye tracking and real-time sentence processing: On opportunities and efficacy for capturing psycholinguistic effects of different magnitudes and diversity. Behavior Research Methods, 56(4), 3504-3522.

    As discussed in the Gorilla documentation, the Gorilla layout engine lays everything out in a 4:3 frame and makes that frame as big as possible. The normalized coordinates are then expressed relative to this frame; for example, the coordinate 0.5, 0.5 will always be the center of the screen, regardless of the size of the participant's screen. We used the normalized coordinates in our analysis.

    This represents the model's confidence in finding a face (and accurately predicting eye movements). Values vary from 0 to 1, and numbers less than 0.5 suggest that the model has probably converged. Another metric is .face_conf, which represents the support vector machine (SVM) classifier score for the face model fit. This score indicates how strongly the image under the model resembles a face. Values vary from 0 to 1, and here numbers greater than 0.5 are indicative of a good model fit.

-   One chunk copied from https://slcladal.netlify.app/pwr.html

    If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time.

-   One extensive methods chunk copied from Sarrett et al. (2022):

    Each item within a set was repeated 4 times as the target word. This yielded 240 trials (15 sets × 4 items per set × 4 repetitions). Each item set consisted of one Spanish-Spanish cohort pair and one Spanish-English cohort pair. Both items in a Spanish-Spanish pair had a"reciprocal" competitor relationship (that is, we could test activation for cielo given ciencia, and for ciencia given cielo). Consequently, there were 120 trials in the Spanish-Spanish condition. In contrast, only one item from the Spanish-English pair had the specified competitor relationship (we could test activation for frontera border, given botas, but when hearing frontera, there was no competitor). Thus, there were only 60 trials for the Spanish-English as well as the No Competitor conditions. Items occurred in each of the four corners of the screen on an equal number of trials.
:::

First and foremost, we would like to sincerely apologize for the textual overlap identified in our manuscript. We want to be clear that this was not intentional, and we take this concern very seriously. We are grateful to the reviewer for bringing this to our attention.

Upon reviewing the identified sections, we found that several sentences were unintentionally reproduced with language that closely resembled the original sources. To address this, we have thoroughly revised the relevant passages to ensure they are fully original, appropriately paraphrased, and, where applicable, directly cited.

We have conducted a full review of the manuscript to ensure there are no additional instances of overlap and have taken this as an opportunity to reinforce our commitment to academic integrity and responsible writing practices. Below you will find the rewritten paragraphs.

::: memo-excerpt
Gorilla standardizes gaze coordinates relative to a fixed 4:3 aspect ratio that is centered within the participant’s browser window. Rather than using raw screen pixels, gaze positions are transformed into normalized coordinates ranging from 0 to 1, where (0, 0) represents the bottom-left corner of the Gorilla stage and (1, 1) represents the top-right. This normalization accounts for potential whitespace or letterboxing that occurs when a participant’s screen has a different aspect ratio than the Gorilla stage. As a result, normalized coordinates are comparable across participants, regardless of their individual screen resolutions or browser sizes. For example, a normalized coordinate of (0.5, 0.5) always refers to the center of the Gorilla stage, ensuring spatial consistency in analyses.
:::

::: memo-excerpt
To ensure data quality, we removed rows with poor convergence and low face confidence from our eye-tracking dataset. The Gorilla eye-tracking output includes two key columns for this purpose: convergence and face_conf (similar variables may be available in other platforms as well). The convergence column contains values between 0 and 1, with lower values indicating better convergence—that is, greater model confidence in predicting gaze location and finding a face. Values below 0.5 typically reflect adequate convergence. The face_conf column reflects how confidently the algorithm detected a face in the frame, also ranging from 0 to 1. Here, values above 0.5 indicate a good model fit.
:::

::: memo-excerpt
Before proceeding, make sure to load the required packages by running the code below. If you already have these packages installed and loaded, feel free to skip this step. The code in this tutorial will not run correctly if any of the necessary packages are missing or not properly loaded. Depending on your computer’s performance, it may take a few moments for the packages to load—please be patient.
:::

::: memo-excerpt
Each item within a set appeared four times as the target word, resulting in a total of 240 trials (15 sets × 4 items per set × 4 repetitions). Each set included one Spanish–Spanish cohort pair and one Spanish–English cohort pair. In the Spanish–Spanish condition, both words in the pair served as mutual competitors—for example, cielo activated ciencia, and vice versa. This bidirectional relationship yielded 120 trials for the Spanish–Spanish condition.

In contrast, the Spanish–English pairs had an asymmetrical relationship: only one item in each pair functioned as a competitor (e.g., botas could activate frontera, but frontera did not have a corresponding competitor). As a result, there were 60 trials each for the Spanish–English and No Competitor conditions. Across all trials, target items were equally distributed among the four screen quadrants to ensure balanced visual presentation
:::

## Specific concerns

::: memo-reviewer
***Reviewer 1***: Pages 2 to 6 start with strawman arguments. This paper probably works best if we get straight to where we are as of March 2025. We know web-based eye-tracking works. There are many published replications. Researchers (including us) have written guides. This manuscript is, in fact, a guide. What the field needs right now is guidance on how to make web-based eye tracking more accessible, transparent, and reliable.

This section, particularly from pages 3-4, also has arguments that have been made from these: cost, weird populations. It is nice to see that the arguments are used but this weakens the overall paper because those arguments are no longer relevant. Furthermore, those studies should be cited for making the same arguments.

Solution/edit: Start the manuscript at the last paragraph (line 121) on Page 6. This is really where the field is right now. The authors' guide offers two contributions to the field at this moment: the webgazeR package and the bilingual lexical competition replication. Focus on these two contributions in the introduction and tell the reader how these two contributions advance our understanding.
:::

We appreciate the reviewer’s thoughtful feedback and agree that the original introduction included arguments (e.g., cost savings, WEIRD populations) that are now well established in the literature and no longer advance the discussion meaningfully. We are especially grateful for the reviewer’s concrete suggestion to reorient the manuscript by starting with the final paragraph on page 6 and centering our contributions.

In response, we have substantially rewritten the introduction to reflect the current state of the field. Rather than revisiting early justifications for webcam-based eye tracking, the revised introduction now:

-   Focuses on webgazer.js as a tool and its positioning relative to research-grade eye trackers;
-   Summarizes what we currently know about the use of webcam eye tracking within the Visual World Paradigm (VWP), including key validation studies and limitations;
-   Introduces the two core contributions of this manuscript:
    1.  The webgazeR R package as an accessible, transparent tool for processing web-based eye-tracking data; and
    2.  A replication of bilingual lexical competition effects using webcam-based eye tracking.

These changes are intended to clarify the tutorial’s relevance and help move the field toward more reliable, reproducible, and accessible use of webcam eye tracking.

::: memo-reviewer
***Reviewer 1*** : On line 83-84 you state: "Another method, which is the focus of this tutorial, is automated eye-tracking or webcam Eye-tracking. Webcam eye-tracking requires three things: 1. A personal computer. 2. An internet connection and 3. A purchased or pre-installed webcamera."

However, this is not entirely true in terms of internet connection for webcam eye-tracking. You could do it without the internet as long as the experiment is fully loaded in advance and all necessary resources are locally available. This is meaningful because some researchers may still be interested in testing a local population or use a high quality webcam to answer a specific research question that requires a particular framerate. Additionally, we also worry about longevity here. It is likely that some eye-tracking could move to tablet based experiments in the future. While we don't know of any current study that has attempted such work, it could be done.
:::

We appreciate and welcome Reviewer 1's opinion here. However, I am not aware of any platform allowing eye-tracking to be conducted off line. I think Labvanced may allow for this, but I have not played around with their eye-tracking component. To hedge this a bit, we included the work generally instead of requires. We also added the term "smartphone". I have done a study with Tobii sticky (referenced) where we used cell phone cameras and eye-tracking.

::: memo-excerpt
One online method growing in popularity—and that may help mitigate some of the aforementioned limitations by increasing the inclusivity and representativeness of participant samples—is automated webcam-based eye-tracking. Webcam eye-tracking typically requires three things : (1) a personal computer or smartphone [see @chen-sankey2023], (2) an internet connection, and (3) a built-in or external camera. Gaze data is collected directly through a web browser without requiring any additional software installation, making it highly accessible.
:::

::: memo-reviewer
***Reviewer 1***: The section titled "Eye-tracking outside the lab" is meant to focus on remote eye-tracking, but it spends too much time comparing it to lab-based methods rather than discussing its own strengths and challenges. While some comparison is useful, the current framing makes it feel like a defense of webcam-based tracking rather than a clear explanation of its methodologies and applications. This weakens the message and confuses the focus. Instead of positioning remote eye-tracking as an inferior alternative, the section would be stronger if it presented it on its own terms—highlighting its advantages, limitations, and successful implementations—before making selective comparisons where necessary.
:::

We have reframed this discussion in the rewritten introduction.

::: memo-reviewer
***Reviewer 1***: Line 140, "Most relevant to this tutorial are online replications using the VWP (Tanenhaus et al., 1995; cf. Cooper, 1974)." What does this sentence refer to? What is the online replication you are referring to here
:::

We have rewrittent this sentence.

::: memo-excerpt
Despite these limitations, one domain where webcam-based eye tracking has shown particular promise is in the adaptation of the Visual World Paradigm (VWP) [@tanenhaus1995; cf. @cooper1974] to online research environments.
:::

::: memo-reviewer
***Reviewer 1***: Page 8, top paragraph some of this VWP stuff is not accurate. VWP can involve written words:

Participants can also simply look and listen (i.e., no need to click on anything) as in the Altmann & Kamide paper the authors cite:

Lines 153-155, "Remarkably, looks to each object align very closely—and with precise timing—with the mental activation of the word or concept it represents" lacks precision.

This statement lacks precision because it overstates the directness of the relationship between eye movements and lexical activation. In reality: 1) Eye movements are influenced by multiple factors, including visual salience, task demands, and prior expectations, not just word activation. 2) Timing variability exists—while VWP studies rely on time-locked analyses, there is no perfect one-to-one mapping between when a concept is activated and when the eyes move. 3) Alternative explanations are possible—such as strategic guessing or attentional shifts unrelated to lexical access.
:::

The reviewer is correct. The VWP can involve words. However, we are describing the classic VWP, which does not include words. We have updated our descirption of the VWP to be a bit clearer and more precise and we belive addresses the reviewers concerns.

::: memo-excerpt
What makes the widespread use of the VWP even more remarkable is the simplicity of the task. In a typical VWP experiment, participants view a display containing several objects. As they listen to a spoken word or phrase, their eye movements are recorded in real time. A robust finding in VWP research is that listeners reliably direct their gaze to the picture representing the spoken word, often before the word has been fully articulated, revealing anticipatory or predictive processing.

While eye movements are often time-locked to linguistic input, the relationship between gaze and lexical processing is not one-to-one. Lexical activation interacts with non-lexical factors such as selective attention, visual salience, task demands, working memory, and prior expectations—all of which can shape where and when participants look [@eberhard1995; @huettig2011; @kamide2003]. Nonetheless, the VWP remains a powerful and flexible tool for studying online language processing, offering fine-grained insights into how linguistic and cognitive processes unfold moment by moment.
:::

::: memo-reviewer
***Reviewer 1***: Page 8, bottom paragraph, there is other work out there. For example, we have a new paper that uses written words and examines spoken Italian word recognition:
:::

We have added this excellent reference to our paper.

::: memo-excerpt
More relevant to the current tutorial are findings from single-word VWP studies conducted online. A few studies have began to look at single-word speech perception online (Bramlett & Wiener, 2025; SLim et al., 2024).
:::

::: memo-reviewer
***Reviewer 1***: Page 9, this is good stuff here! This is where the field currently is. Zoom in on these issues (Of particular interest, time).
:::

We have rewritten this part of the paper and expanded the temporal delay part.

::: memo-excerpt
Several factors have been proposed to explain the poor temporal performance in the VWP. These include reduced spatial precision, computational demands introduced by the WebGazer.js algorithm, slower internet connections, larger areas of interest (AOIs), and calibration quality [@degen2021; @slim2024; @vanboxtel2024]. It is important to note here that poor temporal performance is not always found.

Recent work has begun to address many of these challenges by leveraging updated versions of WebGazer.js and adopting new experimental platforms. For instance, @vos2022 reported a substantial reduction in temporal delays—approximately 50 ms—when using a newer version of WebGazer.js embedded within the jsPsych framework [@deleeuw2015]. Similarly, studies by @prystauka2024 and @bramlett2024, which utilized the Gorilla Experiment Builder in combination with the improved WebGazer algorithm, found timing and competition effects closely aligned with those observed in traditional lab-based VWP studies.

While the extent of temporal delays in the VWP online is still an open question, the general findings underscore the potential of the online version of the VWP, powered by webcam eye-tracking, to achieve results similar to those of traditional lab-based methods. Importantly, they demonstrate that this approach can effectively be used to study competition effects in single-word speech perception.
:::

::: memo-reviewer
***Reviewer 1***: Page 11, this is also interesting, and a nice way to make science even more reproducible! Very cool!
:::

Thank you!

::: memo-reviewer
***Reviewer 1***: Page 12, lines 232-235. We think of competitors and cohorts as different concepts. A competitor can compete for many reasons. Beaker and speaker are rhyme competitors; beaker and beetle are onset competitors. A cohort is a pool of candidates, all of which compete for lexical access. It is not always the 'initial phoneme' (see onset vs. rhyme; or your own examples of /wIs3ld/ and /wIz3rd/
:::

We have changed the wording of the paragraph to reflect a broader range of candidates.

::: memo-excerpt
It is well established that lexical competition plays a central role in language processing [@magnuson2007]. During spoken word recognition, as the auditory signal unfolds over time, multiple lexical candidates—or competitors—can become partially activated. Successful recognition depends on resolving this competition by inhibiting or suppressing mismatching candidates. For example, upon hearing the initial segments of the word wizard, phonologically similar words such as whistle may be briefly activated. As the word continues, additional competitors like blizzard might also become active. For wizard to be accurately recognized, activation of competitors such as whistle and blizzard must ultimately be suppressed.
:::

::: memo-reviewer
Page 13. This issue of recruiting L2 speakers is interesting and something the authors' work can really speak to. Please unpack this and really highlight how this study advances our knowledge with respect to recruitment.
:::

We agree. We have added a discussion of this in the dicussion.

::: memo-reviewer
***Reviewer 1***: Lines 272-273, "(1) between the ages of 18 and 36 years old, (2) native English speakers, (3) were also fluent in Spanish, and (4) residents of the US", Why was this chosen? Was it from the original study? Was it made up or pre-registered?
:::

We have added specific reasons for our filter choices in the manuscript under Participants.

::: memo-excerpt
Participants were recruited through Prolific, an online participant recruitment platform. Inclusion criteria required participants to: (1) be between 18 and 36 years old, (2) be native English speakers, (3) also be fluent in Spanish, and (4) reside in the United States. Criterion 1 was based on findings from @colby2023 , which suggest that age-related changes in spoken word recognition begin to emerge in individuals in their 40s; thus, we limited our sample to participants younger than 36. Criteria 2 and 3 ensured that we were recruiting native English speakers and those fluent in Spanish to test L1 and L2 interactions. Criterion 4 matched the population of the original study, which was conducted with university students in Iowa, and therefore restricted recruitment to U.S. residents.
:::

::: memo-reviewer
***Reviewer 1***:Solution/edit: Here and next few pages (13-16) hammer home the point about using the internet to recruit L2 speakers for web-based studies. What do researchers need to pay attention to? Figure 1 speaks volumes. This suggests only about 15% of the data the authors initially collected was usable. (28/187). What does this mean for researchers?
:::

We really appreciate the solution/edit. This is not really the appropriate place to talk about this in the paper (it is a general methods section).Moreover, the results in Figure 1 are not specific to L2 recruitment, it a general webcam eye-tracking problem that most researchers face. We dicuss solutions to this in the discussion.

::: memo-reviewer
***Reviewer 1***:Page 16, Table 1. N = 91? But only 32 completed the VWP task and 28 were included in the sample. Please clarify.
:::

We have made that section a lot clearer in the manuscript.

::: memo-excerpt
After agreeing to participate, individuals were redirected to the Gorilla experiment platform (www.gorilla.sc; [@anwyl-irvine2020]). A flow diagram of participant progression through the experiment is shown in fig-sankey. In total, 187 participants consented to participate. Of these, 121 passed the headphone screener checkpoint, and 111 proceeded to the VWP task. Out of the 111 participants who entered the VWP, 91 completed the final surveys at the end of the experiment. Among these, 32 participants successfully completed the VWP task with at least 100 trials, while 79 participants did not provide adequate data for inclusion, primarily due to failed calibration attempts. After applying additional exclusion criteria—namely, overall VWP task accuracy below 80% and excessive missing eye-tracking data (\>30%)—the final analytic sample consisted of 28 participants with usable eye-tracking data. Descriptive demographic information for the full sample that made it to the final survey is provided in tbl-demo2.
:::

::: memo-reviewer
***Reviewer 1***:Lines 324-331, this is very odd here because you cite Woods (2017) but it appears that you have cloned the Milne experiment just like our study. The study cites Woods (2017) as a reference for its headphone screening method, but the actual stimuli and experimental design appear to align more closely with Milne et al. (2021), which introduced a Huggins pitch-based screening test. While Woods (2017) is relevant for general headphone screening methods, its test is based on an anti-phase intensity discrimination paradigm (which is what you describe), whereas Milne (2021) specifically develops and validates a method using Huggins pitch—the same stimulus type apparently used in this study (which is what is in the actual shared materials). Given the clear methodological overlap, Milne et al. (2021) should be cited directly rather than (or at least in addition to) Woods (2017). Omitting this citation gives the misleading impression that the study's approach is more distinct from prior work than it actually is. A more transparent discussion of how this study relates to both Woods (2017) and Milne (2021) would strengthen the paper's contribution and clarify its position within existing research.
:::

We want to thank the reviewers for providing a reference for the headphone screener. I was using the Woods headphone screener in a previous experiment and must have copied the Milne (2021) one when conducting the new experiments. In text we now reference Milne et al. in text.

::: memo-excerpt
Headphones were required for all participants. To ensure compliance, we administered a six-trial headphone screening task adapted from @milne2021, which is available for implementation on the Gorilla platform.
:::

::: memo-reviewer
***Reviewer 1*** Page 19, this post-experiment questionnaire is a great idea. Talk more about this. This is novel and a good contribution to the field.
:::

We have expanded the discussion of the post-experiment questionnaire and its broader implications for online eye-tracking research in the Discussion section, where we believe it is more appropriate to reflect on its novelty, potential applications, and future directions.

::: memo-reviewer
***Reviewer 1***: Line 356: "The experiment was programmed in the Gorilla Experiment Platform." Did you use a coded module in gorilla? We looked for one in your open materials but it appears that you used the GUI to build the experiment.
:::

We used the GUI to build the experiment and also added code through scripting. We have clarified this in the text.

::: memo-excerpt
All tasks and questionnaires were developed using the Gorilla Experiment Builder’s graphical user interface (GUI) and integrated coding tools [@anwyl-irvine2020].
:::

::: memo-reviewer
***Reviewer 1***: Line 357: "Upon entering the online study, participants received general information to decide if they wished to participate, after which they provided informed consent." Does the general information part refer to them choosing the study from the prolific queue of studies? We didn't see anything before consent in your materials.
:::

We have updated the language in this section.

::: memo-excerpt
Upon entering the study from Prolific, participants were presented with a consent form.
:::

::: memo-reviewer
***Reviewer 1***:VWP is used oddly throughout the paper e.g., For those who passed the screening, the next task was the VWP. I think you mean VWP task/experiment/module. The sentence that you have implied that the next task was the "visual world paradigm" itself. You named the task VWP in Gorilla but the reader doesn't know that.
:::

We have update the language to make it clearer

::: memo-excerpt
If the headphone screener was passed, participants were next introduced to the VWP task. This began with instructional videos providing specific guidance on the ideal experiment setup for eye-tracking and calibration procedures. You can view the videos here: https://osf.io/mgkd2/. Participants were then required to enter full-screen mode before calibration. A 9-point calibration procedure was used. Calibration occurred every 60 trials for a total of 3 calibrations. Participants had three attempts to successfully complete each calibration phase. If calibration was unsuccessful, participants were directed to an early exit screen, followed by the questionnaire.
:::

::: memo-reviewer
***Reviewer 1***: Lines 373-379, your study introduces key procedural differences from Sarrett et al. (2022) that impact direct comparability. Notably, the trial structure in your study differs in several ways: 1) The preview phase differs—images first appear, disappear, and then reappear only after participants click the start button. Sarrett et al. (2022) did not remove and reintroduce images in this way. 2) Your study includes a forced start button click, which ensures central fixation but alters natural viewing behavior compared to Sarrett et al. (2022), where images remain continuously visible. 3)Your inter-trial interval (ITI) is 500 ms, whereas Sarrett et al. (2022) used 250 ms, potentially affecting trial pacing. 4) Your study implements a 5000 ms forced trial progression, while Sarrett et al. (2022) did not include such a restriction.
:::

We agree that these are important differences to consider. We have added these differences to Page 13, where we talk about the differences between the studies.

We would like to point out that the reviewers are incorrect about number 2 above. In Sarrett et al the center circle changed from red to blue after 1000 ms (we use 1500 ms) and participants had to click on a dot to hear the audio. This is why we included a start button to make sure individuals eye at the center of the screen and is common design for the MAC lab. 

As it relates to number 4, trials progressed after a click, and it was rare for participants to time out of the study.

Given that we replicated the findings pretty closely we believe these are moot points, but we do consider them in the discussion.

::: memo-excerpt
Finally, there were several important design differences between our study and that of @sarrett2022. In @sarrett2022, participants were first presented with a preview of the images in each of the four screen quadrants for 1000 ms, followed by the appearance of a central red dot. Participants were required to click on the dot to initiate audio playback. After selecting the target, there was a 250 ms inter-trial interval (ITI) before the next trial began.

In contrast, our study used a slightly different sequence. Participants were first shown a fixation cross for 500 ms (which also served as the ITI), followed by a longer preview display lasting 1500 ms. After the preview, the images disappeared, and a start button appeared at the center of the screen. Participants were required to click on the start button to initiate audio playback, after which the images reappeared. Once participants selected the target, the experiment immediately proceeded to the next trial. A timeout limit of 5 seconds was imposed for trials where no response was made.

There were several motivations for these design choices. Online studies introduce greater variability across participants’ setups (e.g., internet connection, device variability), and thus we opted for a longer preview period to maximize our chances of observing lexical competetion effects. Prior work has shown that longer preview times can enhance competition effects in the VWP [@apfelbaum2021]. We also introduced a mandatory start button click at the center of the screen to ensure participants began each trial from a centralized gaze position, minimizing quadrant-based biases. Finally, we incorporated a timeout feature to address potential inattentiveness common in unsupervised, online testing environments.

:::

::: memo-reviewer
***Reviewer 1***:
Page 23, at this point the manuscript becomes about webgazeR and the decisions the researcher needs to consider. Please keep the scope of the paper to using the package to analyze the data and how this can be done in a clear and reproducible manner. Things like Gorilla settings (pg. 29) are irrelevant to this and should be removed. This is not a paper about building an experiment on Gorilla. I have to imagine the authors want readers to use the webgazeR package with other i.e., non-Gorilla data, so this should be as accessible and general as possible.
:::

We have removed reference to the Gorilla settings audio figure. However, because we used Gorilla we need to make readers aware that they include those additional audio settings and should include those when analyzing their data. We also added that users using different experimental settings should request this information. 

::: memo-excerpt
Because we are playing audio on each trial and running this experiment from the browser, audio onset is never going to be consistent across participants. In Gorilla there is an option to collect advanced audio features (you must make sure you select this when designing the study) such as when the audio play was requested, played, and ended. We will want to incorporate this timing information into our analysis pipeline. Gorilla records the onset of the audio which varies by participant. We are extracting that in the audio_rt_L2 object by filtering zone_type to content_web_audio and a response equal to "AUDIO PLAY EVENT FIRED". This will tell us when the audio was triggered in the experiment. We are creating a column called (RT_audio) which we will use later on to correct for audio delays. Please note that on some trials the audio may not play. This is a function of the browser a participant is using and the experimenter has no control over this (see https://support.gorilla.sc/support/troubleshooting-and-technical/technical-checklist#autoplayingsoundandvideo). When running your experiment on a different platform, make sure you try and request this information, or at the very least acknowledge audio delay as a limitation. 
:::

::: memo-reviewer
***Reviewer 1***: Lines 454-490: The merge_webcam_files() function is unnecessary and reduces transparency by abstracting simple data processing steps—reading .xlsx files, merging them, renaming columns, and filtering—into a black-boxed process. While the function provides a convenience wrapper, it does not offer any substantial advantages over a straightforward dplyr and purrr approach, yet it introduces potential debugging difficulties. An example of this breaking already in a current structure is if you had multiple types of files in the upload folder. For instance, if you collected both eye-tracking and mouse tracking this would combine them, which is inappropriate.

Additionally, this function is tied specifically to Gorilla's .xlsx format, meaning it will break if Gorilla modifies its data export structure again, as has already happened multiple times. Its reliance on fixed column names (spreadsheet_row, time_elapsed) without verification makes it fragile, and its automatic package installation step is unnecessary in a controlled analysis environment.

A more transparent and flexible approach would simply use:

vwp_files_L2 \<- list.files(here::here("data", "L2", "raw"), full.names = TRUE, pattern = "\\.(csv\|xlsx)\$") %\>% discard(\~ grepl("calibration", .x))

vwp_data_L2 \<- map_dfr(vwp_files_L2, \~ if (grepl("\\.csv\$", .x)) read_csv(.x) else read_excel(.x)) %\>% rename(trial = spreadsheet_row, time = time_elapsed) %\>% filter(type == "prediction", screen_index == 4) %\>% mutate(trial = as.factor(trial), subject = as.factor(participant_id)) %\>% select(-participant_id)
:::

Thank you for this thoughtful suggestion. We have substantially revised the merge_webcam_files() function to address the concerns raised. The updated function now allows users to flexibly specify how dataset columns should be mapped onto standardized names (subject, trial, time, x, y) via a col_map argument. This makes it compatible with a variety of datasets and not tied to any fixed export structure, including Gorilla. Additionally, the function can now read .csv, .tsv, and .xlsx files automatically using a transparent purrr::map_dfr() approach, rather than assuming only .xlsx inputs. A new kind argument has been introduced, allowing platform-specific preprocessing steps to be modularized, supporting future extensions to other experimental platforms such as Labvanced and PsychoPy. Platform-specific steps are isolated cleanly. We acknowledge the risk of mixing unrelated files, and while the function processes all provided file paths, users are expected to specify appropriate file lists. If desired, filename pattern filtering can be easily added.

Additionally, we have introduced a make_webgazer() function that takes an already loaded dataframe and renames columns to conform to webgazerR conventions. This function standardizes column names to subject, trial, time, x, and y, which are the only columns necessary for WebGazerR to operate correctly. All other columns are preserved, ensuring maximum compatibility without unnecessary data loss.

Lastly, in the finalized version, automatic package installation steps will be removed to ensure adherence to reproducibility standards in controlled environments. We believe these changes retain user convenience while substantially improving transparency, flexibility, and robustness.

::: memo-reviewer
***Reviewer 1***: Lines 625-635, The function analyze_sampling_rate() appears to be strongly derived from prior work in Bramlett & Wiener (2024), yet this is not explicitly acknowledged. The structure, core calculations, and approach are highly similar to existing work on participant frame rates, differing mainly in minor modifications such as the removal of frame rate categorization and time binning.

Additionally, the function's structure suggests AI-assisted code generation, as it introduces unnecessary abstraction and overparameterization. Specifically: 1) The two-step structure (Step 1: Validate summary_stat, Step 2: Compute sampling rate) artificially separates operations that could be streamlined. 2) The overuse of parameterization (e.g., dynamically assigning summary_fn \<- if (summary_stat == "median") median else mean) is unnecessary for a function that will almost always use one measure. 3) excessive spaces at the ends of lines before +

While citing Bramlett & Wiener (2024) for sampling rate recommendations is appropriate, if the function was directly inspired by that work, it would be more transparent to acknowledge it explicitly. Otherwise, the function's structure and implementation appear too close to existing methods to be independently developed.
:::

We appreciate the reviewer’s comment. However, we are somewhat confused by this concern. Our analyze_sampling_rate() function substantially extends beyond the code reported in Bramlett & Wiener (2024). Specifically, our function calculates sampling rates both at the trial and participant levels, provides summary outputs, and generates visualization plots, whereas the referenced code in their paper focuses primarily on participant-level cleaning and time binning or per-trial summaries.

While we recognize that the general idea of calculating a sampling rate (i.e., number of frames divided by total time) is necessarily similar, this is a standard method across many eye-tracking and behavioral timing studies. We are not aware of alternative methods for calculating basic sampling rates.

Nevertheless, to appropriately acknowledge prior work, we have added a citation to Bramlett & Wiener (2024) in both the function documentation and the manuscript text. We would like to make it known this manuscript has been in the works since 2020 and we have

::: memo-excerpt
Sampling rate calculations followed standard procedures [e.g., @bramlett2024; @prystauka2024]
 :::

::: memo-reviewer
***Reviewer 1***: Lines 708-722: This is the same pattern again. This function is less transparent than readily usable code. Further, it isn't necessary. If you want to remove something then just filter it.

filtered_data \<- data %\>% filter(med_SR \>= threshold & SR \>= threshold)
:::

We disagree. Not everyone will want to remove the data. Further, decison has to be made at the trial and subject level. The filter function gives users those options and they don't have to write a lot of code to do it. 


::: memo-reviewer
***Reviewer 1***:
Lines 493-495, this needs citing and reasoning.
:::

We have attempted to justify our exclusion decisions. 

::: memo-excerpt
To ensure high-quality data, we applied a set of behavioral and eye-tracking exclusion criteria prior to merging datasets. Participants were excluded if they met any of the following conditions: (1) failure to successfully calibrate throughout the experiment (fewer than 100 completed trials), (2) low behavioral accuracy (below 80%), (3) low sampling rate (below 5 Hz), or (4) a high proportion of gaze samples falling outside the display area (greater than 30%).

Successful calibration is critical for reliable eye-tracking measurements, as poor calibration directly compromises the spatial accuracy of gaze data [@blascheck2017]. Requiring a sufficient number of completed trials is crucial for ensuring adequate statistical power and stable individual-level parameter estimates, particularly in tasks with high trial-to-trial variability [@brysbaert2018]. We choose 100 trials as this meant particpants passed at least two calibration attempts during the study.  Behavioral accuracy ( >= 80%) was used as an additional screening measure because low task performance may indicate a lack of attention, misunderstanding of the task, or random responding, all of which could undermine both the behavioral and eye-movement data quality [@bianco2021]. Filtering based on sampling rate ensures that datasets with too few gaze samples (due to technical or environmental issues) are removed, as low sampling rates significantly degrade temporal precision and bias gaze metrics [@semmlmann2018]. Finally, we excluded participants with excessive off-screen data (>30%) because this indicates poor gaze tracking, likely caused by head movement, poor lighting, or loss of face detection. At this time, there is no set guide on what constitutes acceptable data loss for webcam-based studies. We felt 30% was a reasonable cut-off. At the trial-level, we also removed incorrect trials and trials where sampling rate was < 5 Hz. 

In addition to participant-level exclusions, we also removed incorrect trials from the remaining participants. Restricting analyses to correct trials is standard in Visual World Paradigm research, as incorrect trials could involve different cognitive processes and introduce additional variability into gaze patterns [@dahan2001].
:::


Lines 723-729- why is this separate from 798-781


You are correct. It should not be separate. We have moved this to where we talk about the `gaze_oob` function. 


::: memo-excerpt
If the remove argument is set to TRUE, the function applies an outer-edge filtering method to eliminate these out-of-bounds points [see @bramlett2024].

As shown in @fig-fixquads, no fixation points in our data fell outside the standardized coordinate range.
:::


::: memo-reviewer
***Reviewer 1***:
Lines 765-767— Why would duplicates arise? That is terrifying. Why not just use a left_join()

Lines 574-576, the need for distinct() suggests that right_join() is introducing duplicate rows due to either non-unique join conditions or unintended many-to-many relationships. A simpler fix, rather than applying distinct(), would be to swap the order of the dataframes in the right_join(), ensuring that behavioral trials remain while incorporating available eye-tracking data. Alternatively, using left_join() would make this explicit and avoid unnecessary data loss.

:::

Indeed it is terrifying! We want to thank the reviewer for pointing this out. Delving deeper into the data we noticed some issues with the Gorilla data. First, Gorilla sometimes duplicates time stamps/rows. We reached out to support and they said we could remove the duplicated time stamp. We have made our functions more robust to this, specifically the merge_webcam and analyze_sampling rate. 

Second, in our task, participants sometimes clicked on images during the preview, which registered as a click causing multiple data points per trial. This has been fixed in the code by filtering by screen_name.

Correcting this has solved the above issues. 

::: memo-reviewer
***Reviewer 1***:
Lines 798-802- Your claim here is false. We recommended only outer edge removal. You claim that we recommend inner edge removal (our motto: maximize signal and embrace noise by only removing outer points). We would also like you to discuss the "less bias" of outer edge removal more. Unclear why separate from the 723-729 part?
:::

We apologize for misrepresenting your work. We have removed this claim from the paper and for now only recommend the outer-edge removal. We do not dicuss it being a less biased apporach. 

::: memo-excerpt
If the remove argument is set to TRUE, the function applies an outer-edge filtering method to eliminate these out-of-bounds points [see @bramlett2024]. The outer-edge approach appears to be a less biased approach based on demonstrations from @bramlett2024. When employing an outer-edge approach, minimal data less resulted compared to other approaches. 
:::

::: memo-reviewer
***Reviewer 1***:
Between 1203-1204 footnote- so do you have enough trials or not? Please clarify.
:::

In the original study, Sarrett et al. had > 400 trials. Thebob usually uses a lot of trials for their curve fitting approach. We only used 250 trials, half of what they used, and it is unclear if we can apply that approach. We have updated the footnote to make this clearer.

::: memo-excerpt
The curve fitting approach employed by @sarrett2022 might necessitate more trials. The original study had over 400 trials. It is not clear if we can get good fits with the number of trials we have here. 
:::

# Reviewer 2
::: memo-reviewer
***Reviewer 2***:
1. The introduction about the limitations of lab-based eye-tracking might sound a bit exaggerated. For example, "proper calibration, data collection, and analysis" is also required for using web-based eye-tracking. Cost may not be a big issue once the eye-tracker is purchased because it can be used repeatedly with very minimal cost, but web-based data collection requires continuous payment to the platform. The authors also mention that ~20% to 70% data may be wasted due to failures of calibration, etc. This may also lead to extra cost. Thus, I would suggest focusing on the benefits of using web-based eye-tracking instead of the limitations of lab-based eye-tracking in this section.
:::

We appreciate this feedback and agree the original framing may have overemphasized the limitations of lab-based systems. In response to this and related feedback from Reviewer 1, we have substantially revised the introduction. The new version focuses more clearly on the strengths, limitations, and unique opportunities of webcam-based eye-tracking, rather than framing it as a direct alternative to lab-based systems.

::: memo-reviewer
***Reviewer 2***:
2. It would be good to clarify whether the observed ~100 ms delay in competition effects (compared to lab-based studies) is inherent to webcam tracking or a result of other factors (e.g., participant setup or loading of the audio, etc).
:::

Thank you for this suggestion. We now include a discussion of potential sources of this temporal delay, including reduced spatial precision, variability in calibration quality, and latency introduced by the WebGazer.js algorithm. We also reference recent work demonstrating that updated WebGazer versions and better experimental platforms can reduce these delays.


::: memo-reviewer
***Reviewer 2***:
1. I would suggest the authors include a flowchart to summarize the processing pipeline.

:::

This is an excellent idea. We now include a flowchart that illustrates the preprocessing pipeline, including which webgazeR functions are used for each step.

::: memo-reviewer
***Reviewer 2***:
2. Maybe Figure 1 can be revised so it's easier to interpret.
:::

Not sure. 

::: memo-reviewer
***Reviewer 2***:
1. Perhaps the authors could provide references or sources for using alternative methods (e.g., GAMMs or growth curve analysis)
:::

We agree and have clarified this in the text. Several references to existing tutorials are already included, but we have now made these more explicit and consolidated them in a section summarizing available analytical approaches.

::: memo-excerpt
After you have preprocessed the data, the next step is analysis. When analyzing VWP data there are many analytic approaches to choose from (e.g., growth curve analysis (GCA), cluster permutation tests (CPA), generalized additive mixed models (GAMMS), logistic multilevel models, divergent point analysis (DPA), etc.), and a lot has already been written describing these methods and a lot of great tutorials exist showing how to apply these methods to visual world fixation data from the lab [e.g., see @ito2023 for a tutorial on using all these methods] and online [@bramlett2024]. 
:::

::: memo-reviewer
***Reviewer 2***:
6. Can effect sizes or confidence intervals be added to the CPA results?

:::

We appreciate the reviewer’s suggestion. Calculating effect size measures for cluster-based permutation analysis (CPA) is not straightforward, as traditional effect size metrics are not directly applicable. However, we now include a discussion of three commonly proposed approaches for estimating effect sizes in CPA: (1) averaging the effect within the predefined time window, (2) using the maximum effect size observed within the significant cluster, and (3) calculating the average effect across all time points within the cluster. These approaches are described in the revised manuscript to enhance interpretability and support the robustness of our findings.

::: memo-excerpt
Effect size

It is important to address the issue of effect sizes in the context of CPA. Calculating effect sizes for CPA is not straightforward, as the technique is designed to evaluate temporal clusters rather than individual time points. @slim2024 [but also see @meyer2021] outline three possible approaches for estimating effect sizes in CPA: (1) computing the effect size within a predefined time window (often the same window used for identifying clusters), (2) calculating an average effect size across the entire cluster, and (3) reporting the maximum effect observed within the cluster. Each method has its trade-offs in terms of interpretability and comparability across studies, and the choice should be guided by theoretical considerations and the research question at hand.
:::

::: memo-reviewer
***Reviewer 2***:
1. The high attrition rate (~40% due to calibration failures or not having enough trials) raises concerns about the generalizability of web-based eye-tracking. These participants also cost a quota in Gorilla right? Are there any suggested strategies to improve the calibration rates?
:::

We agree this is an important consideration. While Gorilla does not charge per participant if one holds a subscription, researchers should still consider the time and resources required. We provide strategies for improving calibration success in the discussion, emphasizing webcam quality, environmental conditions, and clearer instructions.

::: memo-reviewer
***Reviewer 2***:
2. Although the questionnaire (Table 9) is useful, would it make more sense for future studies to ask participants to fill in the questionnaire before the experiment (for example, to pre-screen webcam quality)?
:::

This is a valuable suggestion. We now mention this in the discussion, noting that asking participants about webcam type ahead of time could help screen for higher-quality setups and reduce calibration failures.

::: memo-excerpt
 Our questionnaire suggested that participants using external webcams had significantly better calibration success compared to those relying on built-in webcams. External webcams generally provide higher resolution and frame rates, which are critical for accurate eye-tracking. Researchers should encourage participants to use external webcams whenever possible. Researchers could even administer a questionnaire before the experiment and branch participants out of the experiment that do not report using an external webcam. 
:::

::: memo-reviewer
***Reviewer 2***:
3. Power analysis could have been provided to justify the sample size.
:::

We agree with the reviewer that a justification of the sample size is warranted.

Given that we observed significant effects, we believe a post-hoc power analysis would not provide meaningful additional insight. Post-hoc power is generally uninformative when effects are already statistically significant, as it merely reflects the observed p-values and effect sizes.

Our main goal here was a tutorial and not necessarily finding a significant effect. 

::: memo-excerpt

Participants

Participants were recruited through Prolific, an online participant recruitment platform. Our goal was to approximately double the sample size of Sarrett et al. (2022) to enhance statistical power and ensure greater generalizability of the findings. However, due to practical constraints and the challenges associated with online webcam eye-tracking (e.g., calibration failures) and also the limited pool of binlingual Spanish speakers, we were unable to achieve the targeted usable sample size. Therefore, we report the final sample based on all participants who met our predefined inclusion criteria. Future work will be needed to replicate and extend these findings.
:::

::: memo-reviewer
***Reviewer 2***:
1. Define "WEIRD-A" (line 57) upon first use. Not all readers may be familiar with the acronym.
:::

We did define this term in the introduction.


::: memo-excerpt
Behavioral science research has long struggled with a lack of diversity, relying heavily on participants who are predominantly Western, Educated, Industrialized, Rich, and Democratic (WEIRD) [@henrich2010]. Additionally, we propose adding able-bodied to this acronym (WEIRD-A) [@peterson2021], to highlight the exclusion of individuals with disabilities who may face barriers to accessing research facilities. 
:::

::: memo-reviewer
***Reviewer 2***:
2. Specify why 300 ms was subtracted from the time alignment (lines 864-865). While justified in the repository, this should be briefly explained in the main text.
:::

This has been clarified in the manuscript. The 300 ms offset accounts for a 100 ms silence buffer at the start of audio playback and a 200 ms oculomotor delay [@viviani1990].


::: memo-excerpt
To achieve this, we subtract RT_audio from time for each trial. In addition, we subtract 300 ms from this to account for the 100 ms of silence at the beginning of each audio clip and 200 ms to account for the oculomotor delay when planning an eye movement [@viviani1990]. 
:::

::: memo-reviewer
***Reviewer 2***:
3. The quadrant vs. Gorilla-provided AOI comparison (Figures 5-9) is interesting but could be condensed. The key takeaway (similar patterns, noisier with smaller AOIs) could be summarized more succinctly.
:::

Thank you. We’ve revised this section to be more concise while still emphasizing the key takeaway: quadrant-based AOIs yielded similar patterns but with slightly less spatial precision than Gorilla-provided coordinates.

::: memo-reviewer
***Reviewer 2***:
4. "Poor vs. Good Calibrators" seems to be overly detailed. Perhaps, it would be good to focus on strategies for getting good calibration.
:::

We appreciate the suggestion and have reduced the detail in this section, instead emphasizing practical recommendations for achieving successful calibration in remote studies.


