
---
title: Responses to editor and reviewer concerns
format:
  hikmah-response-typst: default
bibliography: references.bib
---

Thank you for giving us the opportunity to revise and resubmit our manuscript, "Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research" (Manuscript ID `RMAL-D-25-00055`). We found the Editor's and the Reviewers' comments to be very insightful and helpful and we have used these comments to rewrite and improve the manuscript in a number of ways. I would like to say that I really appreciate the time Reviewer 1 took to provide constructive comments and edits, including providing code. In this memo, we describe how we have revised the paper in response to these comments. The Editor's and Reviewers' comments are cited or summarized in [red italicized text]{.memo-reviewer-inline}, and our responses are in black Roman text. We include excerpts from our revised manuscript in [blue]{.memo-excerpt-inline}.

We have made a few changes to the webgazeR package to make it robust and flexible to different types of data. It is not perfect and we hope users will post issues so we can make the package better. 

# Reviewer 1

## Borader concerns

The authors noted overlap in the wording of some of the methods discussed in the paper. 

::: memo-reviewer
***Reviewer 1***: There are some places of plagiarism. We do not think this was intentional, but it is a serious concern and one that the authors should be aware of and remedy.

- Two extensive chunks copied from:
  Prystauka, Y., Altmann, G. T., & Rothman, J. (2024). Online eye tracking and real-time sentence processing: On opportunities and efficacy for capturing psycholinguistic effects of different magnitudes and diversity. Behavior Research Methods, 56(4), 3504-3522.

  As discussed in the Gorilla documentation, the Gorilla layout engine lays everything out in a 4:3 frame and makes that frame as big as possible. The normalized coordinates are then expressed relative to this frame; for example, the coordinate 0.5, 0.5 will always be the center of the screen, regardless of the size of the participant's screen. We used the normalized coordinates in our analysis.

  This represents the model's confidence in finding a face (and accurately predicting eye movements). Values vary from 0 to 1, and numbers less than 0.5 suggest that the model has probably converged. Another metric is .face_conf, which represents the support vector machine (SVM) classifier score for the face model fit. This score indicates how strongly the image under the model resembles a face. Values vary from 0 to 1, and here numbers greater than 0.5 are indicative of a good model fit.

- One chunk copied from
https://slcladal.netlify.app/pwr.html

  If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time.


- One extensive methods chunk copied from Sarrett et al. (2022):

  Each item within a set was repeated 4 times as the target word. This yielded 240 trials (15 sets × 4 items per set × 4 repetitions). Each item set consisted of one Spanish-Spanish cohort pair and one Spanish-English cohort pair. Both items in a Spanish-Spanish pair had a"reciprocal" competitor
relationship (that is, we could test activation for cielo given ciencia, and for ciencia given cielo).
Consequently, there were 120 trials in the Spanish-Spanish condition. In contrast, only one item
from the Spanish-English pair had the specified competitor relationship (we could test activation
for frontera border, given botas, but when hearing frontera, there was no competitor). Thus, there were only 60 trials for the Spanish-English as well as the No Competitor conditions. Items
occurred in each of the four corners of the screen on an equal number of trials.
:::

First and foremost, we would like to sincerely apologize for the textual overlap identified in our manuscript. We want to be clear that this was not intentional, and we take this concern very seriously. We are grateful to the reviewer for bringing this to our attention.

Upon reviewing the identified sections, we found that several sentences were unintentionally reproduced with language that closely resembled the original sources. To address this, we have thoroughly revised the relevant passages to ensure they are fully original, appropriately paraphrased, and, where applicable, directly cited.

We have conducted a full review of the manuscript to ensure there are no additional instances of overlap and have taken this as an opportunity to reinforce our commitment to academic integrity and responsible writing practices. Below you will find the rewritten paragraphs. 

::: memo-excerpt
Gorilla standardizes gaze coordinates relative to a fixed 4:3 aspect ratio that is centered within the participant’s browser window. Rather than using raw screen pixels, gaze positions are transformed into normalized coordinates ranging from 0 to 1, where (0, 0) represents the bottom-left corner of the Gorilla stage and (1, 1) represents the top-right. This normalization accounts for potential whitespace or letterboxing that occurs when a participant’s screen has a different aspect ratio than the Gorilla stage. As a result, normalized coordinates are comparable across participants, regardless of their individual screen resolutions or browser sizes. For example, a normalized coordinate of (0.5, 0.5) always refers to the center of the Gorilla stage, ensuring spatial consistency in analyses.
:::

::: memo-excerpt

To ensure data quality, we removed rows with poor convergence and low face confidence from our eye-tracking dataset. The Gorilla eye-tracking output includes two key columns for this purpose: convergence and face_conf (similar variables may be available in other platforms as well). The convergence column contains values between 0 and 1, with lower values indicating better convergence—that is, greater model confidence in predicting gaze location and finding a face. Values below 0.5 typically reflect adequate convergence. The face_conf column reflects how confidently the algorithm detected a face in the frame, also ranging from 0 to 1. Here, values above 0.5 indicate a good model fit.
:::


::: memo-excerpt
Before proceeding, make sure to load the required packages by running the code below. If you already have these packages installed and loaded, feel free to skip this step. The code in this tutorial will not run correctly if any of the necessary packages are missing or not properly loaded. Depending on your computer’s performance, it may take a few moments for the packages to load—please be patient.
:::


::: memo-excerpt
Each item within a set appeared four times as the target word, resulting in a total of 240 trials (15 sets × 4 items per set × 4 repetitions). Each set included one Spanish–Spanish cohort pair and one Spanish–English cohort pair. In the Spanish–Spanish condition, both words in the pair served as mutual competitors—for example, cielo activated ciencia, and vice versa. This bidirectional relationship yielded 120 trials for the Spanish–Spanish condition.

In contrast, the Spanish–English pairs had an asymmetrical relationship: only one item in each pair functioned as a competitor (e.g., botas could activate frontera, but frontera did not have a corresponding competitor). As a result, there were 60 trials each for the Spanish–English and No Competitor conditions. Across all trials, target items were equally distributed among the four screen quadrants to ensure balanced visual presentation
:::

## Specific concerns

::: memo-reviewer
 ***Reviewer 1***: Pages 2 to 6 start with strawman arguments. This paper probably works best if we get straight to where we are as of March 2025. We know web-based eye-tracking works. There are many published replications. Researchers (including us) have written guides. This manuscript is, in fact, a guide. What the field needs right now is guidance on how to make web-based eye tracking more accessible, transparent, and reliable.

This section, particularly from pages 3-4, also has arguments that have been made from these: cost, weird populations. It is nice to see that the arguments are used but this weakens the overall paper because those arguments are no longer relevant. Furthermore, those studies should be cited for making the same arguments.

Solution/edit: Start the manuscript at the last paragraph (line 121) on Page 6. This is really where the field is right now. The authors' guide offers two contributions to the field at this moment: the webgazeR package and the bilingual lexical competition replication. Focus on these two contributions in the introduction and tell the reader how these two contributions advance our understanding.
:::

We appreciate the reviewer’s thoughtful feedback and agree that the original introduction included arguments (e.g., cost savings, WEIRD populations) that are now well established in the literature and no longer advance the discussion meaningfully. We are especially grateful for the reviewer’s concrete suggestion to reorient the manuscript by starting with the final paragraph on page 6 and centering our contributions.

In response, we have substantially rewritten the introduction to reflect the current state of the field. Rather than revisiting early justifications for webcam-based eye tracking, the revised introduction now:

- Focuses on webgazer.js as a tool and its positioning relative to research-grade eye trackers;
- Summarizes what we currently know about the use of webcam eye tracking within the Visual World Paradigm (VWP), including key validation studies and limitations;
- Introduces the two core contributions of this manuscript:
	1. The webgazeR R package as an accessible, transparent tool for processing web-based eye-tracking data; and
	2. A replication of bilingual lexical competition effects using webcam-based eye tracking.

These changes are intended to clarify the tutorial’s relevance and help move the field toward more reliable, reproducible, and accessible use of webcam eye tracking.

::: memo-reviewer
***Reviewer 1 ***: On line 83-84 you state: "Another method, which is the focus of this tutorial, is automated eye-tracking or webcam Eye-tracking. Webcam eye-tracking requires three things: 1. A personal computer. 2. An internet connection and 3. A purchased or pre-installed webcamera."

However, this is not entirely true in terms of internet connection for webcam eye-tracking. You could do it without the internet as long as the experiment is fully loaded in advance and all necessary resources are locally available. This is meaningful because some researchers may still be interested in testing a local population or use a high quality webcam to answer a specific research question that requires a particular framerate. Additionally, we also worry about longevity here. It is likely that some eye-tracking could move to tablet based experiments in the future. While we don't know of any current study that has attempted such work, it could be done.
:::


We appreciate and welcome Reviewer 1's opinion here. However, I am not aware of any platform allowing eye-tracking to be conducted off line. I think Labvanced may allow for this, but I have not played around with their eye-tracking component. To hedge this a bit, we included the work generally instead of requires. We also added the term "smartphone". I have done a study with Tobii sticky (referenced) where we used cell phone cameras and eye-tracking. 

::: memo-excerpt
One online method growing in popularity—and that may help mitigate some of the aforementioned limitations by increasing the inclusivity and representativeness of participant samples—is automated webcam-based eye-tracking. Webcam eye-tracking typically requires three things : (1) a personal computer or smartphone [see @chen-sankey2023], (2) an internet connection, and (3) a built-in or external camera. Gaze data is collected directly through a web browser without requiring any additional software installation, making it highly accessible.
:::


::: memo-reviewer
***Reviewer 1***: The section titled "Eye-tracking outside the lab" is meant to focus on remote eye-tracking, but it spends too much time comparing it to lab-based methods rather than discussing its own strengths and challenges. While some comparison is useful, the current framing makes it feel like a defense of webcam-based tracking rather than a clear explanation of its methodologies and applications. This weakens the message and confuses the focus. Instead of positioning remote eye-tracking as an inferior alternative, the section would be stronger if it presented it on its own terms—highlighting its advantages, limitations, and successful implementations—before making selective comparisons where necessary.
:::

We have reframed this discussion in the rewritten introduction. 

::: memo-reviewer
***Reviewer 1***:
Line 140, "Most relevant to this tutorial are online replications using the VWP (Tanenhaus et al., 1995; cf. Cooper, 1974)." What does this sentence refer to? What is the online replication you are referring to here
:::

We have rewrittent this sentence. 

::: memo-excerpt
Despite these limitations, one domain where webcam-based eye tracking has shown particular promise is in the adaptation of the Visual World Paradigm (VWP) [@tanenhaus1995; cf. @cooper1974] to online research environments. 
:::

::: memo-reviewer
***Reviewer 1***:
Page 8, top paragraph some of this VWP stuff is not accurate. VWP can involve written words:

Participants can also simply look and listen (i.e., no need to click on anything) as in the Altmann & Kamide paper the authors cite:

Lines 153-155, "Remarkably, looks to each object align very closely—and with precise timing—with the mental activation of the word or concept it represents" lacks precision.

This statement lacks precision because it overstates the directness of the relationship between eye movements and lexical activation. In reality: 1) Eye movements are influenced by multiple factors, including visual salience, task demands, and prior expectations, not just word activation.
2) Timing variability exists—while VWP studies rely on time-locked analyses, there is no perfect one-to-one mapping between when a concept is activated and when the eyes move. 3) Alternative explanations are possible—such as strategic guessing or attentional shifts unrelated to lexical access.
:::

The reviewer is correct. The VWP can involve words. However, we are describing the classic VWP, which does not include words. We have updated our descirption of the VWP to be a bit clearer and more precise and we belive addresses the reviewers concerns. 


::: memo-excerpt
What makes the widespread use of the VWP even more remarkable is the simplicity of the task. In a  typical VWP experiment, participants view a display containing several objects. As they listen to a spoken word or phrase, their eye movements are recorded in real time. A robust finding in VWP research is that listeners reliably direct their gaze to the picture representing the spoken word, often before the word has been fully articulated, revealing anticipatory or predictive processing.

While eye movements are often time-locked to linguistic input, the relationship between gaze and lexical processing is not one-to-one. Lexical activation interacts with non-lexical factors such as selective attention, visual salience, task demands, working memory, and prior expectations—all of which can shape where and when participants look [@eberhard1995; @huettig2011; @kamide2003]. Nonetheless, the VWP remains a powerful and flexible tool for studying online language processing, offering fine-grained insights into how linguistic and cognitive processes unfold moment by moment.
:::

::: memo-reviewer
***Reviewer 1***:
Page 8, bottom paragraph, there is other work out there. For example, we have a new paper that uses written words and examines spoken Italian word recognition:
:::

We have added this excellent reference to our paper. 

::: memo-excerpt
More relevant to the current tutorial are findings from single-word VWP studies conducted online. A few studies have began to look at single-word speech perception online (Bramlett & Wiener, 2025; SLim et al., 2024). 
:::

::: memo-reviewer
***Reviewer 1***:
Page 9, this is good stuff here! This is where the field currently is. Zoom in on these issues (Of particular interest, time).
:::


We have rewritten this part of the paper and expanded the temporal delay part. 

::: memo-excerpt
Several factors have been proposed to explain the poor temporal performance in the VWP. These include reduced spatial precision, computational demands introduced by the WebGazer.js algorithm, slower internet connections, larger areas of interest (AOIs), and calibration quality [@degen2021; @slim2024; @vanboxtel2024]. It is important to note here that poor temporal performance is not always found. 

Recent work has begun to address many of these challenges by leveraging updated versions of WebGazer.js and adopting new experimental platforms. For instance, @vos2022 reported a substantial reduction in temporal delays—approximately 50 ms—when using a newer version of WebGazer.js embedded within the jsPsych framework [@deleeuw2015]. Similarly, studies by @prystauka2024 and @bramlett2024, which utilized the Gorilla Experiment Builder in combination with the improved WebGazer algorithm, found timing and competition effects closely aligned with those observed in traditional lab-based VWP studies.

While the eetent of temporal delays in the VWP online is still an open question, the general findings underscore the potential of the online version of the VWP, powered by webcam eye-tracking, to achieve results similar to those of traditional lab-based methods. Importantly, they demonstrate that this approach can effectively be used to study competition effects in single-word speech perception.
:::

::: memo-reviewer
***Reviewer 1***:
Page 11, this is also interesting, and a nice way to make science even more reproducible! Very cool!
:::

Thank you! 


::: memo-reviewer
***Reviewer 1***:
Page 12, lines 232-235. We think of competitors and cohorts as different concepts. A competitor can compete for many reasons. Beaker and speaker are rhyme competitors; beaker and beetle are onset competitors. A cohort is a pool of candidates, all of which compete for lexical access. It is not always the 'initial phoneme' (see onset vs. rhyme; or your own examples of /wIs3ld/ and /wIz3rd/
:::

We have changed the wording of the paragraph to reflect a broader range of candidates. 

::: memo-excerpt
It is well established that lexical competition plays a central role in language processing [@magnuson2007]. During spoken word recognition, as the auditory signal unfolds over time, multiple lexical candidates—or competitors—can become partially activated. Successful recognition depends on resolving this competition by inhibiting or suppressing mismatching candidates. For example, upon hearing the initial segments of the word wizard, phonologically similar words such as whistle may be briefly activated. As the word continues, additional competitors like blizzard might also become active. For wizard to be accurately recognized, activation of competitors such as whistle and blizzard must ultimately be suppressed.
:::


::: memo-reviewer
***Reviewer 1***:
Lines 454-490: The merge_webcam_files() function is unnecessary and reduces transparency by abstracting simple data processing steps—reading .xlsx files, merging them, renaming columns, and filtering—into a black-boxed process. While the function provides a convenience wrapper, it does not offer any substantial advantages over a straightforward dplyr and purrr approach, yet it introduces potential debugging difficulties. An example of this breaking already in a current structure is if you had multiple types of files in the upload folder. For instance, if you collected both eye-tracking and mouse tracking this would combine them, which is inappropriate.

Additionally, this function is tied specifically to Gorilla's .xlsx format, meaning it will break if Gorilla modifies its data export structure again, as has already happened multiple times. Its reliance on fixed column names (spreadsheet_row, time_elapsed) without verification makes it fragile, and its automatic package installation step is unnecessary in a controlled analysis environment.

A more transparent and flexible approach would simply use:

vwp_files_L2 <- list.files(here::here("data", "L2", "raw"), full.names = TRUE, pattern = "\\.(csv|xlsx)$") %>%
discard(~ grepl("calibration", .x))

vwp_data_L2 <- map_dfr(vwp_files_L2, ~ if (grepl("\\.csv$", .x)) read_csv(.x) else read_excel(.x)) %>%
rename(trial = spreadsheet_row, time = time_elapsed) %>%
filter(type == "prediction", screen_index == 4) %>%
mutate(trial = as.factor(trial), subject = as.factor(participant_id)) %>%
select(-participant_id)
:::

Thank you for this thoughtful suggestion. We have substantially revised the merge_webcam_files() function to address the concerns raised. The updated function now allows users to flexibly specify how dataset columns should be mapped onto standardized names (subject, trial, time, x, y) via a col_map argument. This makes it compatible with a variety of datasets and not tied to any fixed export structure, including Gorilla. Additionally, the function can now read .csv, .tsv, and .xlsx files automatically using a transparent purrr::map_dfr() approach, rather than assuming only .xlsx inputs. A new kind argument has been introduced, allowing platform-specific preprocessing steps to be modularized, supporting future extensions to other experimental platforms such as Labvanced and PsychoPy. Platform-specific steps are isolated cleanly. We acknowledge the risk of mixing unrelated files, and while the function processes all provided file paths, users are expected to specify appropriate file lists. If desired, filename pattern filtering can be easily added.

Additionally, we have introduced a make_webgazer() function that takes an already loaded dataframe and renames columns to conform to webgazerR conventions. This function standardizes column names to subject, trial, time, x, and y, which are the only columns necessary for WebGazerR to operate correctly. All other columns are preserved, ensuring maximum compatibility without unnecessary data loss.

Lastly, in the finalized version, automatic package installation steps will be removed to ensure adherence to reproducibility standards in controlled environments. We believe these changes retain user convenience while substantially improving transparency, flexibility, and robustness.


::: memo-reviewer
***Reviewer 1***:
Lines 625-635, The function analyze_sampling_rate() appears to be strongly derived from prior work in Bramlett & Wiener (2024), yet this is not explicitly acknowledged. The structure, core calculations, and approach are highly similar to existing work on participant frame rates, differing mainly in minor modifications such as the removal of frame rate categorization and time binning.

Additionally, the function's structure suggests AI-assisted code generation, as it introduces unnecessary abstraction and overparameterization. Specifically: 1) The two-step structure (Step 1: Validate summary_stat, Step 2: Compute sampling rate) artificially separates operations that could be streamlined. 2) The overuse of parameterization (e.g., dynamically assigning summary_fn <- if (summary_stat == "median") median else mean) is unnecessary for a function that will almost always use one measure. 3) excessive spaces at the ends of lines before +

While citing Bramlett & Wiener (2024) for sampling rate recommendations is appropriate, if the function was directly inspired by that work, it would be more transparent to acknowledge it explicitly. Otherwise, the function's structure and implementation appear too close to existing methods to be independently developed.
:::


We appreciate the reviewer’s comment. However, we are somewhat confused by this concern.
Our analyze_sampling_rate() function substantially extends beyond the code reported in Bramlett & Wiener (2024).
Specifically, our function calculates sampling rates both at the trial and participant levels, provides summary outputs, and generates visualization plots, whereas the referenced code in their paper focuses primarily on participant-level cleaning and time binning or per-trial summaries.

While we recognize that the general idea of calculating a sampling rate (i.e., number of frames divided by total time) is necessarily similar, this is a standard method across many eye-tracking and behavioral timing studies. We are not aware of alternative methods for calculating basic sampling rates.

Nevertheless, to appropriately acknowledge prior work, we have added a citation to Bramlett & Wiener (2024) in both the function documentation and the manuscript text.

::: memo-excerpt
Sampling rate calculations followed standard procedures, e.g., Bramlett & Wiener, 2024.”)
:::

::: memo-reviewer
***Reviewer 1***:
Lines 708-722: This is the same pattern again. This function is less transparent than readily usable code. Further, it isn't necessary. If you want to remove something then just filter it.

filtered_data <- data %>%
filter(med_SR >= threshold & SR >= threshold)
:::

